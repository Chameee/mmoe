{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afad38ad-51b0-4ccd-a2da-04cc7cc84244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e17883c-ea49-4aa0-9a28-18a642d687b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 100\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf67ce54-9afd-4e34-9705-4b5aa29a6ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 15 13:12:40 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.191.01   Driver Version: 450.191.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    72W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff37979-2716-414b-9de0-97d1dff96944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "random.seed(3)\n",
    "np.random.seed(3)\n",
    "seed = 3\n",
    "batch_size = 1024\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c091b661-5e4f-4bfe-bffc-d470de591672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d0cc7d-48da-42f5-9ee9-55dea0fc75ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.jupyter/connection\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e311ac0-e550-4d76-a393-91235d69f58d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1]==1 and len(input_shape)>1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()  # 拉成一维矩阵\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes, )\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e16a1b99-f475-48b7-bccc-dba180e80899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_preparation():\n",
    "    column_names = ['country_y', 'yearmonth', 'media_gained_decisiondate', \n",
    "            'media_total_decisiondate', 'followers_gained_decisiondate', 'followers_total_decisiondate', 'following_gained_decisiondate', \n",
    "            'following_total_decisiondate', 'media_gained_decisiondate_7days', 'media_total_decisiondate_7days', 'followers_gained_decisiondate_7days', \n",
    "            'followers_total_decisiondate_7days', 'following_gained_decisiondate_7days', 'following_total_decisiondate_7days', 'media_gained_decisiondate_15days', \n",
    "            'media_total_decisiondate_15days', 'followers_gained_decisiondate_15days', 'followers_total_decisiondate_15days', 'following_gained_decisiondate_15days', \n",
    "            'following_total_decisiondate_15days', 'media_gained_decisiondate_30days', 'media_total_decisiondate_30days', 'followers_gained_decisiondate_30days', \n",
    "            'followers_total_decisiondate_30days', 'following_gained_decisiondate_30days', 'following_total_decisiondate_30days', 'media_gained_decisiondate_60days', \n",
    "            'media_total_decisiondate_60days', 'followers_gained_decisiondate_60days', 'followers_total_decisiondate_60days', 'following_gained_decisiondate_60days', \n",
    "            'following_total_decisiondate_60days', 'media_gained_decisiondate_90days', 'media_total_decisiondate_90days', 'followers_gained_decisiondate_90days', \n",
    "            'followers_total_decisiondate_90days', 'following_gained_decisiondate_90days', 'following_total_decisiondate_90days', 'followers_rate_decisiondate_7days', \n",
    "            'following_rate_decisiondate_7days', 'media_rate_decisiondate_7days', 'followers_rate_decisiondate_15days', 'following_rate_decisiondate_15days', \n",
    "            'media_rate_decisiondate_15days', 'followers_rate_decisiondate_30days', 'following_rate_decisiondate_30days', 'media_rate_decisiondate_30days', \n",
    "            'followers_rate_decisiondate_60days', 'following_rate_decisiondate_60days', 'media_rate_decisiondate_60days', 'followers_rate_decisiondate_90days', \n",
    "            'following_rate_decisiondate_90days', 'media_rate_decisiondate_90days',  'avg_total_orders', 'avg_new_customers', 'avg_customer_num', \n",
    "            'first_campaign_time', 'new_influencer', 'last_orders_total', 'last_new_customers', 'last_customer_num', 'days_from_last_posting', \n",
    "            'days_from_last_sponsored', 'days_from_last_org_branded', 'days_from_last_org_nonbranded', 'days_interval_posting', 'days_interval_sponsored', \n",
    "            'days_interval_org_branded', 'days_interval_org_nonbranded', '90days_comment_count', '90days_like_count', 'num_sponsored_posts_90days', \n",
    "            'num_organic_branded_90days', 'num_organic_nonbranded_90days', 'num_posts_90days', '60days_comment_count', '60days_like_count', \n",
    "            'num_sponsored_posts_60days', 'num_organic_branded_60days', 'num_organic_nonbranded_60days', 'num_posts_60days', '30days_comment_count', \n",
    "            '30days_like_count', 'num_sponsored_posts_30days', 'num_organic_branded_30days', 'num_organic_nonbranded_30days', 'num_posts_30days', \n",
    "            '15days_comment_count', '15days_like_count', 'num_sponsored_posts_15days', 'num_organic_branded_15days', 'num_organic_nonbranded_15days', \n",
    "            'num_posts_15days', '7days_comment_count', '7days_like_count', 'num_sponsored_posts_7days', 'num_organic_branded_7days', 'num_organic_nonbranded_7days', \n",
    "            'num_posts_7days', 'accept', 'Revenue', 'reputation_change_v2']\n",
    "\n",
    "    df = pd.read_csv('/var/tmp/code/recommendation_model/mmoe_model/data/precampaign_duringcampaign_features_acceptance_decision_performance_0502.csv', delimiter=',', index_col=None, low_memory=False)\n",
    "    df = df[column_names]\n",
    "\n",
    "#     train_df = pd.read_csv('/var/tmp/code/recommendation_model/mmoe_model/data/census-income.data.gz', delimiter=',', header=None, index_col=None, names=column_names)\n",
    "#     test_df = pd.read_csv('/var/tmp/code/recommendation_model/mmoe_model/data/census-income.test.gz', delimiter=',', header=None, index_col=None, names=column_names)\n",
    "\n",
    "    # 论文中第一组任务\n",
    "    label_columns = ['accept', 'Revenue', 'reputation_change_v2']\n",
    "\n",
    "    categorical_columns = ['first_campaign_time', 'country_y', 'yearmonth']\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "    df_transformed = pd.get_dummies(df.drop(label_columns, axis=1), columns=categorical_columns)\n",
    "    df_labels = df[label_columns]\n",
    "\n",
    "\n",
    "    return df_transformed, df_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ff4da4-dd49-4f51-a1cb-76be924248a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transformed, df_labels = data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77cef364-1246-4687-833d-5b86713c7cee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "media_gained_decisiondate                  0.000000\n",
       "media_total_decisiondate                  82.000000\n",
       "followers_gained_decisiondate             -1.000000\n",
       "followers_total_decisiondate            3334.000000\n",
       "following_gained_decisiondate             -3.000000\n",
       "following_total_decisiondate             687.000000\n",
       "media_gained_decisiondate_7days           -1.000000\n",
       "media_total_decisiondate_7days            86.000000\n",
       "followers_gained_decisiondate_7days        3.000000\n",
       "followers_total_decisiondate_7days      3330.000000\n",
       "following_gained_decisiondate_7days        0.000000\n",
       "following_total_decisiondate_7days       689.000000\n",
       "media_gained_decisiondate_15days          -1.000000\n",
       "media_total_decisiondate_15days           88.000000\n",
       "followers_gained_decisiondate_15days      14.000000\n",
       "followers_total_decisiondate_15days     3308.000000\n",
       "following_gained_decisiondate_15days      -1.000000\n",
       "following_total_decisiondate_15days      688.000000\n",
       "media_gained_decisiondate_30days           0.000000\n",
       "media_total_decisiondate_30days           86.000000\n",
       "followers_gained_decisiondate_30days      -2.000000\n",
       "followers_total_decisiondate_30days     3310.000000\n",
       "following_gained_decisiondate_30days      -1.000000\n",
       "following_total_decisiondate_30days      693.000000\n",
       "media_gained_decisiondate_60days           0.000000\n",
       "media_total_decisiondate_60days           83.000000\n",
       "followers_gained_decisiondate_60days       1.000000\n",
       "followers_total_decisiondate_60days     3230.000000\n",
       "following_gained_decisiondate_60days       0.000000\n",
       "following_total_decisiondate_60days      695.000000\n",
       "media_gained_decisiondate_90days           0.000000\n",
       "media_total_decisiondate_90days           80.000000\n",
       "followers_gained_decisiondate_90days      -3.000000\n",
       "followers_total_decisiondate_90days     3226.000000\n",
       "following_gained_decisiondate_90days       1.000000\n",
       "following_total_decisiondate_90days      688.000000\n",
       "followers_rate_decisiondate_7days          0.571429\n",
       "following_rate_decisiondate_7days         -0.285714\n",
       "media_rate_decisiondate_7days             -0.571429\n",
       "followers_rate_decisiondate_15days         1.733333\n",
       "following_rate_decisiondate_15days        -0.066667\n",
       "media_rate_decisiondate_15days            -0.400000\n",
       "followers_rate_decisiondate_30days         0.800000\n",
       "following_rate_decisiondate_30days        -0.200000\n",
       "media_rate_decisiondate_30days            -0.133333\n",
       "followers_rate_decisiondate_60days         1.733333\n",
       "following_rate_decisiondate_60days        -0.133333\n",
       "media_rate_decisiondate_60days            -0.016667\n",
       "followers_rate_decisiondate_90days         1.200000\n",
       "following_rate_decisiondate_90days        -0.011111\n",
       "media_rate_decisiondate_90days             0.022222\n",
       "avg_total_orders                          18.500000\n",
       "avg_new_customers                          0.000000\n",
       "avg_customer_num                           0.000000\n",
       "new_influencer                             0.000000\n",
       "last_orders_total                          0.000000\n",
       "last_new_customers                         0.000000\n",
       "last_customer_num                          0.000000\n",
       "days_from_last_posting                    16.000000\n",
       "days_from_last_sponsored                 178.000000\n",
       "days_from_last_org_branded               145.000000\n",
       "days_from_last_org_nonbranded             16.000000\n",
       "days_interval_posting                      9.787879\n",
       "days_interval_sponsored                   20.833333\n",
       "days_interval_org_branded                 64.666667\n",
       "days_interval_org_nonbranded              14.363636\n",
       "90days_comment_count                     106.600000\n",
       "90days_like_count                        844.000000\n",
       "num_sponsored_posts_90days                 0.000000\n",
       "num_organic_branded_90days                 0.000000\n",
       "num_organic_nonbranded_90days              5.000000\n",
       "num_posts_90days                           5.000000\n",
       "60days_comment_count                     142.333333\n",
       "60days_like_count                        967.666667\n",
       "num_sponsored_posts_60days                 0.000000\n",
       "num_organic_branded_60days                 0.000000\n",
       "num_organic_nonbranded_60days              3.000000\n",
       "num_posts_60days                           3.000000\n",
       "30days_comment_count                     142.333333\n",
       "30days_like_count                        967.666667\n",
       "num_sponsored_posts_30days                 0.000000\n",
       "num_organic_branded_30days                 0.000000\n",
       "num_organic_nonbranded_30days              3.000000\n",
       "num_posts_30days                           3.000000\n",
       "15days_comment_count                       0.000000\n",
       "15days_like_count                          0.000000\n",
       "num_sponsored_posts_15days                 0.000000\n",
       "num_organic_branded_15days                 0.000000\n",
       "num_organic_nonbranded_15days              0.000000\n",
       "num_posts_15days                           0.000000\n",
       "7days_comment_count                        0.000000\n",
       "7days_like_count                           0.000000\n",
       "num_sponsored_posts_7days                  0.000000\n",
       "num_organic_branded_7days                  0.000000\n",
       "num_organic_nonbranded_7days               0.000000\n",
       "num_posts_7days                            0.000000\n",
       "first_campaign_time_0                      0.000000\n",
       "first_campaign_time_2019-07-06             0.000000\n",
       "first_campaign_time_2019-09-20             0.000000\n",
       "first_campaign_time_2019-10-19             0.000000\n",
       "first_campaign_time_2019-12-03             0.000000\n",
       "first_campaign_time_2019-12-18             0.000000\n",
       "first_campaign_time_2020-01-14             0.000000\n",
       "first_campaign_time_2020-01-24             0.000000\n",
       "first_campaign_time_2020-01-26             0.000000\n",
       "first_campaign_time_2020-02-20             0.000000\n",
       "first_campaign_time_2020-03-16             0.000000\n",
       "first_campaign_time_2020-05-08             0.000000\n",
       "first_campaign_time_2020-05-13             0.000000\n",
       "first_campaign_time_2020-10-04             0.000000\n",
       "first_campaign_time_2020-10-11             0.000000\n",
       "first_campaign_time_2020-10-21             0.000000\n",
       "first_campaign_time_2020-10-22             0.000000\n",
       "first_campaign_time_2020-12-15             0.000000\n",
       "first_campaign_time_2020-12-17             0.000000\n",
       "first_campaign_time_2021-01-26             0.000000\n",
       "first_campaign_time_2021-01-28             0.000000\n",
       "first_campaign_time_2021-02-01             0.000000\n",
       "first_campaign_time_2021-02-08             0.000000\n",
       "first_campaign_time_2021-02-11             0.000000\n",
       "first_campaign_time_2021-02-12             0.000000\n",
       "first_campaign_time_2021-02-13             0.000000\n",
       "first_campaign_time_2021-02-16             0.000000\n",
       "first_campaign_time_2021-02-22             0.000000\n",
       "first_campaign_time_2021-02-25             0.000000\n",
       "first_campaign_time_2021-02-26             0.000000\n",
       "first_campaign_time_2021-03-02             0.000000\n",
       "first_campaign_time_2021-03-09             0.000000\n",
       "first_campaign_time_2021-03-12             0.000000\n",
       "first_campaign_time_2021-03-14             0.000000\n",
       "first_campaign_time_2021-03-22             0.000000\n",
       "first_campaign_time_2021-03-26             0.000000\n",
       "first_campaign_time_2021-04-01             0.000000\n",
       "first_campaign_time_2021-04-08             0.000000\n",
       "first_campaign_time_2021-04-09             0.000000\n",
       "first_campaign_time_2021-04-15             0.000000\n",
       "first_campaign_time_2021-04-18             0.000000\n",
       "first_campaign_time_2021-04-20             0.000000\n",
       "first_campaign_time_2021-04-25             0.000000\n",
       "first_campaign_time_2021-04-29             0.000000\n",
       "first_campaign_time_2021-05-01             0.000000\n",
       "first_campaign_time_2021-05-06             0.000000\n",
       "first_campaign_time_2021-05-15             0.000000\n",
       "first_campaign_time_2021-05-20             0.000000\n",
       "first_campaign_time_2021-06-01             0.000000\n",
       "first_campaign_time_2021-06-14             0.000000\n",
       "first_campaign_time_2021-06-16             0.000000\n",
       "first_campaign_time_2021-06-18             0.000000\n",
       "first_campaign_time_2021-06-20             0.000000\n",
       "first_campaign_time_2021-06-21             0.000000\n",
       "first_campaign_time_2021-06-23             0.000000\n",
       "first_campaign_time_2021-07-01             0.000000\n",
       "first_campaign_time_2021-07-05             0.000000\n",
       "first_campaign_time_2021-07-08             0.000000\n",
       "first_campaign_time_2021-07-14             0.000000\n",
       "first_campaign_time_2021-07-19             0.000000\n",
       "first_campaign_time_2021-07-25             0.000000\n",
       "first_campaign_time_2021-08-01             0.000000\n",
       "first_campaign_time_2021-08-05             0.000000\n",
       "first_campaign_time_2021-08-08             0.000000\n",
       "first_campaign_time_2021-08-09             0.000000\n",
       "first_campaign_time_2021-08-14             0.000000\n",
       "first_campaign_time_2021-08-20             0.000000\n",
       "first_campaign_time_2021-08-23             0.000000\n",
       "first_campaign_time_2021-08-28             0.000000\n",
       "first_campaign_time_2021-09-01             0.000000\n",
       "first_campaign_time_2021-09-05             0.000000\n",
       "first_campaign_time_2021-09-10             0.000000\n",
       "first_campaign_time_2021-09-15             0.000000\n",
       "first_campaign_time_2021-09-20             0.000000\n",
       "first_campaign_time_2021-09-21             0.000000\n",
       "first_campaign_time_2021-09-24             0.000000\n",
       "first_campaign_time_2021-10-01             0.000000\n",
       "first_campaign_time_2021-10-07             0.000000\n",
       "first_campaign_time_2021-10-15             0.000000\n",
       "first_campaign_time_2021-10-20             0.000000\n",
       "first_campaign_time_2021-10-23             0.000000\n",
       "first_campaign_time_2021-11-21             0.000000\n",
       "first_campaign_time_2021-11-22             0.000000\n",
       "first_campaign_time_2021-11-23             0.000000\n",
       "first_campaign_time_2021-11-24             0.000000\n",
       "first_campaign_time_2021-11-25             0.000000\n",
       "first_campaign_time_2021-11-26             0.000000\n",
       "first_campaign_time_2021-11-27             0.000000\n",
       "first_campaign_time_2021-11-29             0.000000\n",
       "first_campaign_time_2021-12-25             1.000000\n",
       "first_campaign_time_2021-12-26             0.000000\n",
       "first_campaign_time_2021-12-28             0.000000\n",
       "first_campaign_time_2022-01-01             0.000000\n",
       "first_campaign_time_2022-01-05             0.000000\n",
       "first_campaign_time_2022-01-10             0.000000\n",
       "first_campaign_time_2022-01-12             0.000000\n",
       "first_campaign_time_2022-01-20             0.000000\n",
       "first_campaign_time_2022-01-24             0.000000\n",
       "first_campaign_time_2022-01-27             0.000000\n",
       "first_campaign_time_2022-01-29             0.000000\n",
       "first_campaign_time_2022-01-31             0.000000\n",
       "first_campaign_time_2022-02-01             0.000000\n",
       "first_campaign_time_2022-02-03             0.000000\n",
       "first_campaign_time_2022-02-05             0.000000\n",
       "first_campaign_time_2022-02-07             0.000000\n",
       "first_campaign_time_2022-02-10             0.000000\n",
       "first_campaign_time_2022-02-14             0.000000\n",
       "first_campaign_time_2022-02-20             0.000000\n",
       "first_campaign_time_2022-02-23             0.000000\n",
       "first_campaign_time_2022-03-01             0.000000\n",
       "first_campaign_time_2022-03-10             0.000000\n",
       "first_campaign_time_2022-03-15             0.000000\n",
       "first_campaign_time_2022-03-16             0.000000\n",
       "first_campaign_time_2022-03-20             0.000000\n",
       "first_campaign_time_2022-03-21             0.000000\n",
       "first_campaign_time_2022-03-24             0.000000\n",
       "first_campaign_time_2022-03-25             0.000000\n",
       "first_campaign_time_2022-03-28             0.000000\n",
       "first_campaign_time_2022-03-31             0.000000\n",
       "first_campaign_time_2022-04-03             0.000000\n",
       "first_campaign_time_2022-04-21             0.000000\n",
       "first_campaign_time_2022-04-23             0.000000\n",
       "first_campaign_time_2022-04-26             0.000000\n",
       "first_campaign_time_2022-04-28             0.000000\n",
       "first_campaign_time_2022-05-08             0.000000\n",
       "first_campaign_time_2022-05-12             0.000000\n",
       "first_campaign_time_2022-05-13             0.000000\n",
       "first_campaign_time_2022-05-18             0.000000\n",
       "first_campaign_time_2022-05-19             0.000000\n",
       "first_campaign_time_2022-05-25             0.000000\n",
       "first_campaign_time_2022-05-26             0.000000\n",
       "first_campaign_time_2022-05-27             0.000000\n",
       "first_campaign_time_2022-05-29             0.000000\n",
       "first_campaign_time_2022-06-01             0.000000\n",
       "first_campaign_time_2022-06-02             0.000000\n",
       "first_campaign_time_2022-06-08             0.000000\n",
       "first_campaign_time_2022-06-16             0.000000\n",
       "first_campaign_time_2022-06-20             0.000000\n",
       "first_campaign_time_2022-06-23             0.000000\n",
       "first_campaign_time_2022-06-24             0.000000\n",
       "first_campaign_time_2022-06-30             0.000000\n",
       "first_campaign_time_2022-07-07             0.000000\n",
       "first_campaign_time_2022-07-26             0.000000\n",
       "first_campaign_time_2022-08-01             0.000000\n",
       "first_campaign_time_2022-08-11             0.000000\n",
       "first_campaign_time_2022-08-17             0.000000\n",
       "first_campaign_time_2022-08-18             0.000000\n",
       "first_campaign_time_2022-08-20             0.000000\n",
       "first_campaign_time_2022-09-02             0.000000\n",
       "first_campaign_time_2022-09-05             0.000000\n",
       "first_campaign_time_2022-09-07             0.000000\n",
       "first_campaign_time_2022-09-12             0.000000\n",
       "first_campaign_time_2022-09-15             0.000000\n",
       "first_campaign_time_2022-09-16             0.000000\n",
       "first_campaign_time_2022-09-19             0.000000\n",
       "first_campaign_time_2022-10-03             0.000000\n",
       "first_campaign_time_2022-10-10             0.000000\n",
       "first_campaign_time_2022-10-12             0.000000\n",
       "first_campaign_time_2022-10-13             0.000000\n",
       "first_campaign_time_2022-10-19             0.000000\n",
       "first_campaign_time_2022-10-23             0.000000\n",
       "first_campaign_time_2022-10-24             0.000000\n",
       "first_campaign_time_2022-10-25             0.000000\n",
       "first_campaign_time_2022-10-27             0.000000\n",
       "first_campaign_time_2022-11-07             0.000000\n",
       "first_campaign_time_2022-11-19             0.000000\n",
       "first_campaign_time_2022-11-20             0.000000\n",
       "first_campaign_time_2022-11-21             0.000000\n",
       "first_campaign_time_2022-11-22             0.000000\n",
       "first_campaign_time_2022-11-24             0.000000\n",
       "first_campaign_time_2022-11-25             0.000000\n",
       "first_campaign_time_2022-11-26             0.000000\n",
       "first_campaign_time_2023-01-01             0.000000\n",
       "first_campaign_time_2023-01-19             0.000000\n",
       "first_campaign_time_2023-01-20             0.000000\n",
       "first_campaign_time_2023-01-26             0.000000\n",
       "first_campaign_time_2023-02-01             0.000000\n",
       "first_campaign_time_2023-02-19             0.000000\n",
       "first_campaign_time_2023-02-20             0.000000\n",
       "first_campaign_time_2023-02-22             0.000000\n",
       "country_y_AT                               0.000000\n",
       "country_y_BE                               0.000000\n",
       "country_y_DE                               1.000000\n",
       "country_y_FR                               0.000000\n",
       "country_y_NL                               0.000000\n",
       "country_y_PL                               0.000000\n",
       "country_y_SE                               0.000000\n",
       "yearmonth_0.0                              1.000000\n",
       "yearmonth_201907.0                         0.000000\n",
       "yearmonth_201909.0                         0.000000\n",
       "yearmonth_201912.0                         0.000000\n",
       "yearmonth_202001.0                         0.000000\n",
       "yearmonth_202002.0                         0.000000\n",
       "yearmonth_202003.0                         0.000000\n",
       "yearmonth_202005.0                         0.000000\n",
       "yearmonth_202010.0                         0.000000\n",
       "yearmonth_202012.0                         0.000000\n",
       "yearmonth_202101.0                         0.000000\n",
       "yearmonth_202102.0                         0.000000\n",
       "yearmonth_202103.0                         0.000000\n",
       "yearmonth_202104.0                         0.000000\n",
       "yearmonth_202105.0                         0.000000\n",
       "yearmonth_202106.0                         0.000000\n",
       "yearmonth_202107.0                         0.000000\n",
       "yearmonth_202108.0                         0.000000\n",
       "yearmonth_202109.0                         0.000000\n",
       "yearmonth_202110.0                         0.000000\n",
       "yearmonth_202111.0                         0.000000\n",
       "yearmonth_202112.0                         0.000000\n",
       "yearmonth_202201.0                         0.000000\n",
       "yearmonth_202202.0                         0.000000\n",
       "yearmonth_202203.0                         0.000000\n",
       "yearmonth_202204.0                         0.000000\n",
       "yearmonth_202205.0                         0.000000\n",
       "yearmonth_202206.0                         0.000000\n",
       "yearmonth_202207.0                         0.000000\n",
       "yearmonth_202208.0                         0.000000\n",
       "yearmonth_202209.0                         0.000000\n",
       "yearmonth_202210.0                         0.000000\n",
       "yearmonth_202211.0                         0.000000\n",
       "yearmonth_202212.0                         0.000000\n",
       "yearmonth_202301.0                         0.000000\n",
       "yearmonth_202302.0                         0.000000\n",
       "yearmonth_202303.0                         0.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd36d4-d488-45d0-a364-a373695a83a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7fd4e31-8f99-4e3d-9c6f-1297d799049e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def getTensorDataset(my_x, my_y):\n",
    "    tensor_x = torch.Tensor(my_x)\n",
    "    tensor_y = torch.Tensor(my_y)\n",
    "    return torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea4c79de-ff35-44eb-ac05-d2c8eaec1c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "class Tower(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Tower, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "class MMOE(nn.Module):\n",
    "    def __init__(self, input_size, num_experts, experts_out, experts_hidden, towers_hidden, tasks):\n",
    "        super(MMOE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_experts = num_experts\n",
    "        self.experts_out = experts_out\n",
    "        self.experts_hidden = experts_hidden\n",
    "        self.towers_hidden = towers_hidden\n",
    "        self.tasks = tasks\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.experts = nn.ModuleList([Expert(self.input_size, self.experts_out, self.experts_hidden) for i in range(self.num_experts)])\n",
    "        self.w_gates = nn.ParameterList([nn.Parameter(torch.randn(input_size, num_experts), requires_grad=True) for i in range(self.tasks)])\n",
    "        self.towers = nn.ModuleList([Tower(self.experts_out, 1, self.towers_hidden) for i in range(self.tasks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        experts_o = [e(x) for e in self.experts]\n",
    "        experts_o_tensor = torch.stack(experts_o)\n",
    "\n",
    "        gates_o = [self.softmax(x @ g) for g in self.w_gates]\n",
    "\n",
    "        tower_input = [g.t().unsqueeze(2).expand(-1, -1, self.experts_out) * experts_o_tensor for g in gates_o]\n",
    "        tower_input = [torch.sum(ti, dim=0) for ti in tower_input]\n",
    "\n",
    "        final_output = [t(ti) for t, ti in zip(self.towers, tower_input)]\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87e3bf25-26cc-4085-ac9d-956a2a2b5195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def test(loader):\n",
    "    t1_pred, t2_pred, t3_pred, t1_target, t2_target, t3_target = [], [], [], [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = []\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            yhat = model(x)\n",
    "            y1, y2, y3 = y[:, 0], y[:, 1], y[:, 2]\n",
    "            yhat_1, yhat_2, yhat_3 = yhat[0], yhat[1], yhat[2]\n",
    "\n",
    "            loss1 = bce_loss_fn(yhat_1, y1.view(-1, 1))\n",
    "            loss2 = mse_loss_fn(yhat_2, y2.view(-1, 1))\n",
    "            loss3 = mse_loss_fn(yhat_3, y3.view(-1, 1))\n",
    "            loss = loss1 + loss2 + loss3\n",
    "\n",
    "            t1_hat, t2_hat, t3_hat = list(yhat_1.cpu()), list(yhat_2.cpu()), list(yhat_3.cpu())\n",
    "\n",
    "            t1_pred += t1_hat\n",
    "            t2_pred += t2_hat\n",
    "            t3_pred += t3_hat\n",
    "            t1_target += list(y1.cpu())\n",
    "            t2_target += list(y2.cpu())\n",
    "            t3_target += list(y3.cpu())\n",
    "\n",
    "    # t1_pred = [1 if x else 0 for x in list(t1_pred)]\n",
    "    # t2_pred = [1 if x else 0 for x in list(t2_pred)]\n",
    "\n",
    "    auc_1 = roc_auc_score(t1_target, t1_pred)\n",
    "    # auc_2 = roc_auc_score(t2_target, t2_pred)\n",
    "    # auc_3 = roc_auc_score(t3_target, t3_pred)\n",
    "    return auc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22bfcdd9-cba7-4b10-af0e-012770c3eb06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_data, train_label, validation_data, validation_label, test_data, test_label, output_info = data_preparation()\n",
    "#print(output_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ca6cbe0-a2d5-4e03-b588-1554cf21cca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labels_np = df_labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb28e2-c9f3-4d10-b405-8dbc4234438e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72844cf-5d44-4eed-a735-a64a6aaa1064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6046b8e-079f-4fd7-9b60-481889c0a8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6d1252f-f052-4c94-96d4-327fbd6bd0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_label_tmp = np.column_stack((np.argmax(df_labels_np_tp[0]), np.argmax(df_labels_np_tp[1]), np.argmax(df_labels_np_tp[2])))\n",
    "df_dataset = getTensorDataset(df_transformed.to_numpy(), df_labels_np)\n",
    "train_size = int(0.8 * len(df_dataset))\n",
    "test_size = len(df_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset=df_dataset, lengths=[train_size, test_size], generator=torch.manual_seed(0))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "758a2f8b-f31a-4ff9-8852-ff8b6783be64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378961, 320)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10109850-0e3c-4a3d-9b7d-5e04a8061bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9f2659b-698f-4f97-be17-852ece6ee67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2b2712e-82b7-4b12-ab29-f1eec2d2795d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MMOE(input_size=320, num_experts=6, experts_out=16, experts_hidden=32, towers_hidden=8, tasks=3)\n",
    "model = model.to(device)\n",
    "lr = 1e-4\n",
    "n_epochs = 10\n",
    "bce_loss_fn = nn.BCELoss(reduction='mean')\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "losses = []\n",
    "val_loss = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7ddefaa-a243-4c2a-8f4e-6c642dcf029a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10\n",
      "train loss1: 17.30103, train loss2: 0.13399, train loss3: 1.76482\n",
      "train loss1: 17.40749, train loss2: 0.11996, train loss3: 2.16972\n",
      "train loss1: 18.95155, train loss2: 0.19151, train loss3: 2.59185\n",
      "train loss1: 18.23562, train loss2: 0.04386, train loss3: 0.25947\n",
      "train loss1: 16.43505, train loss2: 0.06076, train loss3: 1.69100\n",
      "train loss1: 18.36349, train loss2: 0.37047, train loss3: 1.51856\n",
      "train loss1: 17.30774, train loss2: 0.05799, train loss3: 0.85337\n",
      "train loss1: 16.16909, train loss2: 0.24172, train loss3: 4.57718\n",
      "train loss1: 17.91301, train loss2: 0.09147, train loss3: 0.96518\n",
      "train loss1: 15.67210, train loss2: 0.49436, train loss3: 1.65566\n",
      "train loss1: 14.61100, train loss2: 0.03607, train loss3: 2.14322\n",
      "train loss1: 14.61942, train loss2: 3.51073, train loss3: 0.18444\n",
      "train loss1: 16.26641, train loss2: 1.94085, train loss3: 0.54771\n",
      "train loss1: 15.78852, train loss2: 0.05364, train loss3: 27.09285\n",
      "train loss1: 16.92664, train loss2: 0.23138, train loss3: 0.15247\n",
      "train loss1: 13.89658, train loss2: 0.36594, train loss3: 0.86948\n",
      "train loss1: 13.74427, train loss2: 0.35966, train loss3: 0.50793\n",
      "train loss1: 14.30181, train loss2: 0.11855, train loss3: 2.94771\n",
      "train loss1: 14.74114, train loss2: 0.89057, train loss3: 0.44252\n",
      "train loss1: 15.23823, train loss2: 12.74020, train loss3: 0.19742\n",
      "train loss1: 13.60077, train loss2: 0.04702, train loss3: 2.18470\n",
      "train loss1: 15.52768, train loss2: 1.58459, train loss3: 0.39067\n",
      "train loss1: 14.18782, train loss2: 0.03970, train loss3: 0.43868\n",
      "train loss1: 15.15787, train loss2: 0.02380, train loss3: 0.25876\n",
      "train loss1: 14.59378, train loss2: 3.54093, train loss3: 0.16369\n",
      "train loss1: 14.87024, train loss2: 0.31953, train loss3: 1.12117\n",
      "train loss1: 11.99354, train loss2: 0.12044, train loss3: 0.47706\n",
      "train loss1: 14.02447, train loss2: 0.09472, train loss3: 0.58114\n",
      "train loss1: 14.46780, train loss2: 0.11491, train loss3: 0.52136\n",
      "train loss1: 12.27459, train loss2: 0.89254, train loss3: 8.13704\n",
      "train loss1: 14.13678, train loss2: 0.09861, train loss3: 0.18919\n",
      "train loss1: 16.25904, train loss2: 0.10553, train loss3: 0.14650\n",
      "train loss1: 12.55581, train loss2: 0.03254, train loss3: 2.39281\n",
      "train loss1: 14.61887, train loss2: 14.90289, train loss3: 1.94942\n",
      "train loss1: 14.28856, train loss2: 0.24594, train loss3: 1.00195\n",
      "train loss1: 13.94181, train loss2: 0.26166, train loss3: 1.10464\n",
      "train loss1: 13.35149, train loss2: 0.02543, train loss3: 0.05107\n",
      "train loss1: 12.88468, train loss2: 0.77617, train loss3: 0.55078\n",
      "train loss1: 11.82378, train loss2: 0.07508, train loss3: 0.63615\n",
      "train loss1: 14.22391, train loss2: 0.04961, train loss3: 1.29246\n",
      "train loss1: 12.80500, train loss2: 0.08492, train loss3: 1.70491\n",
      "train loss1: 14.56861, train loss2: 0.15359, train loss3: 0.40023\n",
      "train loss1: 13.00577, train loss2: 0.80844, train loss3: 1.92965\n",
      "train loss1: 13.34225, train loss2: 0.04043, train loss3: 0.18242\n",
      "train loss1: 13.30559, train loss2: 4.29517, train loss3: 0.30513\n",
      "train loss1: 12.26190, train loss2: 0.03812, train loss3: 7.49543\n",
      "train loss1: 14.89309, train loss2: 0.14688, train loss3: 0.06450\n",
      "train loss1: 12.65043, train loss2: 0.04664, train loss3: 0.37231\n",
      "train loss1: 13.01951, train loss2: 1.07503, train loss3: 2.41271\n",
      "train loss1: 12.70659, train loss2: 0.08336, train loss3: 0.27606\n",
      "train loss1: 14.07223, train loss2: 3.02100, train loss3: 0.90659\n",
      "train loss1: 12.54034, train loss2: 0.03512, train loss3: 0.88936\n",
      "train loss1: 12.48210, train loss2: 0.30212, train loss3: 1.64149\n",
      "train loss1: 13.49761, train loss2: 8.80010, train loss3: 3.91997\n",
      "train loss1: 13.31384, train loss2: 0.10933, train loss3: 12.20995\n",
      "train loss1: 13.54963, train loss2: 0.07230, train loss3: 0.18371\n",
      "train loss1: 10.90934, train loss2: 0.61361, train loss3: 0.18655\n",
      "train loss1: 12.55990, train loss2: 0.23720, train loss3: 0.30914\n",
      "train loss1: 13.22571, train loss2: 0.11777, train loss3: 0.40666\n",
      "train loss1: 13.34737, train loss2: 0.10392, train loss3: 0.38776\n",
      "train loss1: 12.11469, train loss2: 0.09240, train loss3: 0.43958\n",
      "train loss1: 11.90637, train loss2: 0.02426, train loss3: 0.38992\n",
      "train loss1: 11.67883, train loss2: 0.24649, train loss3: 0.12633\n",
      "train loss1: 11.79546, train loss2: 0.10311, train loss3: 0.46672\n",
      "train loss1: 12.79483, train loss2: 0.07043, train loss3: 0.51597\n",
      "train loss1: 13.20879, train loss2: 0.11416, train loss3: 3.10070\n",
      "train loss1: 11.22417, train loss2: 0.32269, train loss3: 1.90560\n",
      "train loss1: 11.96490, train loss2: 0.79752, train loss3: 0.89171\n",
      "train loss1: 11.88472, train loss2: 0.05249, train loss3: 7.37572\n",
      "train loss1: 11.10592, train loss2: 0.46243, train loss3: 9.80275\n",
      "train loss1: 12.42024, train loss2: 0.06931, train loss3: 5.30102\n",
      "train loss1: 11.48665, train loss2: 0.06900, train loss3: 0.98984\n",
      "train loss1: 10.73642, train loss2: 0.10784, train loss3: 0.95800\n",
      "train loss1: 11.68989, train loss2: 0.40778, train loss3: 0.86910\n",
      "train loss1: 13.47627, train loss2: 0.05923, train loss3: 2.14867\n",
      "train loss1: 11.53339, train loss2: 0.07146, train loss3: 0.38946\n",
      "train loss1: 11.48524, train loss2: 0.62642, train loss3: 0.25062\n",
      "train loss1: 11.65862, train loss2: 0.17102, train loss3: 0.53487\n",
      "train loss1: 12.60276, train loss2: 0.19283, train loss3: 0.57431\n",
      "train loss1: 11.11026, train loss2: 0.26020, train loss3: 0.23699\n",
      "train loss1: 9.68668, train loss2: 0.01409, train loss3: 0.98389\n",
      "train loss1: 10.47362, train loss2: 0.04304, train loss3: 0.28877\n",
      "train loss1: 10.86703, train loss2: 0.71522, train loss3: 0.52814\n",
      "train loss1: 8.95530, train loss2: 2.12924, train loss3: 0.29702\n",
      "train loss1: 8.81155, train loss2: 147.06575, train loss3: 2.98984\n",
      "train loss1: 10.31461, train loss2: 0.34178, train loss3: 2.08550\n",
      "train loss1: 12.51209, train loss2: 2.62440, train loss3: 0.31588\n",
      "train loss1: 10.65585, train loss2: 0.03446, train loss3: 0.25380\n",
      "train loss1: 10.99656, train loss2: 0.08009, train loss3: 0.25025\n",
      "train loss1: 10.52114, train loss2: 0.02870, train loss3: 0.39591\n",
      "train loss1: 10.73471, train loss2: 0.96347, train loss3: 16.92567\n",
      "train loss1: 11.62784, train loss2: 0.02870, train loss3: 0.65402\n",
      "train loss1: 9.25924, train loss2: 0.42680, train loss3: 0.13741\n",
      "train loss1: 10.29200, train loss2: 0.03970, train loss3: 0.63633\n",
      "train loss1: 8.22634, train loss2: 0.03692, train loss3: 0.93149\n",
      "train loss1: 10.76863, train loss2: 0.12585, train loss3: 0.74789\n",
      "train loss1: 9.88978, train loss2: 0.21892, train loss3: 0.13240\n",
      "train loss1: 8.76052, train loss2: 0.15791, train loss3: 0.51358\n",
      "train loss1: 9.81389, train loss2: 0.09847, train loss3: 0.86407\n",
      "train loss1: 8.82692, train loss2: 0.82551, train loss3: 5.49900\n",
      "train loss1: 8.05031, train loss2: 0.05983, train loss3: 0.79698\n",
      "train loss1: 9.08697, train loss2: 0.46593, train loss3: 0.78042\n",
      "train loss1: 9.26976, train loss2: 0.06066, train loss3: 0.41441\n",
      "train loss1: 8.48055, train loss2: 0.03229, train loss3: 2.68564\n",
      "train loss1: 9.51710, train loss2: 0.04554, train loss3: 0.71398\n",
      "train loss1: 11.29125, train loss2: 0.05985, train loss3: 2.13547\n",
      "train loss1: 9.85245, train loss2: 0.07772, train loss3: 0.59764\n",
      "train loss1: 9.54747, train loss2: 0.26261, train loss3: 5.04324\n",
      "train loss1: 8.99338, train loss2: 0.01587, train loss3: 0.80770\n",
      "train loss1: 9.50504, train loss2: 0.05223, train loss3: 1.44349\n",
      "train loss1: 8.99461, train loss2: 0.03351, train loss3: 0.73940\n",
      "train loss1: 9.26357, train loss2: 0.04792, train loss3: 0.30608\n",
      "train loss1: 9.04035, train loss2: 0.12056, train loss3: 0.13588\n",
      "train loss1: 9.36377, train loss2: 0.37950, train loss3: 7.96929\n",
      "train loss1: 8.31390, train loss2: 1.65042, train loss3: 1.51964\n",
      "train loss1: 9.69478, train loss2: 0.06353, train loss3: 0.89176\n",
      "train loss1: 8.71750, train loss2: 0.03109, train loss3: 0.47007\n",
      "train loss1: 8.78399, train loss2: 0.45406, train loss3: 5.80350\n",
      "train loss1: 8.66158, train loss2: 0.16332, train loss3: 27.78949\n",
      "train loss1: 7.30982, train loss2: 0.21582, train loss3: 0.11970\n",
      "train loss1: 8.50507, train loss2: 0.06553, train loss3: 1.06039\n",
      "train loss1: 9.38354, train loss2: 0.05943, train loss3: 1.06381\n",
      "train loss1: 8.15095, train loss2: 0.14122, train loss3: 3.59691\n",
      "train loss1: 7.16158, train loss2: 0.05996, train loss3: 0.38469\n",
      "train loss1: 7.43394, train loss2: 0.01591, train loss3: 2.72846\n",
      "train loss1: 7.48610, train loss2: 0.04434, train loss3: 0.34169\n",
      "train loss1: 7.23898, train loss2: 0.06226, train loss3: 0.11959\n",
      "train loss1: 8.30387, train loss2: 0.07074, train loss3: 0.25798\n",
      "train loss1: 7.13322, train loss2: 192.45750, train loss3: 0.20296\n",
      "train loss1: 6.88109, train loss2: 0.07520, train loss3: 1.29858\n",
      "train loss1: 8.33878, train loss2: 1.79059, train loss3: 1.84821\n",
      "train loss1: 6.89625, train loss2: 0.03515, train loss3: 0.77520\n",
      "train loss1: 7.48185, train loss2: 0.06920, train loss3: 0.37015\n",
      "train loss1: 9.19263, train loss2: 0.06825, train loss3: 2.91791\n",
      "train loss1: 6.95683, train loss2: 0.41245, train loss3: 0.39079\n",
      "train loss1: 6.59233, train loss2: 0.13968, train loss3: 1.13155\n",
      "train loss1: 8.13805, train loss2: 0.42312, train loss3: 11.92939\n",
      "train loss1: 7.76155, train loss2: 0.06004, train loss3: 0.39713\n",
      "train loss1: 7.41472, train loss2: 0.69103, train loss3: 0.76448\n",
      "train loss1: 8.41658, train loss2: 0.70114, train loss3: 0.41864\n",
      "train loss1: 8.46033, train loss2: 0.08259, train loss3: 2.10546\n",
      "train loss1: 7.49590, train loss2: 0.12371, train loss3: 139.44902\n",
      "train loss1: 6.51744, train loss2: 3.47968, train loss3: 0.66412\n",
      "train loss1: 6.02870, train loss2: 0.04352, train loss3: 0.21432\n",
      "train loss1: 7.04003, train loss2: 0.06969, train loss3: 0.79074\n",
      "train loss1: 7.56561, train loss2: 48.89745, train loss3: 0.51644\n",
      "train loss1: 9.46723, train loss2: 0.09067, train loss3: 0.13124\n",
      "train loss1: 6.00316, train loss2: 2.19868, train loss3: 0.13295\n",
      "train loss1: 6.89691, train loss2: 0.22537, train loss3: 1.31249\n",
      "train loss1: 7.38391, train loss2: 0.13850, train loss3: 0.53872\n",
      "train loss1: 8.41137, train loss2: 0.14990, train loss3: 0.10546\n",
      "train loss1: 6.80838, train loss2: 0.06878, train loss3: 2.59866\n",
      "train loss1: 6.78373, train loss2: 1.70199, train loss3: 3.30665\n",
      "train loss1: 6.76196, train loss2: 0.02560, train loss3: 0.31170\n",
      "train loss1: 7.82417, train loss2: 0.34436, train loss3: 1.11732\n",
      "train loss1: 5.31913, train loss2: 0.04156, train loss3: 0.37180\n",
      "train loss1: 7.62350, train loss2: 0.03382, train loss3: 5.16178\n",
      "train loss1: 6.99230, train loss2: 0.20146, train loss3: 0.22503\n",
      "train loss1: 6.66061, train loss2: 0.06173, train loss3: 1.44396\n",
      "train loss1: 7.85780, train loss2: 0.03781, train loss3: 5.79574\n",
      "train loss1: 8.49256, train loss2: 0.28213, train loss3: 0.76532\n",
      "train loss1: 5.75030, train loss2: 0.03995, train loss3: 1.11512\n",
      "train loss1: 5.93721, train loss2: 0.22375, train loss3: 0.59498\n",
      "train loss1: 6.59626, train loss2: 0.09694, train loss3: 0.18133\n",
      "train loss1: 6.17322, train loss2: 0.06426, train loss3: 0.65986\n",
      "train loss1: 6.18099, train loss2: 0.03405, train loss3: 3.87507\n",
      "train loss1: 7.16425, train loss2: 0.24450, train loss3: 0.45833\n",
      "train loss1: 6.18837, train loss2: 0.13341, train loss3: 0.52158\n",
      "train loss1: 5.55302, train loss2: 0.11328, train loss3: 0.21716\n",
      "train loss1: 5.42846, train loss2: 0.02884, train loss3: 4.74474\n",
      "train loss1: 6.37028, train loss2: 0.02626, train loss3: 0.62339\n",
      "train loss1: 5.71925, train loss2: 0.06933, train loss3: 2.19641\n",
      "train loss1: 5.66528, train loss2: 0.12099, train loss3: 0.73711\n",
      "train loss1: 5.65135, train loss2: 0.53846, train loss3: 0.78749\n",
      "train loss1: 5.10636, train loss2: 0.05600, train loss3: 2.05753\n",
      "train loss1: 5.63263, train loss2: 0.10338, train loss3: 0.29401\n",
      "train loss1: 7.12692, train loss2: 1.43892, train loss3: 0.41958\n",
      "train loss1: 5.40990, train loss2: 0.02887, train loss3: 0.64170\n",
      "train loss1: 6.01616, train loss2: 0.08361, train loss3: 0.72644\n",
      "train loss1: 5.66402, train loss2: 0.15943, train loss3: 0.36695\n",
      "train loss1: 5.58617, train loss2: 1.95482, train loss3: 2.32638\n",
      "train loss1: 6.27309, train loss2: 0.30559, train loss3: 0.14995\n",
      "train loss1: 6.33551, train loss2: 0.03410, train loss3: 0.26667\n",
      "train loss1: 5.77041, train loss2: 0.03108, train loss3: 0.12292\n",
      "train loss1: 6.74266, train loss2: 0.06359, train loss3: 1.05820\n",
      "train loss1: 6.17955, train loss2: 0.03742, train loss3: 1.51460\n",
      "train loss1: 6.61453, train loss2: 111.87734, train loss3: 0.77856\n",
      "train loss1: 5.27728, train loss2: 0.05165, train loss3: 0.79328\n",
      "train loss1: 5.54948, train loss2: 0.08347, train loss3: 1.16158\n",
      "train loss1: 5.81412, train loss2: 2.67040, train loss3: 1.69819\n",
      "train loss1: 5.53734, train loss2: 0.36651, train loss3: 2.60533\n",
      "train loss1: 6.72237, train loss2: 3.79368, train loss3: 1.49431\n",
      "train loss1: 4.75990, train loss2: 0.04852, train loss3: 0.20971\n",
      "train loss1: 6.41644, train loss2: 0.12593, train loss3: 0.53791\n",
      "train loss1: 5.28008, train loss2: 0.02314, train loss3: 0.56934\n",
      "train loss1: 4.38525, train loss2: 0.06904, train loss3: 26.28121\n",
      "train loss1: 5.30617, train loss2: 0.08993, train loss3: 2.19055\n",
      "train loss1: 5.37470, train loss2: 0.26316, train loss3: 0.22684\n",
      "train loss1: 5.93392, train loss2: 0.10657, train loss3: 0.55301\n",
      "train loss1: 5.34756, train loss2: 0.04336, train loss3: 17.64220\n",
      "train loss1: 5.46491, train loss2: 1.43650, train loss3: 0.69524\n",
      "train loss1: 6.22986, train loss2: 0.45706, train loss3: 0.94865\n",
      "train loss1: 4.89805, train loss2: 0.56557, train loss3: 2.55310\n",
      "train loss1: 6.04288, train loss2: 0.03968, train loss3: 9.89756\n",
      "train loss1: 5.67209, train loss2: 0.03042, train loss3: 3.79517\n",
      "train loss1: 5.65618, train loss2: 0.04403, train loss3: 0.19391\n",
      "train loss1: 4.99124, train loss2: 0.30026, train loss3: 1.09413\n",
      "train loss1: 5.66864, train loss2: 0.12288, train loss3: 1.12701\n",
      "train loss1: 4.34335, train loss2: 0.18398, train loss3: 23.38218\n",
      "train loss1: 5.42219, train loss2: 0.34695, train loss3: 0.93004\n",
      "train loss1: 5.26196, train loss2: 0.03283, train loss3: 1.72849\n",
      "train loss1: 4.66436, train loss2: 0.07932, train loss3: 9.76018\n",
      "train loss1: 5.12468, train loss2: 0.69144, train loss3: 1.35044\n",
      "train loss1: 5.20697, train loss2: 0.75996, train loss3: 0.60448\n",
      "train loss1: 5.92060, train loss2: 46.85927, train loss3: 0.99853\n",
      "train loss1: 4.57316, train loss2: 0.13354, train loss3: 2.64590\n",
      "train loss1: 5.24972, train loss2: 0.04713, train loss3: 0.39177\n",
      "train loss1: 4.46837, train loss2: 0.06266, train loss3: 3.08060\n",
      "train loss1: 4.87885, train loss2: 0.05728, train loss3: 0.16526\n",
      "train loss1: 5.25267, train loss2: 0.02642, train loss3: 0.33417\n",
      "train loss1: 4.11744, train loss2: 0.06983, train loss3: 0.22837\n",
      "train loss1: 5.39039, train loss2: 0.22609, train loss3: 12.13746\n",
      "train loss1: 5.44826, train loss2: 0.16976, train loss3: 0.57939\n",
      "train loss1: 5.64670, train loss2: 12.29066, train loss3: 6.64329\n",
      "train loss1: 4.74407, train loss2: 0.28593, train loss3: 0.31171\n",
      "train loss1: 4.71643, train loss2: 0.02747, train loss3: 20.11978\n",
      "train loss1: 3.74571, train loss2: 0.03479, train loss3: 1.67238\n",
      "train loss1: 3.98218, train loss2: 45.40768, train loss3: 0.33587\n",
      "train loss1: 4.35596, train loss2: 0.02107, train loss3: 1.75335\n",
      "train loss1: 4.50080, train loss2: 0.25448, train loss3: 0.36140\n",
      "train loss1: 3.38699, train loss2: 0.08747, train loss3: 3.05128\n",
      "train loss1: 3.42390, train loss2: 0.06091, train loss3: 1.81521\n",
      "train loss1: 3.90215, train loss2: 0.05491, train loss3: 0.55794\n",
      "train loss1: 3.66776, train loss2: 0.11769, train loss3: 0.34655\n",
      "train loss1: 4.26786, train loss2: 2.21823, train loss3: 9.49776\n",
      "train loss1: 2.59516, train loss2: 0.05381, train loss3: 1.36302\n",
      "train loss1: 3.57586, train loss2: 0.19078, train loss3: 0.52225\n",
      "train loss1: 2.95439, train loss2: 0.16240, train loss3: 0.27482\n",
      "train loss1: 3.04995, train loss2: 0.49504, train loss3: 54.83620\n",
      "train loss1: 3.24727, train loss2: 0.16938, train loss3: 0.45856\n",
      "train loss1: 3.23916, train loss2: 0.39245, train loss3: 1.40079\n",
      "train loss1: 3.07227, train loss2: 0.20109, train loss3: 0.84438\n",
      "train loss1: 2.54182, train loss2: 4.32190, train loss3: 0.42227\n",
      "train loss1: 2.94124, train loss2: 0.11636, train loss3: 0.53308\n",
      "train loss1: 2.39755, train loss2: 0.04918, train loss3: 0.70824\n",
      "train loss1: 1.78938, train loss2: 2.03074, train loss3: 0.44865\n",
      "train loss1: 2.14478, train loss2: 0.04050, train loss3: 0.20628\n",
      "train loss1: 3.04329, train loss2: 0.14039, train loss3: 0.42545\n",
      "train loss1: 2.26661, train loss2: 0.89034, train loss3: 1.04462\n",
      "train loss1: 2.46621, train loss2: 0.14587, train loss3: 0.64377\n",
      "train loss1: 2.59266, train loss2: 0.07286, train loss3: 0.82313\n",
      "train loss1: 1.81315, train loss2: 0.04502, train loss3: 0.27661\n",
      "train loss1: 2.12154, train loss2: 0.03702, train loss3: 0.35228\n",
      "train loss1: 1.95717, train loss2: 0.15725, train loss3: 0.35473\n",
      "train loss1: 2.13252, train loss2: 0.12914, train loss3: 1.08297\n",
      "train loss1: 1.58962, train loss2: 0.10216, train loss3: 7.08375\n",
      "train loss1: 1.53671, train loss2: 0.08145, train loss3: 0.25186\n",
      "train loss1: 1.94122, train loss2: 0.14782, train loss3: 1.36605\n",
      "train loss1: 1.85270, train loss2: 0.38604, train loss3: 0.23091\n",
      "train loss1: 1.73447, train loss2: 0.11247, train loss3: 1.04807\n",
      "train loss1: 1.49526, train loss2: 0.44262, train loss3: 0.15563\n",
      "train loss1: 1.43733, train loss2: 0.15923, train loss3: 238.96983\n",
      "train loss1: 1.93314, train loss2: 0.51819, train loss3: 0.15273\n",
      "train loss1: 1.32587, train loss2: 0.02612, train loss3: 15.76679\n",
      "train loss1: 1.37737, train loss2: 0.21986, train loss3: 25.99287\n",
      "train loss1: 2.17705, train loss2: 0.12638, train loss3: 11.88054\n",
      "train loss1: 1.48111, train loss2: 0.04094, train loss3: 0.27538\n",
      "train loss1: 1.32017, train loss2: 0.38978, train loss3: 1.54451\n",
      "train loss1: 1.65062, train loss2: 0.06426, train loss3: 0.45501\n",
      "train loss1: 1.77456, train loss2: 0.03554, train loss3: 0.83717\n",
      "train loss1: 1.92286, train loss2: 0.06437, train loss3: 5.97527\n",
      "train loss1: 1.25969, train loss2: 0.09139, train loss3: 0.25781\n",
      "train loss1: 1.64623, train loss2: 0.07008, train loss3: 28.05021\n",
      "train loss1: 1.13806, train loss2: 0.30626, train loss3: 0.54317\n",
      "train loss1: 1.49755, train loss2: 0.08927, train loss3: 2.24098\n",
      "train loss1: 1.89200, train loss2: 0.05766, train loss3: 0.75651\n",
      "train loss1: 1.16959, train loss2: 0.06311, train loss3: 0.35750\n",
      "train loss1: 1.64215, train loss2: 0.14878, train loss3: 0.33964\n",
      "train loss1: 1.80394, train loss2: 0.02817, train loss3: 0.57132\n",
      "train loss1: 1.38316, train loss2: 0.10680, train loss3: 2.27967\n",
      "train loss1: 1.48387, train loss2: 0.06451, train loss3: 0.11919\n",
      "train loss1: 1.32121, train loss2: 0.23067, train loss3: 0.26000\n",
      "train loss1: 1.40869, train loss2: 0.15865, train loss3: 0.94381\n",
      "train loss1: 1.57828, train loss2: 0.06943, train loss3: 0.21440\n",
      "train loss1: 1.65571, train loss2: 0.03261, train loss3: 16.22476\n",
      "train loss1: 1.19978, train loss2: 0.12996, train loss3: 1.49728\n",
      "train loss1: 1.51719, train loss2: 0.04264, train loss3: 1.43013\n",
      "train loss1: 1.39964, train loss2: 0.04458, train loss3: 0.60138\n",
      "train loss1: 1.47398, train loss2: 0.17759, train loss3: 2.26992\n",
      "train loss1: 1.24893, train loss2: 0.05212, train loss3: 0.12787\n",
      "train loss1: 1.16590, train loss2: 0.09240, train loss3: 2.85648\n",
      "train loss1: 1.76656, train loss2: 0.06364, train loss3: 0.51956\n",
      "train loss1: 1.11489, train loss2: 0.05499, train loss3: 11.92359\n",
      "train loss1: 0.95046, train loss2: 3.59184, train loss3: 0.65265\n",
      "train loss1: 1.34763, train loss2: 0.08561, train loss3: 0.94764\n",
      "train loss1: 1.70435, train loss2: 0.02959, train loss3: 0.57133\n",
      "train loss1: 0.82982, train loss2: 0.00846, train loss3: 2946.66113\n",
      "Epoch: 1/10\n",
      "train loss1: 1.62012, train loss2: 0.05365, train loss3: 0.51611\n",
      "train loss1: 1.29249, train loss2: 0.23893, train loss3: 0.30263\n",
      "train loss1: 1.06948, train loss2: 0.19778, train loss3: 19.67303\n",
      "train loss1: 1.55735, train loss2: 0.12280, train loss3: 0.25979\n",
      "train loss1: 1.34075, train loss2: 0.72212, train loss3: 0.40893\n",
      "train loss1: 1.18148, train loss2: 0.16467, train loss3: 0.13114\n",
      "train loss1: 0.92008, train loss2: 0.12901, train loss3: 1.77440\n",
      "train loss1: 1.45700, train loss2: 0.07702, train loss3: 0.46029\n",
      "train loss1: 1.26476, train loss2: 0.15337, train loss3: 27.04358\n",
      "train loss1: 1.27432, train loss2: 0.28220, train loss3: 1.12964\n",
      "train loss1: 1.31795, train loss2: 0.02959, train loss3: 1.24588\n",
      "train loss1: 1.21786, train loss2: 0.44317, train loss3: 3.54569\n",
      "train loss1: 1.18727, train loss2: 0.05534, train loss3: 2.19707\n",
      "train loss1: 1.13793, train loss2: 0.28140, train loss3: 0.40589\n",
      "train loss1: 1.40101, train loss2: 0.18929, train loss3: 2.58716\n",
      "train loss1: 1.21677, train loss2: 0.04680, train loss3: 0.46282\n",
      "train loss1: 1.37433, train loss2: 0.28590, train loss3: 1.93255\n",
      "train loss1: 1.33457, train loss2: 0.33218, train loss3: 0.21849\n",
      "train loss1: 1.25015, train loss2: 0.80224, train loss3: 0.81736\n",
      "train loss1: 1.12643, train loss2: 0.05175, train loss3: 0.89239\n",
      "train loss1: 1.41867, train loss2: 0.08081, train loss3: 2.57153\n",
      "train loss1: 1.18768, train loss2: 0.08987, train loss3: 0.27531\n",
      "train loss1: 1.37007, train loss2: 0.04200, train loss3: 0.31136\n",
      "train loss1: 1.15191, train loss2: 0.31450, train loss3: 0.23908\n",
      "train loss1: 1.42775, train loss2: 2.02574, train loss3: 1.59582\n",
      "train loss1: 1.39247, train loss2: 14.84288, train loss3: 1.93135\n",
      "train loss1: 1.35956, train loss2: 0.02731, train loss3: 1.33660\n",
      "train loss1: 1.43253, train loss2: 0.06667, train loss3: 1.14842\n",
      "train loss1: 0.86018, train loss2: 0.15178, train loss3: 0.18619\n",
      "train loss1: 1.18972, train loss2: 0.07774, train loss3: 0.68736\n",
      "train loss1: 0.99400, train loss2: 0.03730, train loss3: 0.12011\n",
      "train loss1: 1.33104, train loss2: 1.23291, train loss3: 1.03155\n",
      "train loss1: 1.57613, train loss2: 0.04367, train loss3: 0.53719\n",
      "train loss1: 1.08914, train loss2: 0.11150, train loss3: 0.22089\n",
      "train loss1: 1.12310, train loss2: 0.09884, train loss3: 1.79386\n",
      "train loss1: 1.19317, train loss2: 0.08687, train loss3: 13.29181\n",
      "train loss1: 1.44115, train loss2: 0.07631, train loss3: 1.11108\n",
      "train loss1: 1.16355, train loss2: 0.04604, train loss3: 0.67397\n",
      "train loss1: 1.20791, train loss2: 0.13256, train loss3: 2.20028\n",
      "train loss1: 1.03902, train loss2: 0.19000, train loss3: 9.73403\n",
      "train loss1: 1.06672, train loss2: 0.06759, train loss3: 1.60820\n",
      "train loss1: 1.17291, train loss2: 0.16873, train loss3: 237.81705\n",
      "train loss1: 1.09497, train loss2: 0.28266, train loss3: 0.30936\n",
      "train loss1: 0.79813, train loss2: 0.02996, train loss3: 0.50295\n",
      "train loss1: 1.06396, train loss2: 0.06229, train loss3: 0.35653\n",
      "train loss1: 1.31017, train loss2: 0.04531, train loss3: 3.37037\n",
      "train loss1: 0.94152, train loss2: 0.07395, train loss3: 2.60119\n",
      "train loss1: 1.15831, train loss2: 0.35704, train loss3: 0.45728\n",
      "train loss1: 1.12559, train loss2: 0.10315, train loss3: 0.35469\n",
      "train loss1: 1.00413, train loss2: 0.07587, train loss3: 0.17528\n",
      "train loss1: 0.91023, train loss2: 2.26661, train loss3: 2.43176\n",
      "train loss1: 0.99912, train loss2: 0.03376, train loss3: 2.20145\n",
      "train loss1: 1.43180, train loss2: 0.05282, train loss3: 0.49603\n",
      "train loss1: 1.09046, train loss2: 0.07580, train loss3: 2.19258\n",
      "train loss1: 0.94493, train loss2: 0.05212, train loss3: 0.38466\n",
      "train loss1: 1.24554, train loss2: 0.27924, train loss3: 0.73557\n",
      "train loss1: 1.12872, train loss2: 0.27510, train loss3: 0.43823\n",
      "train loss1: 1.29189, train loss2: 0.05282, train loss3: 1.47672\n",
      "train loss1: 0.93719, train loss2: 2.89973, train loss3: 0.17076\n",
      "train loss1: 1.09313, train loss2: 0.02577, train loss3: 0.39952\n",
      "train loss1: 0.92065, train loss2: 0.08610, train loss3: 0.47505\n",
      "train loss1: 1.18430, train loss2: 0.19322, train loss3: 0.26449\n",
      "train loss1: 0.84977, train loss2: 0.02777, train loss3: 29.74100\n",
      "train loss1: 1.29109, train loss2: 0.03922, train loss3: 0.37081\n",
      "train loss1: 1.09979, train loss2: 0.93323, train loss3: 0.43954\n",
      "train loss1: 0.91741, train loss2: 0.38227, train loss3: 0.86650\n",
      "train loss1: 0.98714, train loss2: 2.13439, train loss3: 3.46726\n",
      "train loss1: 1.15684, train loss2: 1.66769, train loss3: 5.00501\n",
      "train loss1: 1.21747, train loss2: 0.21080, train loss3: 0.61195\n",
      "train loss1: 0.87771, train loss2: 0.14358, train loss3: 0.41074\n",
      "train loss1: 1.16083, train loss2: 0.70080, train loss3: 1.11829\n",
      "train loss1: 1.38859, train loss2: 0.09300, train loss3: 0.22496\n",
      "train loss1: 1.57571, train loss2: 4.43464, train loss3: 1.45026\n",
      "train loss1: 1.13790, train loss2: 0.04036, train loss3: 0.46611\n",
      "train loss1: 1.48640, train loss2: 0.05525, train loss3: 0.43498\n",
      "train loss1: 1.01382, train loss2: 0.04947, train loss3: 0.20853\n",
      "train loss1: 1.12407, train loss2: 0.73673, train loss3: 1.43778\n",
      "train loss1: 1.07163, train loss2: 1.74088, train loss3: 0.24856\n",
      "train loss1: 0.98117, train loss2: 2.14328, train loss3: 0.25590\n",
      "train loss1: 1.11512, train loss2: 0.03370, train loss3: 0.67670\n",
      "train loss1: 1.31475, train loss2: 8.80285, train loss3: 0.06595\n",
      "train loss1: 1.00772, train loss2: 0.08353, train loss3: 4.78642\n",
      "train loss1: 0.75528, train loss2: 0.03554, train loss3: 0.24792\n",
      "train loss1: 1.08591, train loss2: 0.03280, train loss3: 0.29075\n",
      "train loss1: 1.17881, train loss2: 11.37957, train loss3: 0.89340\n",
      "train loss1: 1.03703, train loss2: 0.43355, train loss3: 1.08629\n",
      "train loss1: 1.08122, train loss2: 0.05387, train loss3: 5.09493\n",
      "train loss1: 1.02343, train loss2: 0.08034, train loss3: 0.22795\n",
      "train loss1: 1.02928, train loss2: 0.02334, train loss3: 1.40935\n",
      "train loss1: 0.86347, train loss2: 0.88834, train loss3: 7.43069\n",
      "train loss1: 1.07989, train loss2: 0.07355, train loss3: 0.85054\n",
      "train loss1: 0.77791, train loss2: 0.07444, train loss3: 0.26437\n",
      "train loss1: 0.99306, train loss2: 0.10882, train loss3: 0.21051\n",
      "train loss1: 0.95373, train loss2: 0.02495, train loss3: 1.38667\n",
      "train loss1: 1.35479, train loss2: 0.62592, train loss3: 0.73556\n",
      "train loss1: 1.04440, train loss2: 0.38292, train loss3: 0.14210\n",
      "train loss1: 1.02875, train loss2: 0.06737, train loss3: 0.76157\n",
      "train loss1: 0.80837, train loss2: 0.04088, train loss3: 0.17063\n",
      "train loss1: 0.93734, train loss2: 0.06824, train loss3: 0.87973\n",
      "train loss1: 1.06057, train loss2: 0.06381, train loss3: 0.79461\n",
      "train loss1: 1.26133, train loss2: 0.48767, train loss3: 8.45674\n",
      "train loss1: 0.77557, train loss2: 0.84680, train loss3: 0.37014\n",
      "train loss1: 0.86633, train loss2: 0.02802, train loss3: 0.71885\n",
      "train loss1: 0.90749, train loss2: 0.03186, train loss3: 4.54249\n",
      "train loss1: 0.91432, train loss2: 0.08491, train loss3: 1.23181\n",
      "train loss1: 0.73509, train loss2: 0.05357, train loss3: 1.29464\n",
      "train loss1: 0.95924, train loss2: 0.03098, train loss3: 0.47509\n",
      "train loss1: 0.93140, train loss2: 0.09286, train loss3: 0.37862\n",
      "train loss1: 1.04838, train loss2: 0.03279, train loss3: 0.05539\n",
      "train loss1: 1.10376, train loss2: 0.44515, train loss3: 0.18255\n",
      "train loss1: 1.11916, train loss2: 0.16092, train loss3: 0.16940\n",
      "train loss1: 1.00578, train loss2: 0.08679, train loss3: 0.26146\n",
      "train loss1: 0.98138, train loss2: 0.20027, train loss3: 1.10831\n",
      "train loss1: 1.10737, train loss2: 0.04651, train loss3: 0.28703\n",
      "train loss1: 1.15079, train loss2: 0.03110, train loss3: 1.06288\n",
      "train loss1: 1.24219, train loss2: 0.07810, train loss3: 1.31071\n",
      "train loss1: 1.01972, train loss2: 0.19596, train loss3: 1.64043\n",
      "train loss1: 0.90708, train loss2: 0.20782, train loss3: 0.41513\n",
      "train loss1: 0.86441, train loss2: 0.03927, train loss3: 0.26874\n",
      "train loss1: 0.93681, train loss2: 0.68767, train loss3: 0.42609\n",
      "train loss1: 0.72902, train loss2: 0.03931, train loss3: 0.74459\n",
      "train loss1: 0.97457, train loss2: 0.02223, train loss3: 0.14291\n",
      "train loss1: 0.92790, train loss2: 0.03174, train loss3: 5.41563\n",
      "train loss1: 1.06093, train loss2: 0.18368, train loss3: 0.81997\n",
      "train loss1: 0.96079, train loss2: 2.01087, train loss3: 0.48977\n",
      "train loss1: 0.97372, train loss2: 0.12689, train loss3: 0.18130\n",
      "train loss1: 1.11337, train loss2: 0.19411, train loss3: 1.22003\n",
      "train loss1: 0.95960, train loss2: 0.12173, train loss3: 1.80457\n",
      "train loss1: 1.02322, train loss2: 0.03209, train loss3: 0.20179\n",
      "train loss1: 1.07222, train loss2: 0.10403, train loss3: 0.13290\n",
      "train loss1: 0.73534, train loss2: 0.02229, train loss3: 0.62444\n",
      "train loss1: 0.84755, train loss2: 0.02121, train loss3: 0.24606\n",
      "train loss1: 1.13195, train loss2: 0.05952, train loss3: 0.62908\n",
      "train loss1: 0.81261, train loss2: 0.06713, train loss3: 0.14188\n",
      "train loss1: 0.86975, train loss2: 2.64551, train loss3: 3.04951\n",
      "train loss1: 1.11533, train loss2: 0.03234, train loss3: 0.41395\n",
      "train loss1: 1.03755, train loss2: 0.62481, train loss3: 0.16108\n",
      "train loss1: 1.22106, train loss2: 0.07997, train loss3: 0.17027\n",
      "train loss1: 0.77850, train loss2: 0.10253, train loss3: 0.68960\n",
      "train loss1: 0.71021, train loss2: 46.97627, train loss3: 0.79564\n",
      "train loss1: 0.98717, train loss2: 0.08044, train loss3: 0.23199\n",
      "train loss1: 1.06385, train loss2: 0.44849, train loss3: 4.31875\n",
      "train loss1: 0.92106, train loss2: 0.04152, train loss3: 1.36586\n",
      "train loss1: 1.33209, train loss2: 1.15290, train loss3: 0.71974\n",
      "train loss1: 0.93411, train loss2: 0.04043, train loss3: 15.21021\n",
      "train loss1: 1.05159, train loss2: 0.86065, train loss3: 1.22478\n",
      "train loss1: 0.82117, train loss2: 0.04444, train loss3: 20.31615\n",
      "train loss1: 0.87715, train loss2: 4.27084, train loss3: 2.53880\n",
      "train loss1: 0.96482, train loss2: 0.06673, train loss3: 0.68127\n",
      "train loss1: 1.06681, train loss2: 0.24859, train loss3: 0.61263\n",
      "train loss1: 0.74827, train loss2: 0.16315, train loss3: 0.66591\n",
      "train loss1: 0.80053, train loss2: 0.07842, train loss3: 0.73236\n",
      "train loss1: 0.81663, train loss2: 1.32334, train loss3: 1.48299\n",
      "train loss1: 0.94459, train loss2: 0.10176, train loss3: 0.48869\n",
      "train loss1: 1.16764, train loss2: 0.04325, train loss3: 6.04603\n",
      "train loss1: 1.23814, train loss2: 0.03315, train loss3: 9.77821\n",
      "train loss1: 0.83931, train loss2: 192.53720, train loss3: 0.83168\n",
      "train loss1: 0.81486, train loss2: 0.08028, train loss3: 0.84380\n",
      "train loss1: 0.95114, train loss2: 111.88871, train loss3: 2.07753\n",
      "train loss1: 0.76048, train loss2: 0.05779, train loss3: 0.67473\n",
      "train loss1: 0.85868, train loss2: 0.10417, train loss3: 5.13801\n",
      "train loss1: 0.94761, train loss2: 1.39371, train loss3: 1.28159\n",
      "train loss1: 0.98845, train loss2: 3.90403, train loss3: 0.90725\n",
      "train loss1: 0.82706, train loss2: 0.03962, train loss3: 1.21613\n",
      "train loss1: 1.07554, train loss2: 0.08693, train loss3: 0.12074\n",
      "train loss1: 0.74383, train loss2: 0.15904, train loss3: 0.25664\n",
      "train loss1: 0.98619, train loss2: 0.02622, train loss3: 4.17796\n",
      "train loss1: 0.82633, train loss2: 0.07463, train loss3: 0.24336\n",
      "train loss1: 1.31662, train loss2: 0.03549, train loss3: 0.70827\n",
      "train loss1: 0.74486, train loss2: 0.12518, train loss3: 0.82809\n",
      "train loss1: 1.04115, train loss2: 0.50249, train loss3: 0.79598\n",
      "train loss1: 0.78276, train loss2: 0.05114, train loss3: 3.25370\n",
      "train loss1: 0.96014, train loss2: 0.17643, train loss3: 0.30063\n",
      "train loss1: 0.85366, train loss2: 0.03590, train loss3: 0.60694\n",
      "train loss1: 0.87707, train loss2: 0.08473, train loss3: 0.68743\n",
      "train loss1: 0.83315, train loss2: 0.21357, train loss3: 2.16170\n",
      "train loss1: 0.86992, train loss2: 0.16273, train loss3: 0.46768\n",
      "train loss1: 0.87281, train loss2: 0.26389, train loss3: 0.36509\n",
      "train loss1: 0.74306, train loss2: 0.02584, train loss3: 1.38199\n",
      "train loss1: 0.90585, train loss2: 0.50506, train loss3: 2.71924\n",
      "train loss1: 0.83488, train loss2: 0.07803, train loss3: 0.60650\n",
      "train loss1: 0.94534, train loss2: 0.06539, train loss3: 0.94914\n",
      "train loss1: 0.87827, train loss2: 0.49655, train loss3: 0.85595\n",
      "train loss1: 0.82338, train loss2: 0.46886, train loss3: 0.22995\n",
      "train loss1: 0.97510, train loss2: 0.06151, train loss3: 1.30848\n",
      "train loss1: 0.99124, train loss2: 0.11428, train loss3: 4.28061\n",
      "train loss1: 0.75244, train loss2: 0.03668, train loss3: 0.24847\n",
      "train loss1: 0.73395, train loss2: 0.28466, train loss3: 0.55852\n",
      "train loss1: 0.87612, train loss2: 1.25060, train loss3: 0.44182\n",
      "train loss1: 0.78517, train loss2: 0.09443, train loss3: 0.83806\n",
      "train loss1: 0.75291, train loss2: 0.08034, train loss3: 1.34732\n",
      "train loss1: 0.85054, train loss2: 0.05710, train loss3: 0.56248\n",
      "train loss1: 0.82958, train loss2: 0.04511, train loss3: 0.15584\n",
      "train loss1: 0.82834, train loss2: 0.04913, train loss3: 55.32039\n",
      "train loss1: 0.83021, train loss2: 0.17096, train loss3: 0.68026\n",
      "train loss1: 0.76888, train loss2: 0.08626, train loss3: 2.24655\n",
      "train loss1: 0.82133, train loss2: 0.05539, train loss3: 3.84845\n",
      "train loss1: 1.12210, train loss2: 0.05169, train loss3: 19.50887\n",
      "train loss1: 0.89680, train loss2: 1.63872, train loss3: 3.23166\n",
      "train loss1: 0.72629, train loss2: 0.04114, train loss3: 2.52430\n",
      "train loss1: 0.80988, train loss2: 0.04040, train loss3: 0.71972\n",
      "train loss1: 0.87403, train loss2: 0.19478, train loss3: 0.65143\n",
      "train loss1: 0.80150, train loss2: 1.14709, train loss3: 1.04354\n",
      "train loss1: 0.84786, train loss2: 0.03581, train loss3: 1.06658\n",
      "train loss1: 0.74591, train loss2: 0.10167, train loss3: 0.48741\n",
      "train loss1: 0.87512, train loss2: 0.38964, train loss3: 2.67464\n",
      "train loss1: 0.88625, train loss2: 0.02352, train loss3: 155.44949\n",
      "train loss1: 0.82802, train loss2: 0.06599, train loss3: 5.81429\n",
      "train loss1: 0.91258, train loss2: 0.06860, train loss3: 0.90211\n",
      "train loss1: 0.71652, train loss2: 0.02136, train loss3: 0.36618\n",
      "train loss1: 1.04267, train loss2: 0.08738, train loss3: 3.20148\n",
      "train loss1: 0.89943, train loss2: 0.04505, train loss3: 0.95168\n",
      "train loss1: 0.81813, train loss2: 0.24909, train loss3: 0.34386\n",
      "train loss1: 0.88365, train loss2: 0.14072, train loss3: 0.08770\n",
      "train loss1: 0.88714, train loss2: 0.06199, train loss3: 0.20053\n",
      "train loss1: 0.88551, train loss2: 0.05264, train loss3: 0.29375\n",
      "train loss1: 0.88153, train loss2: 0.05037, train loss3: 1.12863\n",
      "train loss1: 0.92699, train loss2: 0.05778, train loss3: 0.33444\n",
      "train loss1: 1.08519, train loss2: 0.19952, train loss3: 5.84583\n",
      "train loss1: 0.71830, train loss2: 0.30297, train loss3: 5.71389\n",
      "train loss1: 0.85598, train loss2: 0.12506, train loss3: 0.40321\n",
      "train loss1: 0.87797, train loss2: 0.10056, train loss3: 0.62110\n",
      "train loss1: 1.01920, train loss2: 11.21644, train loss3: 0.16310\n",
      "train loss1: 0.73401, train loss2: 0.49926, train loss3: 0.53432\n",
      "train loss1: 0.78015, train loss2: 0.27434, train loss3: 0.26736\n",
      "train loss1: 0.70037, train loss2: 0.51375, train loss3: 0.14828\n",
      "train loss1: 0.88781, train loss2: 0.06057, train loss3: 0.52491\n",
      "train loss1: 0.78906, train loss2: 0.08944, train loss3: 8.44160\n",
      "train loss1: 0.69913, train loss2: 0.78664, train loss3: 0.37468\n",
      "train loss1: 0.88022, train loss2: 0.08196, train loss3: 9.65999\n",
      "train loss1: 0.85510, train loss2: 0.02403, train loss3: 4.02412\n",
      "train loss1: 0.91297, train loss2: 1.87114, train loss3: 0.66562\n",
      "train loss1: 0.86012, train loss2: 0.82263, train loss3: 8.11066\n",
      "train loss1: 0.97454, train loss2: 0.58490, train loss3: 0.88364\n",
      "train loss1: 0.81938, train loss2: 0.06974, train loss3: 1.24934\n",
      "train loss1: 0.87421, train loss2: 0.05870, train loss3: 0.51937\n",
      "train loss1: 0.81366, train loss2: 0.92324, train loss3: 0.71586\n",
      "train loss1: 0.90310, train loss2: 0.03443, train loss3: 0.94281\n",
      "train loss1: 0.80273, train loss2: 0.03703, train loss3: 0.64027\n",
      "train loss1: 0.81808, train loss2: 0.30609, train loss3: 0.39068\n",
      "train loss1: 0.81499, train loss2: 0.39850, train loss3: 21.33712\n",
      "train loss1: 0.86265, train loss2: 0.04733, train loss3: 0.66861\n",
      "train loss1: 0.81378, train loss2: 0.10942, train loss3: 0.79934\n",
      "train loss1: 0.92354, train loss2: 0.03305, train loss3: 12.38552\n",
      "train loss1: 0.99877, train loss2: 0.09877, train loss3: 0.19991\n",
      "train loss1: 0.94433, train loss2: 0.08762, train loss3: 5.26293\n",
      "train loss1: 0.96378, train loss2: 0.43219, train loss3: 0.09718\n",
      "train loss1: 1.01605, train loss2: 0.40749, train loss3: 0.52538\n",
      "train loss1: 0.85271, train loss2: 0.06826, train loss3: 0.20386\n",
      "train loss1: 0.96270, train loss2: 0.04394, train loss3: 0.54689\n",
      "train loss1: 0.77015, train loss2: 0.29897, train loss3: 0.64719\n",
      "train loss1: 0.84352, train loss2: 0.03475, train loss3: 1.85392\n",
      "train loss1: 0.93530, train loss2: 1.92656, train loss3: 2.62098\n",
      "train loss1: 0.89309, train loss2: 1.51647, train loss3: 0.68297\n",
      "train loss1: 0.81494, train loss2: 0.09323, train loss3: 2.51279\n",
      "train loss1: 1.13992, train loss2: 1.62205, train loss3: 0.27252\n",
      "train loss1: 1.04310, train loss2: 1.08058, train loss3: 2.26986\n",
      "train loss1: 1.01163, train loss2: 0.06339, train loss3: 0.62348\n",
      "train loss1: 0.72349, train loss2: 0.27523, train loss3: 0.50903\n",
      "train loss1: 0.69805, train loss2: 0.10008, train loss3: 0.75733\n",
      "train loss1: 0.74485, train loss2: 151.29817, train loss3: 9.30585\n",
      "train loss1: 0.81547, train loss2: 0.02581, train loss3: 0.13504\n",
      "train loss1: 0.76499, train loss2: 0.05966, train loss3: 0.55609\n",
      "train loss1: 0.68072, train loss2: 0.04631, train loss3: 2.45605\n",
      "train loss1: 0.81755, train loss2: 45.61894, train loss3: 0.32579\n",
      "train loss1: 0.92829, train loss2: 0.17344, train loss3: 11.90650\n",
      "train loss1: 0.80948, train loss2: 0.05877, train loss3: 1.10974\n",
      "train loss1: 0.88035, train loss2: 0.56224, train loss3: 0.52434\n",
      "train loss1: 0.92215, train loss2: 0.05373, train loss3: 2.20542\n",
      "train loss1: 0.75339, train loss2: 0.81933, train loss3: 0.41828\n",
      "train loss1: 0.71155, train loss2: 0.05080, train loss3: 24.36967\n",
      "train loss1: 0.83017, train loss2: 0.06334, train loss3: 0.48635\n",
      "train loss1: 0.83145, train loss2: 0.20119, train loss3: 0.11316\n",
      "train loss1: 0.82182, train loss2: 0.62407, train loss3: 1.35141\n",
      "train loss1: 0.75471, train loss2: 3.51208, train loss3: 29.40062\n",
      "train loss1: 0.98517, train loss2: 0.09737, train loss3: 14.17291\n",
      "train loss1: 0.94462, train loss2: 0.04064, train loss3: 1.65039\n",
      "train loss1: 0.71087, train loss2: 0.01957, train loss3: 1.66839\n",
      "train loss1: 0.75585, train loss2: 0.02909, train loss3: 0.27450\n",
      "train loss1: 0.80532, train loss2: 48.35342, train loss3: 0.17946\n",
      "train loss1: 0.81965, train loss2: 0.07952, train loss3: 0.18182\n",
      "train loss1: 0.79328, train loss2: 3.83492, train loss3: 17.54455\n",
      "train loss1: 0.77093, train loss2: 0.16376, train loss3: 3.53390\n",
      "train loss1: 0.96643, train loss2: 0.22616, train loss3: 0.08590\n",
      "train loss1: 0.82645, train loss2: 0.63226, train loss3: 0.14033\n",
      "train loss1: 0.79082, train loss2: 0.39051, train loss3: 0.11800\n",
      "train loss1: 0.72950, train loss2: 0.13512, train loss3: 0.55005\n",
      "train loss1: 0.70195, train loss2: 0.06393, train loss3: 3.16728\n",
      "train loss1: 0.70581, train loss2: 0.06429, train loss3: 6.72500\n",
      "train loss1: 0.78033, train loss2: 0.06393, train loss3: 0.52370\n",
      "train loss1: 0.73215, train loss2: 0.53519, train loss3: 184.78362\n",
      "train loss1: 0.97265, train loss2: 0.06984, train loss3: 0.53653\n",
      "train loss1: 0.78670, train loss2: 0.05761, train loss3: 0.36057\n",
      "train loss1: 0.83853, train loss2: 0.14823, train loss3: 7.05546\n",
      "train loss1: 0.76262, train loss2: 0.03791, train loss3: 1.49106\n",
      "train loss1: 0.98774, train loss2: 0.03388, train loss3: 0.97911\n",
      "train loss1: 0.66945, train loss2: 0.02108, train loss3: 0.00519\n",
      "Epoch: 2/10\n",
      "train loss1: 0.67823, train loss2: 0.11256, train loss3: 0.84070\n",
      "train loss1: 0.70487, train loss2: 0.02886, train loss3: 0.52798\n",
      "train loss1: 1.03003, train loss2: 48.48769, train loss3: 0.29733\n",
      "train loss1: 0.80312, train loss2: 0.30254, train loss3: 1.06121\n",
      "train loss1: 1.01300, train loss2: 0.01477, train loss3: 0.13826\n",
      "train loss1: 0.71464, train loss2: 2.27624, train loss3: 1.01644\n",
      "train loss1: 0.72446, train loss2: 2.16187, train loss3: 2.86371\n",
      "train loss1: 0.83225, train loss2: 0.18253, train loss3: 0.69324\n",
      "train loss1: 1.01881, train loss2: 0.09470, train loss3: 0.67388\n",
      "train loss1: 0.94080, train loss2: 0.02177, train loss3: 1.68759\n",
      "train loss1: 0.86693, train loss2: 0.07552, train loss3: 0.77876\n",
      "train loss1: 0.96077, train loss2: 0.43756, train loss3: 0.80125\n",
      "train loss1: 0.74720, train loss2: 0.06059, train loss3: 2.63843\n",
      "train loss1: 0.71459, train loss2: 0.06599, train loss3: 0.47876\n",
      "train loss1: 0.70426, train loss2: 0.04704, train loss3: 0.78254\n",
      "train loss1: 0.89242, train loss2: 0.04119, train loss3: 1.86565\n",
      "train loss1: 0.76913, train loss2: 0.07839, train loss3: 0.83247\n",
      "train loss1: 0.71102, train loss2: 0.17099, train loss3: 1.25060\n",
      "train loss1: 0.69936, train loss2: 0.96139, train loss3: 0.43780\n",
      "train loss1: 0.77211, train loss2: 0.21297, train loss3: 2.35710\n",
      "train loss1: 0.73178, train loss2: 0.02622, train loss3: 0.69286\n",
      "train loss1: 0.72284, train loss2: 0.02128, train loss3: 0.32828\n",
      "train loss1: 0.86807, train loss2: 0.05168, train loss3: 0.67088\n",
      "train loss1: 0.73639, train loss2: 46.76915, train loss3: 0.58049\n",
      "train loss1: 0.68095, train loss2: 0.06850, train loss3: 4.16112\n",
      "train loss1: 0.72828, train loss2: 0.14774, train loss3: 3.27931\n",
      "train loss1: 0.68484, train loss2: 0.06871, train loss3: 0.98168\n",
      "train loss1: 1.10616, train loss2: 0.04957, train loss3: 0.44644\n",
      "train loss1: 0.91548, train loss2: 0.03912, train loss3: 5.42310\n",
      "train loss1: 0.75670, train loss2: 0.03753, train loss3: 0.96370\n",
      "train loss1: 0.79947, train loss2: 0.04351, train loss3: 0.67613\n",
      "train loss1: 0.94061, train loss2: 2.84276, train loss3: 0.67170\n",
      "train loss1: 0.95402, train loss2: 192.06041, train loss3: 0.67410\n",
      "train loss1: 0.75228, train loss2: 0.14879, train loss3: 0.27641\n",
      "train loss1: 0.70564, train loss2: 0.08903, train loss3: 1.67398\n",
      "train loss1: 1.03404, train loss2: 0.03604, train loss3: 26.32968\n",
      "train loss1: 0.75577, train loss2: 0.13310, train loss3: 0.12059\n",
      "train loss1: 0.72606, train loss2: 0.03800, train loss3: 0.83664\n",
      "train loss1: 0.81268, train loss2: 0.02201, train loss3: 0.17746\n",
      "train loss1: 0.85976, train loss2: 0.25016, train loss3: 2.00614\n",
      "train loss1: 0.87274, train loss2: 0.27484, train loss3: 0.82040\n",
      "train loss1: 0.73917, train loss2: 0.18202, train loss3: 0.58295\n",
      "train loss1: 0.72512, train loss2: 2.15659, train loss3: 4.67204\n",
      "train loss1: 0.73931, train loss2: 0.07972, train loss3: 0.45489\n",
      "train loss1: 0.71910, train loss2: 0.10546, train loss3: 0.67182\n",
      "train loss1: 0.76610, train loss2: 0.07850, train loss3: 0.29664\n",
      "train loss1: 0.91018, train loss2: 0.44072, train loss3: 1.77583\n",
      "train loss1: 0.85756, train loss2: 0.02093, train loss3: 9.05191\n",
      "train loss1: 0.85621, train loss2: 0.74272, train loss3: 0.92310\n",
      "train loss1: 0.69059, train loss2: 0.04417, train loss3: 0.41528\n",
      "train loss1: 0.68193, train loss2: 0.96855, train loss3: 0.17545\n",
      "train loss1: 0.81517, train loss2: 0.22928, train loss3: 0.41353\n",
      "train loss1: 0.79344, train loss2: 0.05600, train loss3: 5.73817\n",
      "train loss1: 0.78809, train loss2: 2.18786, train loss3: 0.25841\n",
      "train loss1: 0.77428, train loss2: 0.22432, train loss3: 2.27711\n",
      "train loss1: 0.68363, train loss2: 0.03293, train loss3: 1.69922\n",
      "train loss1: 0.87810, train loss2: 1.13380, train loss3: 0.31449\n",
      "train loss1: 0.73529, train loss2: 0.17948, train loss3: 1.23940\n",
      "train loss1: 0.84032, train loss2: 0.09014, train loss3: 0.49060\n",
      "train loss1: 0.83535, train loss2: 0.14515, train loss3: 4.22951\n",
      "train loss1: 0.72563, train loss2: 0.07852, train loss3: 0.57652\n",
      "train loss1: 0.75813, train loss2: 0.22225, train loss3: 0.17222\n",
      "train loss1: 0.84447, train loss2: 121.34919, train loss3: 3.57624\n",
      "train loss1: 0.68696, train loss2: 0.09755, train loss3: 0.36400\n",
      "train loss1: 0.80160, train loss2: 0.13073, train loss3: 3.77675\n",
      "train loss1: 0.74156, train loss2: 0.07385, train loss3: 0.21310\n",
      "train loss1: 0.85058, train loss2: 0.04896, train loss3: 0.44224\n",
      "train loss1: 0.70878, train loss2: 0.03744, train loss3: 0.41472\n",
      "train loss1: 0.77277, train loss2: 0.08471, train loss3: 0.26675\n",
      "train loss1: 0.67182, train loss2: 0.32826, train loss3: 0.60771\n",
      "train loss1: 0.70060, train loss2: 0.21521, train loss3: 1.01292\n",
      "train loss1: 0.77665, train loss2: 0.09568, train loss3: 2.61020\n",
      "train loss1: 0.73964, train loss2: 0.12424, train loss3: 0.51794\n",
      "train loss1: 0.70831, train loss2: 0.07401, train loss3: 0.54857\n",
      "train loss1: 0.81041, train loss2: 0.04409, train loss3: 2.59771\n",
      "train loss1: 0.71876, train loss2: 1.00515, train loss3: 1.81620\n",
      "train loss1: 0.91865, train loss2: 0.02209, train loss3: 1.69635\n",
      "train loss1: 0.88593, train loss2: 0.05453, train loss3: 1.48414\n",
      "train loss1: 0.80328, train loss2: 0.16391, train loss3: 56.02242\n",
      "train loss1: 0.81967, train loss2: 147.27756, train loss3: 0.26671\n",
      "train loss1: 0.84732, train loss2: 0.06721, train loss3: 24.09562\n",
      "train loss1: 0.78664, train loss2: 0.05076, train loss3: 0.28883\n",
      "train loss1: 0.68045, train loss2: 0.19333, train loss3: 1.12182\n",
      "train loss1: 0.81352, train loss2: 0.07447, train loss3: 0.80347\n",
      "train loss1: 0.68701, train loss2: 0.18419, train loss3: 0.67082\n",
      "train loss1: 0.83851, train loss2: 0.14191, train loss3: 1.21062\n",
      "train loss1: 0.83501, train loss2: 0.02933, train loss3: 0.40476\n",
      "train loss1: 0.68375, train loss2: 0.04057, train loss3: 0.87583\n",
      "train loss1: 0.77844, train loss2: 0.34060, train loss3: 7.19947\n",
      "train loss1: 0.71022, train loss2: 0.52751, train loss3: 0.86703\n",
      "train loss1: 0.79106, train loss2: 0.07726, train loss3: 0.15773\n",
      "train loss1: 0.95772, train loss2: 0.08405, train loss3: 0.60387\n",
      "train loss1: 0.72429, train loss2: 0.05767, train loss3: 1.20682\n",
      "train loss1: 0.92497, train loss2: 0.10654, train loss3: 0.50236\n",
      "train loss1: 0.74310, train loss2: 0.39647, train loss3: 1.52731\n",
      "train loss1: 0.77755, train loss2: 1.85381, train loss3: 28.01381\n",
      "train loss1: 0.70515, train loss2: 0.03058, train loss3: 0.14877\n",
      "train loss1: 0.68372, train loss2: 0.01545, train loss3: 0.14417\n",
      "train loss1: 0.74674, train loss2: 0.02077, train loss3: 1.68538\n",
      "train loss1: 0.81090, train loss2: 0.04086, train loss3: 0.28242\n",
      "train loss1: 0.69817, train loss2: 0.07078, train loss3: 0.55158\n",
      "train loss1: 0.69841, train loss2: 0.10094, train loss3: 9.48466\n",
      "train loss1: 0.71758, train loss2: 0.29104, train loss3: 0.32923\n",
      "train loss1: 0.92669, train loss2: 0.07434, train loss3: 0.59496\n",
      "train loss1: 0.77242, train loss2: 0.11237, train loss3: 1.33140\n",
      "train loss1: 0.71481, train loss2: 0.22703, train loss3: 0.59036\n",
      "train loss1: 0.73943, train loss2: 0.61098, train loss3: 0.68113\n",
      "train loss1: 0.79995, train loss2: 0.20905, train loss3: 0.23190\n",
      "train loss1: 0.65579, train loss2: 2.66456, train loss3: 0.13297\n",
      "train loss1: 0.68183, train loss2: 0.19837, train loss3: 0.76296\n",
      "train loss1: 0.81960, train loss2: 0.03031, train loss3: 1.72540\n",
      "train loss1: 0.79693, train loss2: 0.12556, train loss3: 0.21156\n",
      "train loss1: 0.70097, train loss2: 0.02770, train loss3: 0.59741\n",
      "train loss1: 0.68344, train loss2: 1.67712, train loss3: 0.25774\n",
      "train loss1: 0.78494, train loss2: 0.18631, train loss3: 1.56827\n",
      "train loss1: 0.69240, train loss2: 0.03758, train loss3: 1.34007\n",
      "train loss1: 0.69456, train loss2: 0.13573, train loss3: 0.43331\n",
      "train loss1: 0.73210, train loss2: 1.04545, train loss3: 3.36524\n",
      "train loss1: 0.71512, train loss2: 0.05819, train loss3: 0.15028\n",
      "train loss1: 0.78572, train loss2: 0.05704, train loss3: 0.48378\n",
      "train loss1: 0.74120, train loss2: 1.07126, train loss3: 0.61768\n",
      "train loss1: 0.73684, train loss2: 8.69708, train loss3: 0.07846\n",
      "train loss1: 0.79633, train loss2: 0.02776, train loss3: 1.18767\n",
      "train loss1: 0.81777, train loss2: 0.29187, train loss3: 0.47051\n",
      "train loss1: 0.77649, train loss2: 0.05170, train loss3: 1.65648\n",
      "train loss1: 0.76488, train loss2: 0.08575, train loss3: 0.23653\n",
      "train loss1: 0.68407, train loss2: 0.16371, train loss3: 0.51117\n",
      "train loss1: 0.69991, train loss2: 0.32749, train loss3: 1.20172\n",
      "train loss1: 0.79452, train loss2: 0.23718, train loss3: 12.92530\n",
      "train loss1: 0.73449, train loss2: 1.21366, train loss3: 0.55859\n",
      "train loss1: 0.70779, train loss2: 0.07467, train loss3: 3.35927\n",
      "train loss1: 0.78034, train loss2: 0.08840, train loss3: 0.73470\n",
      "train loss1: 0.68822, train loss2: 0.06963, train loss3: 17.13088\n",
      "train loss1: 0.87799, train loss2: 0.26758, train loss3: 1.02784\n",
      "train loss1: 0.75071, train loss2: 0.08730, train loss3: 0.25031\n",
      "train loss1: 0.87802, train loss2: 0.04988, train loss3: 0.47073\n",
      "train loss1: 0.77742, train loss2: 0.23714, train loss3: 0.40402\n",
      "train loss1: 0.68658, train loss2: 0.05328, train loss3: 2.49807\n",
      "train loss1: 0.81977, train loss2: 0.30571, train loss3: 147.89404\n",
      "train loss1: 0.66879, train loss2: 0.09419, train loss3: 1.79541\n",
      "train loss1: 0.78770, train loss2: 0.19089, train loss3: 2.59467\n",
      "train loss1: 0.70632, train loss2: 0.03768, train loss3: 0.51097\n",
      "train loss1: 0.79950, train loss2: 0.01504, train loss3: 0.52272\n",
      "train loss1: 0.69510, train loss2: 0.52659, train loss3: 0.61322\n",
      "train loss1: 0.75671, train loss2: 0.05476, train loss3: 0.41878\n",
      "train loss1: 0.70986, train loss2: 0.11433, train loss3: 1.34527\n",
      "train loss1: 0.77347, train loss2: 0.07845, train loss3: 2.86319\n",
      "train loss1: 0.70922, train loss2: 0.03607, train loss3: 0.88684\n",
      "train loss1: 0.75410, train loss2: 0.04008, train loss3: 9.43073\n",
      "train loss1: 0.69102, train loss2: 0.13720, train loss3: 0.29710\n",
      "train loss1: 0.87048, train loss2: 0.28359, train loss3: 244.24678\n",
      "train loss1: 0.68879, train loss2: 1.66245, train loss3: 0.29096\n",
      "train loss1: 0.68575, train loss2: 0.06694, train loss3: 1.63272\n",
      "train loss1: 0.70219, train loss2: 0.03524, train loss3: 1.03224\n",
      "train loss1: 0.71379, train loss2: 0.04230, train loss3: 2.19231\n",
      "train loss1: 0.70654, train loss2: 0.28393, train loss3: 2.77983\n",
      "train loss1: 0.67607, train loss2: 0.16949, train loss3: 0.41600\n",
      "train loss1: 0.71918, train loss2: 0.20571, train loss3: 0.19043\n",
      "train loss1: 0.70943, train loss2: 0.12790, train loss3: 0.14269\n",
      "train loss1: 0.87637, train loss2: 0.04404, train loss3: 13.07200\n",
      "train loss1: 1.11761, train loss2: 0.05329, train loss3: 1.03572\n",
      "train loss1: 0.83978, train loss2: 0.05579, train loss3: 0.33033\n",
      "train loss1: 0.78719, train loss2: 0.40072, train loss3: 2.81723\n",
      "train loss1: 0.78658, train loss2: 1.64192, train loss3: 0.29002\n",
      "train loss1: 0.86877, train loss2: 0.03486, train loss3: 0.88353\n",
      "train loss1: 0.78126, train loss2: 0.42015, train loss3: 0.22288\n",
      "train loss1: 0.78930, train loss2: 0.17159, train loss3: 0.51735\n",
      "train loss1: 0.95723, train loss2: 0.45405, train loss3: 1.97508\n",
      "train loss1: 0.79303, train loss2: 8.87767, train loss3: 0.38624\n",
      "train loss1: 0.66978, train loss2: 0.10617, train loss3: 0.38718\n",
      "train loss1: 0.70106, train loss2: 0.06778, train loss3: 21.17766\n",
      "train loss1: 0.67977, train loss2: 0.29054, train loss3: 3.07569\n",
      "train loss1: 0.81044, train loss2: 0.07955, train loss3: 0.88038\n",
      "train loss1: 0.75436, train loss2: 0.29195, train loss3: 0.31175\n",
      "train loss1: 0.69147, train loss2: 0.03043, train loss3: 0.24354\n",
      "train loss1: 0.83751, train loss2: 0.11619, train loss3: 27.17250\n",
      "train loss1: 0.70169, train loss2: 0.05791, train loss3: 0.48040\n",
      "train loss1: 0.67791, train loss2: 0.06386, train loss3: 0.69563\n",
      "train loss1: 0.70756, train loss2: 0.09640, train loss3: 0.19090\n",
      "train loss1: 0.72743, train loss2: 0.11758, train loss3: 5.43945\n",
      "train loss1: 0.79662, train loss2: 0.03230, train loss3: 15.36965\n",
      "train loss1: 0.66938, train loss2: 1.94573, train loss3: 1.41403\n",
      "train loss1: 0.78790, train loss2: 0.23309, train loss3: 0.53619\n",
      "train loss1: 0.67711, train loss2: 0.50589, train loss3: 3.02879\n",
      "train loss1: 0.67825, train loss2: 0.17909, train loss3: 0.25041\n",
      "train loss1: 0.71679, train loss2: 0.60647, train loss3: 1.11606\n",
      "train loss1: 0.69424, train loss2: 0.05559, train loss3: 3.42064\n",
      "train loss1: 0.80982, train loss2: 0.04629, train loss3: 1.92640\n",
      "train loss1: 0.79017, train loss2: 0.05427, train loss3: 0.14681\n",
      "train loss1: 0.77981, train loss2: 0.04049, train loss3: 8.28914\n",
      "train loss1: 0.68119, train loss2: 0.55926, train loss3: 1.03529\n",
      "train loss1: 0.74887, train loss2: 15.10657, train loss3: 0.37700\n",
      "train loss1: 0.74885, train loss2: 0.07590, train loss3: 0.43589\n",
      "train loss1: 0.67370, train loss2: 0.75569, train loss3: 0.39301\n",
      "train loss1: 0.68497, train loss2: 0.48071, train loss3: 0.17910\n",
      "train loss1: 0.71714, train loss2: 0.08793, train loss3: 1.52886\n",
      "train loss1: 0.67701, train loss2: 0.18229, train loss3: 0.46292\n",
      "train loss1: 0.67501, train loss2: 3.35267, train loss3: 2.71032\n",
      "train loss1: 0.76989, train loss2: 1.08356, train loss3: 0.40784\n",
      "train loss1: 0.77665, train loss2: 0.04440, train loss3: 1.37642\n",
      "train loss1: 0.66644, train loss2: 0.42374, train loss3: 1.37795\n",
      "train loss1: 0.72731, train loss2: 0.04960, train loss3: 0.82324\n",
      "train loss1: 0.83484, train loss2: 0.03466, train loss3: 0.06385\n",
      "train loss1: 0.69774, train loss2: 0.05302, train loss3: 1.81462\n",
      "train loss1: 0.80973, train loss2: 0.21571, train loss3: 0.44462\n",
      "train loss1: 0.71282, train loss2: 0.09877, train loss3: 0.40805\n",
      "train loss1: 0.73385, train loss2: 1.59653, train loss3: 0.19001\n",
      "train loss1: 0.66771, train loss2: 0.16532, train loss3: 0.39369\n",
      "train loss1: 0.71713, train loss2: 0.05103, train loss3: 2.45295\n",
      "train loss1: 0.72465, train loss2: 0.04863, train loss3: 0.18279\n",
      "train loss1: 0.79621, train loss2: 0.21856, train loss3: 0.51027\n",
      "train loss1: 0.72891, train loss2: 0.08210, train loss3: 0.12062\n",
      "train loss1: 0.86295, train loss2: 0.61406, train loss3: 3.14187\n",
      "train loss1: 0.68198, train loss2: 2.27545, train loss3: 0.23000\n",
      "train loss1: 0.76601, train loss2: 0.02156, train loss3: 0.49354\n",
      "train loss1: 0.78023, train loss2: 0.02658, train loss3: 3.70452\n",
      "train loss1: 0.77242, train loss2: 0.57107, train loss3: 0.29289\n",
      "train loss1: 0.66496, train loss2: 0.04393, train loss3: 1.12377\n",
      "train loss1: 0.66849, train loss2: 0.04879, train loss3: 0.21025\n",
      "train loss1: 0.71852, train loss2: 0.07013, train loss3: 2.46931\n",
      "train loss1: 0.72055, train loss2: 0.03238, train loss3: 4.98822\n",
      "train loss1: 0.66903, train loss2: 0.09419, train loss3: 0.15040\n",
      "train loss1: 0.69999, train loss2: 0.02101, train loss3: 0.53284\n",
      "train loss1: 0.73739, train loss2: 4.20399, train loss3: 0.36995\n",
      "train loss1: 0.79572, train loss2: 0.08558, train loss3: 1.73719\n",
      "train loss1: 0.66045, train loss2: 0.35295, train loss3: 6.29346\n",
      "train loss1: 0.69031, train loss2: 0.05813, train loss3: 12.33205\n",
      "train loss1: 0.67753, train loss2: 0.05449, train loss3: 1.08732\n",
      "train loss1: 0.88675, train loss2: 0.03963, train loss3: 15.91437\n",
      "train loss1: 0.67325, train loss2: 0.03468, train loss3: 0.85327\n",
      "train loss1: 0.70635, train loss2: 0.45929, train loss3: 0.96006\n",
      "train loss1: 0.69580, train loss2: 0.56200, train loss3: 0.13255\n",
      "train loss1: 0.68560, train loss2: 0.01636, train loss3: 0.12069\n",
      "train loss1: 0.68550, train loss2: 0.01644, train loss3: 1.62167\n",
      "train loss1: 0.72172, train loss2: 0.28274, train loss3: 0.15430\n",
      "train loss1: 0.86309, train loss2: 3.82324, train loss3: 1.80581\n",
      "train loss1: 0.72389, train loss2: 0.15710, train loss3: 9.17775\n",
      "train loss1: 0.77175, train loss2: 0.44576, train loss3: 0.28895\n",
      "train loss1: 0.77574, train loss2: 0.17611, train loss3: 2.04987\n",
      "train loss1: 0.75784, train loss2: 0.07274, train loss3: 1.34627\n",
      "train loss1: 0.76957, train loss2: 0.04195, train loss3: 0.63871\n",
      "train loss1: 0.69998, train loss2: 0.04379, train loss3: 31.07210\n",
      "train loss1: 0.68722, train loss2: 0.12812, train loss3: 0.60953\n",
      "train loss1: 0.66361, train loss2: 0.25171, train loss3: 0.58639\n",
      "train loss1: 0.76789, train loss2: 0.08974, train loss3: 0.27367\n",
      "train loss1: 0.69052, train loss2: 0.13534, train loss3: 2.91546\n",
      "train loss1: 0.71251, train loss2: 0.19428, train loss3: 1.36650\n",
      "train loss1: 0.67914, train loss2: 0.02784, train loss3: 0.37778\n",
      "train loss1: 0.72646, train loss2: 0.10299, train loss3: 24.15605\n",
      "train loss1: 0.76909, train loss2: 0.12160, train loss3: 0.22173\n",
      "train loss1: 0.70611, train loss2: 0.01512, train loss3: 1.97800\n",
      "train loss1: 0.66381, train loss2: 0.11234, train loss3: 0.48142\n",
      "train loss1: 0.68508, train loss2: 1.44244, train loss3: 0.73812\n",
      "train loss1: 0.88763, train loss2: 1.78485, train loss3: 0.96481\n",
      "train loss1: 0.68559, train loss2: 0.08618, train loss3: 5.30908\n",
      "train loss1: 0.81378, train loss2: 0.03600, train loss3: 0.24351\n",
      "train loss1: 0.68216, train loss2: 0.06513, train loss3: 1.74999\n",
      "train loss1: 0.74750, train loss2: 2.57818, train loss3: 0.39031\n",
      "train loss1: 0.78012, train loss2: 0.03077, train loss3: 1.18510\n",
      "train loss1: 0.66714, train loss2: 0.03499, train loss3: 1.23688\n",
      "train loss1: 0.88505, train loss2: 0.05656, train loss3: 0.32780\n",
      "train loss1: 0.76405, train loss2: 0.70395, train loss3: 0.27386\n",
      "train loss1: 0.69215, train loss2: 0.17779, train loss3: 0.14722\n",
      "train loss1: 0.74565, train loss2: 0.78990, train loss3: 2.40587\n",
      "train loss1: 0.68456, train loss2: 45.44114, train loss3: 0.15087\n",
      "train loss1: 0.69220, train loss2: 3.50321, train loss3: 0.35160\n",
      "train loss1: 0.75869, train loss2: 0.29204, train loss3: 19.19279\n",
      "train loss1: 0.67232, train loss2: 0.34496, train loss3: 0.49467\n",
      "train loss1: 0.70823, train loss2: 0.11631, train loss3: 184.66063\n",
      "train loss1: 0.80229, train loss2: 0.86736, train loss3: 0.23608\n",
      "train loss1: 0.70065, train loss2: 0.03898, train loss3: 0.30743\n",
      "train loss1: 0.66211, train loss2: 0.01878, train loss3: 0.49101\n",
      "train loss1: 0.67761, train loss2: 0.23211, train loss3: 0.20153\n",
      "train loss1: 0.67609, train loss2: 0.80594, train loss3: 0.54713\n",
      "train loss1: 0.69885, train loss2: 0.17670, train loss3: 1.18879\n",
      "train loss1: 0.69051, train loss2: 0.28923, train loss3: 2.05061\n",
      "train loss1: 0.86302, train loss2: 0.48483, train loss3: 1.98148\n",
      "train loss1: 0.67073, train loss2: 0.07052, train loss3: 7.91651\n",
      "train loss1: 0.81286, train loss2: 0.19981, train loss3: 0.19263\n",
      "train loss1: 0.66761, train loss2: 0.40120, train loss3: 0.20272\n",
      "train loss1: 0.69487, train loss2: 0.08430, train loss3: 1.75514\n",
      "train loss1: 0.79908, train loss2: 11.27970, train loss3: 0.62932\n",
      "train loss1: 0.79576, train loss2: 0.07667, train loss3: 0.20676\n",
      "train loss1: 0.76470, train loss2: 0.03144, train loss3: 0.53529\n",
      "train loss1: 0.85790, train loss2: 0.04364, train loss3: 0.25600\n",
      "train loss1: 0.67561, train loss2: 0.03814, train loss3: 0.52921\n",
      "train loss1: 0.68024, train loss2: 0.04068, train loss3: 0.93608\n",
      "train loss1: 0.83350, train loss2: 2.00138, train loss3: 0.34552\n",
      "train loss1: 0.66582, train loss2: 0.03564, train loss3: 12.43239\n",
      "train loss1: 0.66092, train loss2: 0.89215, train loss3: 10.45835\n",
      "train loss1: 0.68334, train loss2: 0.04777, train loss3: 0.77883\n",
      "train loss1: 0.72756, train loss2: 0.46914, train loss3: 0.47890\n",
      "train loss1: 0.80955, train loss2: 0.04261, train loss3: 2.66493\n",
      "train loss1: 0.66964, train loss2: 0.02334, train loss3: 0.47478\n",
      "train loss1: 0.76942, train loss2: 0.06691, train loss3: 0.18843\n",
      "train loss1: 0.67369, train loss2: 0.15931, train loss3: 0.66440\n",
      "train loss1: 0.60101, train loss2: 0.14868, train loss3: 0.09419\n",
      "Epoch: 3/10\n",
      "train loss1: 0.67175, train loss2: 0.24884, train loss3: 0.39003\n",
      "train loss1: 0.92776, train loss2: 0.07174, train loss3: 139.52876\n",
      "train loss1: 0.67954, train loss2: 0.01546, train loss3: 1.12499\n",
      "train loss1: 0.87279, train loss2: 0.22215, train loss3: 0.57426\n",
      "train loss1: 0.67074, train loss2: 0.07001, train loss3: 0.81368\n",
      "train loss1: 0.86494, train loss2: 0.03952, train loss3: 0.20525\n",
      "train loss1: 0.74750, train loss2: 0.38830, train loss3: 10.05969\n",
      "train loss1: 0.66300, train loss2: 0.06367, train loss3: 15.24901\n",
      "train loss1: 0.77426, train loss2: 0.26339, train loss3: 0.21136\n",
      "train loss1: 0.76441, train loss2: 0.04427, train loss3: 0.52378\n",
      "train loss1: 0.75602, train loss2: 0.03014, train loss3: 2.58537\n",
      "train loss1: 0.68391, train loss2: 0.44581, train loss3: 0.66825\n",
      "train loss1: 0.68025, train loss2: 0.10221, train loss3: 0.23422\n",
      "train loss1: 0.71249, train loss2: 0.05784, train loss3: 4.33265\n",
      "train loss1: 0.69568, train loss2: 147.06114, train loss3: 2.18503\n",
      "train loss1: 0.69153, train loss2: 0.34312, train loss3: 0.40195\n",
      "train loss1: 0.65914, train loss2: 0.09621, train loss3: 8.62831\n",
      "train loss1: 0.69854, train loss2: 0.03845, train loss3: 0.47625\n",
      "train loss1: 0.69745, train loss2: 0.56345, train loss3: 17.23741\n",
      "train loss1: 0.88841, train loss2: 0.15269, train loss3: 0.90566\n",
      "train loss1: 0.80587, train loss2: 0.03727, train loss3: 2.03062\n",
      "train loss1: 0.64502, train loss2: 0.03111, train loss3: 0.86351\n",
      "train loss1: 0.66918, train loss2: 0.04874, train loss3: 0.15834\n",
      "train loss1: 0.66003, train loss2: 2.58151, train loss3: 0.23241\n",
      "train loss1: 0.76003, train loss2: 4.09322, train loss3: 6.25814\n",
      "train loss1: 0.87830, train loss2: 0.04243, train loss3: 0.84421\n",
      "train loss1: 0.68553, train loss2: 0.09680, train loss3: 0.25503\n",
      "train loss1: 0.66029, train loss2: 0.03515, train loss3: 0.20554\n",
      "train loss1: 0.69486, train loss2: 1.38772, train loss3: 0.33519\n",
      "train loss1: 0.71627, train loss2: 0.15797, train loss3: 4.89627\n",
      "train loss1: 0.70803, train loss2: 0.26054, train loss3: 0.69593\n",
      "train loss1: 0.72890, train loss2: 0.02953, train loss3: 1.68437\n",
      "train loss1: 0.76051, train loss2: 0.05115, train loss3: 1.13151\n",
      "train loss1: 0.66574, train loss2: 111.92566, train loss3: 0.13596\n",
      "train loss1: 0.69991, train loss2: 0.07049, train loss3: 0.46769\n",
      "train loss1: 0.70181, train loss2: 1.97043, train loss3: 0.31038\n",
      "train loss1: 0.68298, train loss2: 2.01417, train loss3: 0.64451\n",
      "train loss1: 0.69241, train loss2: 0.09893, train loss3: 0.15962\n",
      "train loss1: 0.65378, train loss2: 0.69975, train loss3: 1.08998\n",
      "train loss1: 0.69138, train loss2: 0.02306, train loss3: 3.03151\n",
      "train loss1: 0.67495, train loss2: 51.41899, train loss3: 1.00345\n",
      "train loss1: 0.66397, train loss2: 0.05254, train loss3: 0.14989\n",
      "train loss1: 0.68193, train loss2: 0.23368, train loss3: 1.09029\n",
      "train loss1: 0.68213, train loss2: 0.70126, train loss3: 3.05440\n",
      "train loss1: 0.77505, train loss2: 0.06300, train loss3: 1.00753\n",
      "train loss1: 0.70936, train loss2: 0.11347, train loss3: 0.06752\n",
      "train loss1: 0.76509, train loss2: 0.07966, train loss3: 6.99031\n",
      "train loss1: 0.70916, train loss2: 1.88184, train loss3: 0.27023\n",
      "train loss1: 0.76597, train loss2: 0.04519, train loss3: 0.41817\n",
      "train loss1: 0.65640, train loss2: 0.22286, train loss3: 0.32033\n",
      "train loss1: 0.67912, train loss2: 0.07453, train loss3: 9.23403\n",
      "train loss1: 0.66075, train loss2: 0.23484, train loss3: 0.46178\n",
      "train loss1: 0.66098, train loss2: 0.05857, train loss3: 0.45871\n",
      "train loss1: 0.79431, train loss2: 0.01728, train loss3: 12.46688\n",
      "train loss1: 0.76610, train loss2: 0.25755, train loss3: 0.42575\n",
      "train loss1: 0.67391, train loss2: 0.74059, train loss3: 3.36445\n",
      "train loss1: 0.67909, train loss2: 0.44649, train loss3: 0.23932\n",
      "train loss1: 0.73544, train loss2: 0.02749, train loss3: 6.88970\n",
      "train loss1: 0.78170, train loss2: 0.04119, train loss3: 0.33599\n",
      "train loss1: 0.67557, train loss2: 0.04814, train loss3: 0.39676\n",
      "train loss1: 0.67820, train loss2: 0.11560, train loss3: 1.36665\n",
      "train loss1: 0.70139, train loss2: 0.14239, train loss3: 55.26604\n",
      "train loss1: 0.73890, train loss2: 0.14433, train loss3: 0.45764\n",
      "train loss1: 0.66983, train loss2: 0.24190, train loss3: 0.54245\n",
      "train loss1: 0.70749, train loss2: 0.29016, train loss3: 1.65515\n",
      "train loss1: 0.69497, train loss2: 0.89155, train loss3: 1.02883\n",
      "train loss1: 0.66150, train loss2: 0.06138, train loss3: 0.47325\n",
      "train loss1: 0.85213, train loss2: 3.34445, train loss3: 0.73708\n",
      "train loss1: 0.77244, train loss2: 0.05716, train loss3: 0.24452\n",
      "train loss1: 0.67017, train loss2: 0.08313, train loss3: 17.60000\n",
      "train loss1: 0.68070, train loss2: 0.07891, train loss3: 0.32490\n",
      "train loss1: 0.68460, train loss2: 0.33183, train loss3: 0.15861\n",
      "train loss1: 0.66977, train loss2: 0.15018, train loss3: 0.22957\n",
      "train loss1: 0.65378, train loss2: 0.08314, train loss3: 0.18015\n",
      "train loss1: 0.65592, train loss2: 0.96681, train loss3: 2.42933\n",
      "train loss1: 0.66693, train loss2: 0.08157, train loss3: 0.99130\n",
      "train loss1: 0.71597, train loss2: 0.03584, train loss3: 0.70291\n",
      "train loss1: 0.68654, train loss2: 0.03557, train loss3: 5.52593\n",
      "train loss1: 0.66693, train loss2: 0.03199, train loss3: 1.62900\n",
      "train loss1: 0.68951, train loss2: 0.32682, train loss3: 0.08187\n",
      "train loss1: 0.75530, train loss2: 0.80521, train loss3: 2.34349\n",
      "train loss1: 0.65279, train loss2: 0.08409, train loss3: 0.20776\n",
      "train loss1: 0.67440, train loss2: 0.11413, train loss3: 0.41340\n",
      "train loss1: 0.65146, train loss2: 0.19559, train loss3: 20.24556\n",
      "train loss1: 0.64938, train loss2: 0.01837, train loss3: 3.94363\n",
      "train loss1: 0.74141, train loss2: 0.06125, train loss3: 0.11456\n",
      "train loss1: 0.64163, train loss2: 0.09256, train loss3: 3.69996\n",
      "train loss1: 0.65251, train loss2: 0.07630, train loss3: 1.48290\n",
      "train loss1: 0.73204, train loss2: 3.74372, train loss3: 5.38529\n",
      "train loss1: 0.79402, train loss2: 0.07949, train loss3: 0.45500\n",
      "train loss1: 0.64594, train loss2: 0.07049, train loss3: 1.10373\n",
      "train loss1: 0.64483, train loss2: 0.08115, train loss3: 0.18848\n",
      "train loss1: 0.68825, train loss2: 0.01768, train loss3: 0.16400\n",
      "train loss1: 0.68080, train loss2: 0.08282, train loss3: 0.97877\n",
      "train loss1: 0.66580, train loss2: 0.03113, train loss3: 1.20983\n",
      "train loss1: 0.77886, train loss2: 0.05918, train loss3: 11.86315\n",
      "train loss1: 0.65424, train loss2: 0.13628, train loss3: 0.28825\n",
      "train loss1: 0.74178, train loss2: 0.16317, train loss3: 0.88546\n",
      "train loss1: 0.73982, train loss2: 0.03800, train loss3: 0.15275\n",
      "train loss1: 0.67328, train loss2: 0.76757, train loss3: 0.25524\n",
      "train loss1: 0.66367, train loss2: 0.13083, train loss3: 0.70616\n",
      "train loss1: 0.64769, train loss2: 13.89625, train loss3: 187.61441\n",
      "train loss1: 0.75552, train loss2: 2.32164, train loss3: 0.32801\n",
      "train loss1: 0.67146, train loss2: 0.01642, train loss3: 0.48783\n",
      "train loss1: 0.65290, train loss2: 0.02701, train loss3: 20.84559\n",
      "train loss1: 0.66331, train loss2: 0.02369, train loss3: 0.87592\n",
      "train loss1: 0.74153, train loss2: 0.16334, train loss3: 0.31146\n",
      "train loss1: 0.83101, train loss2: 0.20691, train loss3: 0.27477\n",
      "train loss1: 0.67171, train loss2: 0.30638, train loss3: 0.59317\n",
      "train loss1: 0.66516, train loss2: 0.43816, train loss3: 0.35328\n",
      "train loss1: 0.74358, train loss2: 0.47339, train loss3: 0.38900\n",
      "train loss1: 0.64559, train loss2: 0.05342, train loss3: 0.37100\n",
      "train loss1: 0.70120, train loss2: 0.11374, train loss3: 5.74245\n",
      "train loss1: 0.66240, train loss2: 0.46042, train loss3: 0.23768\n",
      "train loss1: 0.64791, train loss2: 0.07355, train loss3: 0.80988\n",
      "train loss1: 0.66337, train loss2: 0.03601, train loss3: 0.28300\n",
      "train loss1: 0.68526, train loss2: 0.92654, train loss3: 0.34118\n",
      "train loss1: 0.73749, train loss2: 0.06925, train loss3: 0.36779\n",
      "train loss1: 0.81252, train loss2: 0.14498, train loss3: 0.35917\n",
      "train loss1: 0.75326, train loss2: 0.26384, train loss3: 2.08286\n",
      "train loss1: 0.75153, train loss2: 0.10449, train loss3: 0.57025\n",
      "train loss1: 0.72678, train loss2: 0.11402, train loss3: 0.43754\n",
      "train loss1: 0.77125, train loss2: 0.12364, train loss3: 0.56101\n",
      "train loss1: 0.71605, train loss2: 0.02061, train loss3: 1.32048\n",
      "train loss1: 0.65868, train loss2: 0.04825, train loss3: 0.41918\n",
      "train loss1: 0.66749, train loss2: 0.07344, train loss3: 0.46820\n",
      "train loss1: 0.74805, train loss2: 0.58795, train loss3: 0.75848\n",
      "train loss1: 0.65865, train loss2: 0.08822, train loss3: 2.59399\n",
      "train loss1: 0.65969, train loss2: 0.94018, train loss3: 0.09779\n",
      "train loss1: 0.65169, train loss2: 0.10115, train loss3: 3.67387\n",
      "train loss1: 0.66947, train loss2: 0.17925, train loss3: 1.31538\n",
      "train loss1: 0.67989, train loss2: 0.26440, train loss3: 6.92660\n",
      "train loss1: 0.73838, train loss2: 0.25976, train loss3: 19.36043\n",
      "train loss1: 0.66388, train loss2: 0.47082, train loss3: 0.98579\n",
      "train loss1: 0.72158, train loss2: 0.06029, train loss3: 0.33868\n",
      "train loss1: 0.67019, train loss2: 0.06599, train loss3: 0.48830\n",
      "train loss1: 0.65304, train loss2: 0.05960, train loss3: 15.37909\n",
      "train loss1: 0.65128, train loss2: 0.15957, train loss3: 1.56120\n",
      "train loss1: 0.67170, train loss2: 10.74113, train loss3: 1.80096\n",
      "train loss1: 0.69699, train loss2: 0.06111, train loss3: 0.36711\n",
      "train loss1: 0.66283, train loss2: 4.76192, train loss3: 5.29274\n",
      "train loss1: 0.65558, train loss2: 0.26016, train loss3: 6.40373\n",
      "train loss1: 0.68724, train loss2: 0.74493, train loss3: 0.42672\n",
      "train loss1: 0.78493, train loss2: 0.09447, train loss3: 0.90937\n",
      "train loss1: 0.65869, train loss2: 0.40031, train loss3: 0.26837\n",
      "train loss1: 0.69484, train loss2: 0.49486, train loss3: 0.74383\n",
      "train loss1: 0.64268, train loss2: 0.07931, train loss3: 1.25931\n",
      "train loss1: 0.64934, train loss2: 0.09129, train loss3: 0.34149\n",
      "train loss1: 0.67157, train loss2: 0.16159, train loss3: 0.68656\n",
      "train loss1: 0.72865, train loss2: 0.24365, train loss3: 8.97838\n",
      "train loss1: 0.67347, train loss2: 0.03710, train loss3: 0.43604\n",
      "train loss1: 0.64868, train loss2: 0.25003, train loss3: 0.58284\n",
      "train loss1: 0.72629, train loss2: 0.04430, train loss3: 1.16944\n",
      "train loss1: 0.81193, train loss2: 1.37697, train loss3: 3.38888\n",
      "train loss1: 0.68450, train loss2: 0.08174, train loss3: 0.45997\n",
      "train loss1: 0.84455, train loss2: 0.06406, train loss3: 0.28529\n",
      "train loss1: 0.65742, train loss2: 0.05203, train loss3: 1.10657\n",
      "train loss1: 0.66527, train loss2: 0.03418, train loss3: 0.43220\n",
      "train loss1: 0.64554, train loss2: 0.04461, train loss3: 0.41092\n",
      "train loss1: 0.75017, train loss2: 0.33929, train loss3: 24.82267\n",
      "train loss1: 0.64796, train loss2: 0.04513, train loss3: 3.61042\n",
      "train loss1: 0.62764, train loss2: 0.76693, train loss3: 0.23117\n",
      "train loss1: 0.66947, train loss2: 0.28789, train loss3: 0.16789\n",
      "train loss1: 0.70528, train loss2: 0.04881, train loss3: 5.27477\n",
      "train loss1: 0.64281, train loss2: 0.31031, train loss3: 0.39818\n",
      "train loss1: 0.67182, train loss2: 0.05850, train loss3: 12.64047\n",
      "train loss1: 0.64891, train loss2: 0.06788, train loss3: 0.74883\n",
      "train loss1: 0.65388, train loss2: 0.07560, train loss3: 0.80558\n",
      "train loss1: 0.85031, train loss2: 0.05388, train loss3: 7.26076\n",
      "train loss1: 0.73648, train loss2: 0.26842, train loss3: 0.98986\n",
      "train loss1: 0.66361, train loss2: 0.01446, train loss3: 0.50979\n",
      "train loss1: 0.68539, train loss2: 0.02133, train loss3: 0.77036\n",
      "train loss1: 0.64233, train loss2: 0.16176, train loss3: 0.45779\n",
      "train loss1: 0.65011, train loss2: 0.89567, train loss3: 0.81876\n",
      "train loss1: 0.66330, train loss2: 0.26986, train loss3: 0.26277\n",
      "train loss1: 0.68341, train loss2: 0.19334, train loss3: 1.65112\n",
      "train loss1: 0.66564, train loss2: 45.45018, train loss3: 0.65072\n",
      "train loss1: 0.66303, train loss2: 0.04309, train loss3: 0.52059\n",
      "train loss1: 0.64296, train loss2: 2.62412, train loss3: 0.27856\n",
      "train loss1: 0.65460, train loss2: 0.26995, train loss3: 0.41855\n",
      "train loss1: 0.65603, train loss2: 0.06825, train loss3: 0.79618\n",
      "train loss1: 0.66496, train loss2: 0.07245, train loss3: 2.27535\n",
      "train loss1: 0.71925, train loss2: 0.12002, train loss3: 0.18975\n",
      "train loss1: 0.64455, train loss2: 0.09751, train loss3: 0.39559\n",
      "train loss1: 0.67076, train loss2: 0.60633, train loss3: 0.84946\n",
      "train loss1: 0.67335, train loss2: 0.04983, train loss3: 0.34925\n",
      "train loss1: 0.73458, train loss2: 0.05894, train loss3: 1.13960\n",
      "train loss1: 0.64027, train loss2: 0.30786, train loss3: 1.08616\n",
      "train loss1: 0.66860, train loss2: 0.53767, train loss3: 1.18717\n",
      "train loss1: 0.66847, train loss2: 0.05114, train loss3: 1.67658\n",
      "train loss1: 0.65902, train loss2: 1.64526, train loss3: 0.83593\n",
      "train loss1: 0.70088, train loss2: 46.97988, train loss3: 2.16791\n",
      "train loss1: 0.66765, train loss2: 0.11102, train loss3: 0.54878\n",
      "train loss1: 0.65323, train loss2: 206.87190, train loss3: 2.34504\n",
      "train loss1: 0.77377, train loss2: 0.03354, train loss3: 0.83601\n",
      "train loss1: 0.64542, train loss2: 0.65123, train loss3: 0.52235\n",
      "train loss1: 0.63640, train loss2: 0.13602, train loss3: 2.62776\n",
      "train loss1: 0.66171, train loss2: 0.05677, train loss3: 1.22305\n",
      "train loss1: 0.63588, train loss2: 0.05266, train loss3: 28.59979\n",
      "train loss1: 0.63953, train loss2: 0.04587, train loss3: 1.18553\n",
      "train loss1: 0.70798, train loss2: 0.06169, train loss3: 2.96816\n",
      "train loss1: 0.65048, train loss2: 0.10204, train loss3: 1.51405\n",
      "train loss1: 0.74838, train loss2: 0.07097, train loss3: 0.70253\n",
      "train loss1: 0.69912, train loss2: 0.05419, train loss3: 1.55321\n",
      "train loss1: 0.65274, train loss2: 0.92080, train loss3: 0.97615\n",
      "train loss1: 0.74615, train loss2: 0.38025, train loss3: 28.64053\n",
      "train loss1: 0.65456, train loss2: 0.06563, train loss3: 0.10092\n",
      "train loss1: 0.65546, train loss2: 0.13296, train loss3: 0.37662\n",
      "train loss1: 0.65123, train loss2: 0.09458, train loss3: 0.42971\n",
      "train loss1: 0.65275, train loss2: 0.02713, train loss3: 0.46226\n",
      "train loss1: 0.74381, train loss2: 0.02992, train loss3: 3.82090\n",
      "train loss1: 0.67402, train loss2: 0.06167, train loss3: 1.39606\n",
      "train loss1: 0.75240, train loss2: 0.02599, train loss3: 0.37385\n",
      "train loss1: 0.65851, train loss2: 0.07656, train loss3: 1.40140\n",
      "train loss1: 0.75026, train loss2: 0.06018, train loss3: 0.27713\n",
      "train loss1: 0.78427, train loss2: 0.51117, train loss3: 0.47712\n",
      "train loss1: 0.69401, train loss2: 1.09053, train loss3: 0.06643\n",
      "train loss1: 0.66019, train loss2: 0.17668, train loss3: 0.25914\n",
      "train loss1: 0.75870, train loss2: 0.04382, train loss3: 0.63802\n",
      "train loss1: 0.75908, train loss2: 0.02825, train loss3: 0.98191\n",
      "train loss1: 0.65469, train loss2: 0.04369, train loss3: 0.33021\n",
      "train loss1: 0.65451, train loss2: 0.06447, train loss3: 0.40950\n",
      "train loss1: 0.65944, train loss2: 4.26298, train loss3: 1.54169\n",
      "train loss1: 0.64532, train loss2: 0.03489, train loss3: 0.37638\n",
      "train loss1: 0.64525, train loss2: 0.07592, train loss3: 11.52726\n",
      "train loss1: 0.67598, train loss2: 0.04247, train loss3: 2.52391\n",
      "train loss1: 0.62695, train loss2: 0.08672, train loss3: 1.91203\n",
      "train loss1: 0.68677, train loss2: 0.11798, train loss3: 0.55111\n",
      "train loss1: 0.75100, train loss2: 0.71063, train loss3: 2.04506\n",
      "train loss1: 0.67216, train loss2: 0.13481, train loss3: 0.49576\n",
      "train loss1: 0.65842, train loss2: 0.03823, train loss3: 0.45120\n",
      "train loss1: 0.64628, train loss2: 0.13966, train loss3: 1.15352\n",
      "train loss1: 0.64996, train loss2: 0.01858, train loss3: 4.48330\n",
      "train loss1: 0.63842, train loss2: 0.03809, train loss3: 1.42940\n",
      "train loss1: 0.65777, train loss2: 1.75106, train loss3: 0.25654\n",
      "train loss1: 0.67151, train loss2: 0.26791, train loss3: 0.34005\n",
      "train loss1: 0.71938, train loss2: 0.75895, train loss3: 0.67897\n",
      "train loss1: 0.67055, train loss2: 0.07981, train loss3: 0.71606\n",
      "train loss1: 0.73671, train loss2: 1.61159, train loss3: 1.29033\n",
      "train loss1: 0.65109, train loss2: 0.03837, train loss3: 1.35814\n",
      "train loss1: 0.62903, train loss2: 0.06437, train loss3: 1.48886\n",
      "train loss1: 0.63459, train loss2: 0.03802, train loss3: 0.61324\n",
      "train loss1: 0.74052, train loss2: 0.07931, train loss3: 0.22664\n",
      "train loss1: 0.78366, train loss2: 0.28956, train loss3: 1.99202\n",
      "train loss1: 0.68053, train loss2: 0.29737, train loss3: 0.28120\n",
      "train loss1: 0.65362, train loss2: 0.06992, train loss3: 2.03536\n",
      "train loss1: 0.62477, train loss2: 0.09644, train loss3: 3.38222\n",
      "train loss1: 0.66509, train loss2: 0.17415, train loss3: 0.19542\n",
      "train loss1: 0.64528, train loss2: 0.04363, train loss3: 242.81728\n",
      "train loss1: 0.66512, train loss2: 0.50057, train loss3: 0.92631\n",
      "train loss1: 0.63576, train loss2: 0.03588, train loss3: 2.55949\n",
      "train loss1: 0.66796, train loss2: 0.04793, train loss3: 0.52358\n",
      "train loss1: 0.67486, train loss2: 0.05849, train loss3: 0.51191\n",
      "train loss1: 0.65262, train loss2: 0.29269, train loss3: 0.16729\n",
      "train loss1: 0.69905, train loss2: 0.16103, train loss3: 0.09753\n",
      "train loss1: 0.62968, train loss2: 0.21162, train loss3: 0.19968\n",
      "train loss1: 0.65782, train loss2: 0.08571, train loss3: 0.61208\n",
      "train loss1: 0.65553, train loss2: 0.13065, train loss3: 0.93405\n",
      "train loss1: 0.63738, train loss2: 0.12704, train loss3: 2.71251\n",
      "train loss1: 0.65671, train loss2: 0.07656, train loss3: 0.13465\n",
      "train loss1: 0.66250, train loss2: 0.11116, train loss3: 0.40315\n",
      "train loss1: 0.71385, train loss2: 0.07363, train loss3: 0.27009\n",
      "train loss1: 0.83080, train loss2: 0.06973, train loss3: 0.21616\n",
      "train loss1: 0.64628, train loss2: 0.13491, train loss3: 0.94473\n",
      "train loss1: 0.64756, train loss2: 0.03174, train loss3: 2.10364\n",
      "train loss1: 0.74243, train loss2: 0.06985, train loss3: 0.28516\n",
      "train loss1: 0.63436, train loss2: 0.04872, train loss3: 1.12290\n",
      "train loss1: 0.64764, train loss2: 0.18126, train loss3: 10.07817\n",
      "train loss1: 0.65943, train loss2: 0.05235, train loss3: 0.40117\n",
      "train loss1: 0.65362, train loss2: 0.66357, train loss3: 1.56590\n",
      "train loss1: 0.65763, train loss2: 0.12926, train loss3: 0.59026\n",
      "train loss1: 0.64837, train loss2: 8.65748, train loss3: 0.48003\n",
      "train loss1: 0.65439, train loss2: 0.06457, train loss3: 0.28440\n",
      "train loss1: 0.71296, train loss2: 0.10316, train loss3: 2.39094\n",
      "train loss1: 0.66907, train loss2: 0.08506, train loss3: 1.42037\n",
      "train loss1: 0.64317, train loss2: 0.28690, train loss3: 0.10233\n",
      "train loss1: 0.69824, train loss2: 5.86758, train loss3: 2.58882\n",
      "train loss1: 0.66074, train loss2: 0.32625, train loss3: 0.94365\n",
      "train loss1: 0.65918, train loss2: 0.62596, train loss3: 5.32964\n",
      "train loss1: 0.63766, train loss2: 0.13868, train loss3: 0.86045\n",
      "train loss1: 0.63456, train loss2: 0.03740, train loss3: 0.32447\n",
      "train loss1: 0.65952, train loss2: 0.03939, train loss3: 1.35636\n",
      "train loss1: 0.64935, train loss2: 0.95605, train loss3: 9.04267\n",
      "train loss1: 0.68650, train loss2: 0.98578, train loss3: 0.93657\n",
      "train loss1: 0.66814, train loss2: 0.06383, train loss3: 0.32077\n",
      "train loss1: 0.63081, train loss2: 0.05441, train loss3: 12.84251\n",
      "train loss1: 0.75124, train loss2: 0.04909, train loss3: 0.49356\n",
      "train loss1: 0.82361, train loss2: 1.13685, train loss3: 0.22465\n",
      "train loss1: 0.71067, train loss2: 2.11449, train loss3: 2.73769\n",
      "train loss1: 0.74798, train loss2: 0.45176, train loss3: 27.88149\n",
      "train loss1: 0.65530, train loss2: 0.21881, train loss3: 1.20043\n",
      "train loss1: 0.63662, train loss2: 0.03738, train loss3: 1.36279\n",
      "train loss1: 0.74314, train loss2: 0.10806, train loss3: 0.71971\n",
      "train loss1: 0.64917, train loss2: 0.19982, train loss3: 0.15591\n",
      "train loss1: 0.62598, train loss2: 0.63909, train loss3: 0.10367\n",
      "train loss1: 0.63331, train loss2: 0.05183, train loss3: 0.32558\n",
      "train loss1: 0.67054, train loss2: 0.00204, train loss3: 0.11371\n",
      "Epoch: 4/10\n",
      "train loss1: 0.66262, train loss2: 1.66612, train loss3: 3.54934\n",
      "train loss1: 0.64498, train loss2: 0.14349, train loss3: 0.19942\n",
      "train loss1: 0.74632, train loss2: 0.04107, train loss3: 1.32225\n",
      "train loss1: 0.62001, train loss2: 0.05585, train loss3: 2.51536\n",
      "train loss1: 0.75541, train loss2: 0.32458, train loss3: 0.25601\n",
      "train loss1: 0.74119, train loss2: 1.47220, train loss3: 0.48054\n",
      "train loss1: 0.65845, train loss2: 0.03479, train loss3: 0.74064\n",
      "train loss1: 0.66842, train loss2: 0.11896, train loss3: 27.88480\n",
      "train loss1: 0.62500, train loss2: 0.53307, train loss3: 0.72148\n",
      "train loss1: 0.65518, train loss2: 0.02890, train loss3: 0.33198\n",
      "train loss1: 0.73806, train loss2: 192.41232, train loss3: 0.36902\n",
      "train loss1: 0.63846, train loss2: 0.07087, train loss3: 0.85229\n",
      "train loss1: 0.64389, train loss2: 0.16725, train loss3: 0.84257\n",
      "train loss1: 0.76948, train loss2: 0.11555, train loss3: 0.29114\n",
      "train loss1: 0.69614, train loss2: 0.02905, train loss3: 0.63685\n",
      "train loss1: 0.65960, train loss2: 0.50141, train loss3: 1.26952\n",
      "train loss1: 0.66452, train loss2: 0.23872, train loss3: 0.54202\n",
      "train loss1: 0.63214, train loss2: 0.04412, train loss3: 2.64610\n",
      "train loss1: 0.64992, train loss2: 0.05212, train loss3: 0.84560\n",
      "train loss1: 0.62886, train loss2: 0.01935, train loss3: 1.39091\n",
      "train loss1: 0.63689, train loss2: 0.22927, train loss3: 3.36651\n",
      "train loss1: 0.65733, train loss2: 0.09798, train loss3: 1.16698\n",
      "train loss1: 0.64407, train loss2: 0.48056, train loss3: 0.76104\n",
      "train loss1: 0.64504, train loss2: 0.62621, train loss3: 0.16775\n",
      "train loss1: 0.74187, train loss2: 0.05671, train loss3: 0.31131\n",
      "train loss1: 0.63645, train loss2: 0.04699, train loss3: 1.01229\n",
      "train loss1: 0.76994, train loss2: 0.46250, train loss3: 2.38448\n",
      "train loss1: 0.65959, train loss2: 0.04556, train loss3: 7.29198\n",
      "train loss1: 0.72466, train loss2: 0.05127, train loss3: 0.56105\n",
      "train loss1: 0.74919, train loss2: 0.16497, train loss3: 25.31488\n",
      "train loss1: 0.75187, train loss2: 0.07195, train loss3: 0.54805\n",
      "train loss1: 0.64082, train loss2: 0.04977, train loss3: 0.82365\n",
      "train loss1: 0.61683, train loss2: 0.40851, train loss3: 0.29743\n",
      "train loss1: 0.63442, train loss2: 0.08628, train loss3: 0.85139\n",
      "train loss1: 0.63218, train loss2: 0.05560, train loss3: 0.39851\n",
      "train loss1: 0.64153, train loss2: 4.69856, train loss3: 2.22356\n",
      "train loss1: 0.66802, train loss2: 4.18967, train loss3: 0.77461\n",
      "train loss1: 0.63319, train loss2: 0.16879, train loss3: 0.98649\n",
      "train loss1: 0.64895, train loss2: 0.04784, train loss3: 0.16750\n",
      "train loss1: 0.78335, train loss2: 0.15381, train loss3: 0.66259\n",
      "train loss1: 0.63398, train loss2: 0.04767, train loss3: 0.53722\n",
      "train loss1: 0.67269, train loss2: 9.23189, train loss3: 0.13221\n",
      "train loss1: 0.66706, train loss2: 0.11324, train loss3: 1.38176\n",
      "train loss1: 0.73681, train loss2: 0.69859, train loss3: 0.31145\n",
      "train loss1: 0.78312, train loss2: 0.02450, train loss3: 0.15659\n",
      "train loss1: 0.72060, train loss2: 0.66903, train loss3: 1.98171\n",
      "train loss1: 0.60984, train loss2: 0.29707, train loss3: 1.17581\n",
      "train loss1: 0.65665, train loss2: 0.08556, train loss3: 0.36895\n",
      "train loss1: 0.66739, train loss2: 0.62270, train loss3: 0.30167\n",
      "train loss1: 0.64570, train loss2: 0.65055, train loss3: 8.62314\n",
      "train loss1: 0.63929, train loss2: 147.07637, train loss3: 0.76272\n",
      "train loss1: 0.65445, train loss2: 0.09542, train loss3: 6.80434\n",
      "train loss1: 0.66667, train loss2: 0.33345, train loss3: 1.20982\n",
      "train loss1: 0.65209, train loss2: 0.04804, train loss3: 3.69513\n",
      "train loss1: 0.65152, train loss2: 0.07833, train loss3: 1.14297\n",
      "train loss1: 0.68947, train loss2: 0.53179, train loss3: 2.55417\n",
      "train loss1: 0.64464, train loss2: 0.05157, train loss3: 0.80484\n",
      "train loss1: 0.62222, train loss2: 0.18015, train loss3: 1.73346\n",
      "train loss1: 0.67209, train loss2: 1.85433, train loss3: 0.15702\n",
      "train loss1: 0.63744, train loss2: 0.12579, train loss3: 0.53716\n",
      "train loss1: 0.64309, train loss2: 0.49429, train loss3: 0.74703\n",
      "train loss1: 0.62327, train loss2: 0.08812, train loss3: 0.59655\n",
      "train loss1: 0.63542, train loss2: 0.17360, train loss3: 1.70017\n",
      "train loss1: 0.62167, train loss2: 0.21612, train loss3: 0.39270\n",
      "train loss1: 0.63362, train loss2: 0.35030, train loss3: 0.09407\n",
      "train loss1: 0.67213, train loss2: 0.12615, train loss3: 1.26457\n",
      "train loss1: 0.64227, train loss2: 0.07451, train loss3: 0.64825\n",
      "train loss1: 0.64460, train loss2: 0.08282, train loss3: 0.23133\n",
      "train loss1: 0.74020, train loss2: 0.02297, train loss3: 0.20938\n",
      "train loss1: 0.64441, train loss2: 0.03195, train loss3: 0.66755\n",
      "train loss1: 0.67371, train loss2: 1.14581, train loss3: 0.20349\n",
      "train loss1: 0.62692, train loss2: 0.01699, train loss3: 3.22998\n",
      "train loss1: 0.65260, train loss2: 0.06685, train loss3: 1.01583\n",
      "train loss1: 0.74649, train loss2: 0.03473, train loss3: 0.40909\n",
      "train loss1: 0.64088, train loss2: 0.10616, train loss3: 0.86883\n",
      "train loss1: 0.64321, train loss2: 0.04753, train loss3: 2.30734\n",
      "train loss1: 0.72126, train loss2: 0.06627, train loss3: 6.05272\n",
      "train loss1: 0.63874, train loss2: 0.04777, train loss3: 0.45036\n",
      "train loss1: 0.62163, train loss2: 0.17210, train loss3: 0.15932\n",
      "train loss1: 0.60664, train loss2: 0.02833, train loss3: 0.25786\n",
      "train loss1: 0.68087, train loss2: 0.02397, train loss3: 0.10928\n",
      "train loss1: 0.72728, train loss2: 0.03664, train loss3: 0.91144\n",
      "train loss1: 0.64556, train loss2: 0.16007, train loss3: 1.05964\n",
      "train loss1: 0.64331, train loss2: 0.18093, train loss3: 0.96082\n",
      "train loss1: 0.62009, train loss2: 0.06691, train loss3: 1.22459\n",
      "train loss1: 0.63450, train loss2: 0.03419, train loss3: 5.19050\n",
      "train loss1: 0.62507, train loss2: 0.04694, train loss3: 0.68440\n",
      "train loss1: 0.63251, train loss2: 0.07959, train loss3: 0.44673\n",
      "train loss1: 0.73231, train loss2: 0.19945, train loss3: 0.23566\n",
      "train loss1: 0.64643, train loss2: 0.34878, train loss3: 2.37188\n",
      "train loss1: 0.64326, train loss2: 0.20650, train loss3: 0.61526\n",
      "train loss1: 0.64670, train loss2: 0.05176, train loss3: 0.51543\n",
      "train loss1: 0.64138, train loss2: 0.05409, train loss3: 0.85017\n",
      "train loss1: 0.63839, train loss2: 0.28935, train loss3: 1.77325\n",
      "train loss1: 0.66329, train loss2: 0.03976, train loss3: 0.29360\n",
      "train loss1: 0.62811, train loss2: 0.06605, train loss3: 2.02748\n",
      "train loss1: 0.63725, train loss2: 0.38695, train loss3: 1.06765\n",
      "train loss1: 0.71152, train loss2: 0.05200, train loss3: 0.64826\n",
      "train loss1: 0.63402, train loss2: 0.06048, train loss3: 1.87965\n",
      "train loss1: 0.63349, train loss2: 45.40773, train loss3: 139.51105\n",
      "train loss1: 0.65218, train loss2: 0.12025, train loss3: 0.32166\n",
      "train loss1: 0.63930, train loss2: 0.04121, train loss3: 0.44563\n",
      "train loss1: 0.62994, train loss2: 0.06776, train loss3: 1.14368\n",
      "train loss1: 0.76315, train loss2: 0.20241, train loss3: 20.63393\n",
      "train loss1: 0.62496, train loss2: 0.09821, train loss3: 1.35345\n",
      "train loss1: 0.71487, train loss2: 0.37199, train loss3: 9.55865\n",
      "train loss1: 0.63274, train loss2: 0.08863, train loss3: 0.53043\n",
      "train loss1: 0.67359, train loss2: 0.05149, train loss3: 0.32123\n",
      "train loss1: 0.64858, train loss2: 0.05729, train loss3: 0.34918\n",
      "train loss1: 0.69333, train loss2: 0.07843, train loss3: 1.80521\n",
      "train loss1: 0.61922, train loss2: 0.10322, train loss3: 3.88988\n",
      "train loss1: 0.61197, train loss2: 0.66947, train loss3: 0.16999\n",
      "train loss1: 0.79153, train loss2: 0.05301, train loss3: 0.26825\n",
      "train loss1: 0.62828, train loss2: 0.07424, train loss3: 0.44763\n",
      "train loss1: 0.82905, train loss2: 0.28072, train loss3: 0.16217\n",
      "train loss1: 0.83906, train loss2: 2.00230, train loss3: 0.57198\n",
      "train loss1: 0.65001, train loss2: 0.14793, train loss3: 0.38798\n",
      "train loss1: 0.63508, train loss2: 0.03513, train loss3: 0.65483\n",
      "train loss1: 0.63433, train loss2: 0.05704, train loss3: 0.33061\n",
      "train loss1: 0.62502, train loss2: 0.01970, train loss3: 0.42700\n",
      "train loss1: 0.62939, train loss2: 3.32766, train loss3: 2.78091\n",
      "train loss1: 0.62520, train loss2: 0.30560, train loss3: 1.10676\n",
      "train loss1: 0.65830, train loss2: 0.08055, train loss3: 0.19096\n",
      "train loss1: 0.61423, train loss2: 0.04636, train loss3: 0.50726\n",
      "train loss1: 0.68208, train loss2: 0.06503, train loss3: 1.11873\n",
      "train loss1: 0.63379, train loss2: 0.05867, train loss3: 2.74370\n",
      "train loss1: 0.65503, train loss2: 0.12431, train loss3: 0.95130\n",
      "train loss1: 0.65222, train loss2: 0.05251, train loss3: 0.25919\n",
      "train loss1: 0.61665, train loss2: 0.02651, train loss3: 0.68231\n",
      "train loss1: 0.61517, train loss2: 3.61610, train loss3: 0.46124\n",
      "train loss1: 0.73721, train loss2: 0.04948, train loss3: 9.22351\n",
      "train loss1: 0.61946, train loss2: 0.08102, train loss3: 0.96952\n",
      "train loss1: 0.63591, train loss2: 0.21374, train loss3: 9.19883\n",
      "train loss1: 0.64906, train loss2: 0.92233, train loss3: 0.21413\n",
      "train loss1: 0.63030, train loss2: 0.07470, train loss3: 5.77624\n",
      "train loss1: 0.77424, train loss2: 0.06849, train loss3: 4.47658\n",
      "train loss1: 0.64447, train loss2: 0.05196, train loss3: 20.07235\n",
      "train loss1: 0.63076, train loss2: 0.09340, train loss3: 0.70810\n",
      "train loss1: 0.61627, train loss2: 11.33299, train loss3: 0.41967\n",
      "train loss1: 0.63830, train loss2: 0.03936, train loss3: 0.24088\n",
      "train loss1: 0.67865, train loss2: 0.04337, train loss3: 1.02719\n",
      "train loss1: 0.66427, train loss2: 0.04446, train loss3: 1.89120\n",
      "train loss1: 0.62463, train loss2: 0.15858, train loss3: 0.29074\n",
      "train loss1: 0.62828, train loss2: 0.31218, train loss3: 0.27351\n",
      "train loss1: 0.63334, train loss2: 2.16466, train loss3: 0.38348\n",
      "train loss1: 0.63244, train loss2: 0.12831, train loss3: 0.40846\n",
      "train loss1: 0.61944, train loss2: 0.06659, train loss3: 0.74314\n",
      "train loss1: 0.62566, train loss2: 0.10131, train loss3: 1.16537\n",
      "train loss1: 0.64141, train loss2: 0.06104, train loss3: 3.03472\n",
      "train loss1: 0.64427, train loss2: 3.47094, train loss3: 1.47532\n",
      "train loss1: 0.68031, train loss2: 0.09232, train loss3: 0.60275\n",
      "train loss1: 0.61769, train loss2: 4.51830, train loss3: 0.36684\n",
      "train loss1: 0.63462, train loss2: 0.04995, train loss3: 1.26089\n",
      "train loss1: 0.65058, train loss2: 0.41723, train loss3: 0.86838\n",
      "train loss1: 0.60209, train loss2: 0.06939, train loss3: 4.76183\n",
      "train loss1: 0.62848, train loss2: 0.03481, train loss3: 0.32524\n",
      "train loss1: 0.61770, train loss2: 0.59912, train loss3: 0.24801\n",
      "train loss1: 0.62422, train loss2: 0.04766, train loss3: 2.40580\n",
      "train loss1: 0.81607, train loss2: 0.03877, train loss3: 12.43911\n",
      "train loss1: 0.64798, train loss2: 0.24412, train loss3: 0.30414\n",
      "train loss1: 0.63549, train loss2: 0.11093, train loss3: 0.34623\n",
      "train loss1: 0.74749, train loss2: 0.05348, train loss3: 57.85828\n",
      "train loss1: 0.65115, train loss2: 14.83886, train loss3: 0.34543\n",
      "train loss1: 0.67177, train loss2: 0.40401, train loss3: 7.97641\n",
      "train loss1: 0.66225, train loss2: 0.11093, train loss3: 0.57553\n",
      "train loss1: 0.60068, train loss2: 0.35029, train loss3: 0.16715\n",
      "train loss1: 0.60858, train loss2: 0.05769, train loss3: 1.02675\n",
      "train loss1: 0.61750, train loss2: 0.23368, train loss3: 1.00770\n",
      "train loss1: 0.64826, train loss2: 0.07734, train loss3: 0.59387\n",
      "train loss1: 0.63031, train loss2: 0.06076, train loss3: 0.28172\n",
      "train loss1: 0.61367, train loss2: 0.10567, train loss3: 0.60630\n",
      "train loss1: 0.65743, train loss2: 0.04351, train loss3: 0.12504\n",
      "train loss1: 0.72383, train loss2: 0.10516, train loss3: 1.07735\n",
      "train loss1: 0.61418, train loss2: 0.02196, train loss3: 3.30989\n",
      "train loss1: 0.63774, train loss2: 0.09322, train loss3: 0.90553\n",
      "train loss1: 0.73812, train loss2: 0.07188, train loss3: 0.58846\n",
      "train loss1: 0.65834, train loss2: 0.02809, train loss3: 0.63150\n",
      "train loss1: 0.64345, train loss2: 0.10012, train loss3: 0.26061\n",
      "train loss1: 0.63456, train loss2: 0.04714, train loss3: 0.13653\n",
      "train loss1: 0.70562, train loss2: 0.03331, train loss3: 0.20204\n",
      "train loss1: 0.62705, train loss2: 0.05545, train loss3: 0.68648\n",
      "train loss1: 0.63348, train loss2: 0.07659, train loss3: 2.56722\n",
      "train loss1: 0.66680, train loss2: 0.05857, train loss3: 5.59862\n",
      "train loss1: 0.61636, train loss2: 0.03890, train loss3: 12.77006\n",
      "train loss1: 0.63390, train loss2: 0.20518, train loss3: 0.74420\n",
      "train loss1: 0.71208, train loss2: 0.20159, train loss3: 2.46806\n",
      "train loss1: 0.63981, train loss2: 2.56667, train loss3: 1.40327\n",
      "train loss1: 0.62415, train loss2: 0.15812, train loss3: 0.23390\n",
      "train loss1: 0.62442, train loss2: 4.31862, train loss3: 0.49793\n",
      "train loss1: 0.65210, train loss2: 1.63262, train loss3: 1.26757\n",
      "train loss1: 0.62767, train loss2: 0.66129, train loss3: 1.31913\n",
      "train loss1: 0.63780, train loss2: 0.40039, train loss3: 1.64773\n",
      "train loss1: 0.62226, train loss2: 0.02929, train loss3: 0.65002\n",
      "train loss1: 0.64977, train loss2: 0.05160, train loss3: 1.24862\n",
      "train loss1: 0.63001, train loss2: 0.93902, train loss3: 1.31223\n",
      "train loss1: 0.64389, train loss2: 0.29027, train loss3: 0.33747\n",
      "train loss1: 0.72410, train loss2: 0.03536, train loss3: 1.16623\n",
      "train loss1: 0.63226, train loss2: 0.75880, train loss3: 0.74653\n",
      "train loss1: 0.71969, train loss2: 1.96986, train loss3: 0.61997\n",
      "train loss1: 0.62384, train loss2: 0.03130, train loss3: 0.90123\n",
      "train loss1: 0.68652, train loss2: 0.05517, train loss3: 0.17670\n",
      "train loss1: 0.63714, train loss2: 0.08939, train loss3: 19.16501\n",
      "train loss1: 0.60458, train loss2: 0.69106, train loss3: 14.24275\n",
      "train loss1: 0.63397, train loss2: 0.04104, train loss3: 5.64093\n",
      "train loss1: 0.63296, train loss2: 0.07769, train loss3: 15.77180\n",
      "train loss1: 0.75559, train loss2: 0.95857, train loss3: 5.16064\n",
      "train loss1: 0.63063, train loss2: 0.03924, train loss3: 0.93262\n",
      "train loss1: 0.63063, train loss2: 0.04208, train loss3: 0.33769\n",
      "train loss1: 0.62919, train loss2: 0.04196, train loss3: 1.15385\n",
      "train loss1: 0.60512, train loss2: 0.06820, train loss3: 0.39224\n",
      "train loss1: 0.72183, train loss2: 0.05653, train loss3: 1.01620\n",
      "train loss1: 0.61685, train loss2: 0.82949, train loss3: 0.46556\n",
      "train loss1: 0.62357, train loss2: 48.45323, train loss3: 0.53347\n",
      "train loss1: 0.62274, train loss2: 0.18496, train loss3: 0.41424\n",
      "train loss1: 0.70640, train loss2: 0.49922, train loss3: 0.26331\n",
      "train loss1: 0.73330, train loss2: 0.03720, train loss3: 0.93462\n",
      "train loss1: 0.62671, train loss2: 0.07543, train loss3: 4.61328\n",
      "train loss1: 0.73808, train loss2: 0.34795, train loss3: 1.84819\n",
      "train loss1: 0.61229, train loss2: 0.24795, train loss3: 2.22148\n",
      "train loss1: 0.72165, train loss2: 0.05095, train loss3: 0.85269\n",
      "train loss1: 0.62434, train loss2: 4.07817, train loss3: 1.46927\n",
      "train loss1: 0.71232, train loss2: 0.15532, train loss3: 5.45405\n",
      "train loss1: 0.64447, train loss2: 1.63646, train loss3: 0.37494\n",
      "train loss1: 0.61980, train loss2: 0.04719, train loss3: 0.21947\n",
      "train loss1: 0.62222, train loss2: 46.89934, train loss3: 0.97092\n",
      "train loss1: 0.62656, train loss2: 0.10997, train loss3: 24.99465\n",
      "train loss1: 0.75649, train loss2: 0.66970, train loss3: 0.35917\n",
      "train loss1: 0.60797, train loss2: 0.04145, train loss3: 27.96262\n",
      "train loss1: 0.62608, train loss2: 0.90237, train loss3: 3.93750\n",
      "train loss1: 0.60847, train loss2: 0.08109, train loss3: 2.13101\n",
      "train loss1: 0.66600, train loss2: 1.61080, train loss3: 0.79945\n",
      "train loss1: 0.63319, train loss2: 0.06625, train loss3: 0.90671\n",
      "train loss1: 0.60973, train loss2: 0.15581, train loss3: 0.19867\n",
      "train loss1: 0.62137, train loss2: 0.21555, train loss3: 0.51567\n",
      "train loss1: 0.61914, train loss2: 0.34500, train loss3: 0.59718\n",
      "train loss1: 0.72817, train loss2: 0.01812, train loss3: 0.36190\n",
      "train loss1: 0.61579, train loss2: 0.05116, train loss3: 1.42390\n",
      "train loss1: 0.61685, train loss2: 0.44007, train loss3: 10.06229\n",
      "train loss1: 0.61679, train loss2: 0.07320, train loss3: 0.83359\n",
      "train loss1: 0.61026, train loss2: 3.47500, train loss3: 0.24509\n",
      "train loss1: 0.61795, train loss2: 1.68159, train loss3: 0.36554\n",
      "train loss1: 0.73979, train loss2: 0.06876, train loss3: 0.20716\n",
      "train loss1: 0.61627, train loss2: 0.03744, train loss3: 0.14535\n",
      "train loss1: 0.61653, train loss2: 0.02370, train loss3: 0.24572\n",
      "train loss1: 0.63457, train loss2: 0.01270, train loss3: 3.54535\n",
      "train loss1: 0.62622, train loss2: 2.34600, train loss3: 0.36082\n",
      "train loss1: 0.61563, train loss2: 0.10333, train loss3: 1.12656\n",
      "train loss1: 0.61236, train loss2: 0.45326, train loss3: 0.57110\n",
      "train loss1: 0.62365, train loss2: 0.06182, train loss3: 0.91666\n",
      "train loss1: 0.71374, train loss2: 0.08697, train loss3: 0.10741\n",
      "train loss1: 0.62522, train loss2: 0.11992, train loss3: 1.45078\n",
      "train loss1: 0.63377, train loss2: 0.06708, train loss3: 0.46181\n",
      "train loss1: 0.60772, train loss2: 0.04162, train loss3: 0.50658\n",
      "train loss1: 0.62022, train loss2: 0.06655, train loss3: 2.05370\n",
      "train loss1: 0.62466, train loss2: 0.03312, train loss3: 0.14577\n",
      "train loss1: 0.69711, train loss2: 0.10606, train loss3: 0.31534\n",
      "train loss1: 0.61893, train loss2: 0.13273, train loss3: 11.77798\n",
      "train loss1: 0.64736, train loss2: 0.04076, train loss3: 0.60544\n",
      "train loss1: 0.62558, train loss2: 0.06496, train loss3: 0.49242\n",
      "train loss1: 0.75542, train loss2: 0.15400, train loss3: 1.04608\n",
      "train loss1: 0.63668, train loss2: 2.24731, train loss3: 2.64209\n",
      "train loss1: 0.81137, train loss2: 0.75709, train loss3: 1.12797\n",
      "train loss1: 0.71197, train loss2: 0.78526, train loss3: 1.01226\n",
      "train loss1: 0.63361, train loss2: 0.09320, train loss3: 0.07955\n",
      "train loss1: 0.65214, train loss2: 0.62381, train loss3: 0.84882\n",
      "train loss1: 0.65514, train loss2: 0.06900, train loss3: 0.19428\n",
      "train loss1: 0.64223, train loss2: 0.07489, train loss3: 0.50613\n",
      "train loss1: 0.61727, train loss2: 0.32974, train loss3: 184.55626\n",
      "train loss1: 0.63073, train loss2: 0.20504, train loss3: 4.52734\n",
      "train loss1: 0.62872, train loss2: 111.97218, train loss3: 0.31560\n",
      "train loss1: 0.63658, train loss2: 0.05517, train loss3: 0.08951\n",
      "train loss1: 0.62738, train loss2: 0.10914, train loss3: 27.37169\n",
      "train loss1: 0.61836, train loss2: 0.19433, train loss3: 18.54840\n",
      "train loss1: 0.63991, train loss2: 0.14053, train loss3: 1.68306\n",
      "train loss1: 0.61248, train loss2: 0.02712, train loss3: 237.72464\n",
      "train loss1: 0.63121, train loss2: 0.12311, train loss3: 1.59059\n",
      "train loss1: 0.61410, train loss2: 0.07155, train loss3: 0.73247\n",
      "train loss1: 0.61210, train loss2: 0.18576, train loss3: 2.09586\n",
      "train loss1: 0.63838, train loss2: 0.06140, train loss3: 1.49200\n",
      "train loss1: 0.61284, train loss2: 0.22900, train loss3: 16.96303\n",
      "train loss1: 0.62244, train loss2: 0.04627, train loss3: 0.71652\n",
      "train loss1: 0.70478, train loss2: 0.09920, train loss3: 0.67344\n",
      "train loss1: 0.60739, train loss2: 0.04023, train loss3: 0.67396\n",
      "train loss1: 0.61405, train loss2: 0.02319, train loss3: 0.26625\n",
      "train loss1: 0.62396, train loss2: 1.03302, train loss3: 0.20831\n",
      "train loss1: 0.62282, train loss2: 0.13211, train loss3: 0.17183\n",
      "train loss1: 0.60116, train loss2: 0.27540, train loss3: 0.67891\n",
      "train loss1: 0.63939, train loss2: 0.48875, train loss3: 0.30811\n",
      "train loss1: 0.60879, train loss2: 0.09821, train loss3: 0.51486\n",
      "train loss1: 0.62078, train loss2: 0.03335, train loss3: 1.44407\n",
      "train loss1: 0.61172, train loss2: 0.24064, train loss3: 1.42686\n",
      "train loss1: 0.61659, train loss2: 0.23712, train loss3: 19.64292\n",
      "train loss1: 0.60977, train loss2: 8.78730, train loss3: 0.72304\n",
      "train loss1: 0.62196, train loss2: 0.23810, train loss3: 0.24506\n",
      "train loss1: 0.62553, train loss2: 0.26970, train loss3: 2.72351\n",
      "train loss1: 0.61936, train loss2: 0.16708, train loss3: 0.74374\n",
      "train loss1: 0.56341, train loss2: 0.00697, train loss3: 0.03143\n",
      "Epoch: 5/10\n",
      "train loss1: 0.62001, train loss2: 0.09982, train loss3: 0.77252\n",
      "train loss1: 0.62595, train loss2: 0.04793, train loss3: 3.79258\n",
      "train loss1: 0.61831, train loss2: 0.24629, train loss3: 1.13157\n",
      "train loss1: 0.64422, train loss2: 0.14033, train loss3: 0.08098\n",
      "train loss1: 0.61759, train loss2: 0.32255, train loss3: 5.85703\n",
      "train loss1: 0.61606, train loss2: 4.18910, train loss3: 1.51766\n",
      "train loss1: 0.62265, train loss2: 0.50984, train loss3: 0.64906\n",
      "train loss1: 0.69968, train loss2: 0.23431, train loss3: 0.60873\n",
      "train loss1: 0.61219, train loss2: 0.10225, train loss3: 0.40715\n",
      "train loss1: 0.60252, train loss2: 2.24234, train loss3: 0.76045\n",
      "train loss1: 0.72069, train loss2: 0.09156, train loss3: 0.13143\n",
      "train loss1: 0.72326, train loss2: 0.04015, train loss3: 2.39720\n",
      "train loss1: 0.60957, train loss2: 0.01885, train loss3: 1.31650\n",
      "train loss1: 0.65482, train loss2: 0.04477, train loss3: 1.23159\n",
      "train loss1: 0.60777, train loss2: 0.08179, train loss3: 4.17971\n",
      "train loss1: 0.61097, train loss2: 3.83498, train loss3: 0.18149\n",
      "train loss1: 0.62447, train loss2: 0.09993, train loss3: 0.43493\n",
      "train loss1: 0.61620, train loss2: 0.09601, train loss3: 0.25999\n",
      "train loss1: 0.60548, train loss2: 0.20336, train loss3: 1.73138\n",
      "train loss1: 0.71060, train loss2: 0.07542, train loss3: 0.72955\n",
      "train loss1: 0.70599, train loss2: 0.10487, train loss3: 5.45393\n",
      "train loss1: 0.61412, train loss2: 0.12489, train loss3: 1.10771\n",
      "train loss1: 0.60617, train loss2: 0.05899, train loss3: 1.51357\n",
      "train loss1: 0.60201, train loss2: 0.08428, train loss3: 19.92665\n",
      "train loss1: 0.63966, train loss2: 1.81360, train loss3: 0.37792\n",
      "train loss1: 0.63015, train loss2: 0.04292, train loss3: 0.50061\n",
      "train loss1: 0.61733, train loss2: 0.05126, train loss3: 0.99201\n",
      "train loss1: 0.61444, train loss2: 0.02088, train loss3: 0.33987\n",
      "train loss1: 0.62113, train loss2: 0.26903, train loss3: 0.77400\n",
      "train loss1: 0.63439, train loss2: 1.65122, train loss3: 0.17771\n",
      "train loss1: 0.72436, train loss2: 0.88145, train loss3: 9.16760\n",
      "train loss1: 0.62571, train loss2: 0.23927, train loss3: 0.51602\n",
      "train loss1: 0.59986, train loss2: 0.23388, train loss3: 0.30200\n",
      "train loss1: 0.61854, train loss2: 0.07183, train loss3: 1.26745\n",
      "train loss1: 0.60901, train loss2: 1.81730, train loss3: 0.28746\n",
      "train loss1: 0.64092, train loss2: 0.59816, train loss3: 0.36238\n",
      "train loss1: 0.60436, train loss2: 0.08761, train loss3: 0.10995\n",
      "train loss1: 0.63561, train loss2: 0.02491, train loss3: 0.91355\n",
      "train loss1: 0.62871, train loss2: 0.92684, train loss3: 0.88287\n",
      "train loss1: 0.61839, train loss2: 0.02064, train loss3: 0.79087\n",
      "train loss1: 0.61626, train loss2: 0.37759, train loss3: 5.90775\n",
      "train loss1: 0.60675, train loss2: 0.04943, train loss3: 1.43993\n",
      "train loss1: 0.59820, train loss2: 0.04306, train loss3: 0.35895\n",
      "train loss1: 0.59870, train loss2: 0.07159, train loss3: 1.44008\n",
      "train loss1: 0.64113, train loss2: 0.16748, train loss3: 0.77978\n",
      "train loss1: 0.61159, train loss2: 0.17384, train loss3: 0.22435\n",
      "train loss1: 0.70900, train loss2: 0.05953, train loss3: 2.51482\n",
      "train loss1: 0.72052, train loss2: 111.87065, train loss3: 0.22439\n",
      "train loss1: 0.61466, train loss2: 0.06299, train loss3: 0.48721\n",
      "train loss1: 0.63831, train loss2: 2.07598, train loss3: 0.75785\n",
      "train loss1: 0.63200, train loss2: 0.13963, train loss3: 12.07220\n",
      "train loss1: 0.60513, train loss2: 0.79611, train loss3: 0.49566\n",
      "train loss1: 0.61900, train loss2: 0.44836, train loss3: 0.56346\n",
      "train loss1: 0.61535, train loss2: 0.08917, train loss3: 0.75894\n",
      "train loss1: 0.61033, train loss2: 0.11319, train loss3: 11.81774\n",
      "train loss1: 0.58340, train loss2: 0.04744, train loss3: 1.42536\n",
      "train loss1: 0.70577, train loss2: 0.02936, train loss3: 0.91806\n",
      "train loss1: 0.62437, train loss2: 0.10323, train loss3: 0.24082\n",
      "train loss1: 0.60449, train loss2: 0.10923, train loss3: 2.71960\n",
      "train loss1: 0.60440, train loss2: 0.06316, train loss3: 138.92613\n",
      "train loss1: 0.62130, train loss2: 0.31165, train loss3: 1.01345\n",
      "train loss1: 0.63250, train loss2: 2.25007, train loss3: 0.38514\n",
      "train loss1: 0.62514, train loss2: 0.14316, train loss3: 0.63477\n",
      "train loss1: 0.60054, train loss2: 0.19646, train loss3: 0.75542\n",
      "train loss1: 0.60524, train loss2: 0.04558, train loss3: 0.57801\n",
      "train loss1: 0.66727, train loss2: 0.02916, train loss3: 5.12075\n",
      "train loss1: 0.60691, train loss2: 1.07496, train loss3: 0.72032\n",
      "train loss1: 0.61646, train loss2: 0.04845, train loss3: 0.54594\n",
      "train loss1: 0.60610, train loss2: 0.09337, train loss3: 1.66957\n",
      "train loss1: 0.70180, train loss2: 0.05369, train loss3: 12.29148\n",
      "train loss1: 0.60020, train loss2: 0.02865, train loss3: 8.66213\n",
      "train loss1: 0.61099, train loss2: 0.05238, train loss3: 0.28980\n",
      "train loss1: 0.61847, train loss2: 0.04693, train loss3: 4.99301\n",
      "train loss1: 0.61343, train loss2: 0.18246, train loss3: 0.31416\n",
      "train loss1: 0.60364, train loss2: 0.27102, train loss3: 1.52150\n",
      "train loss1: 0.62396, train loss2: 4.28836, train loss3: 0.20171\n",
      "train loss1: 0.62264, train loss2: 0.39998, train loss3: 0.52981\n",
      "train loss1: 0.64770, train loss2: 0.34562, train loss3: 1.89075\n",
      "train loss1: 0.74384, train loss2: 0.02395, train loss3: 0.28657\n",
      "train loss1: 0.59554, train loss2: 0.06227, train loss3: 0.74749\n",
      "train loss1: 0.61496, train loss2: 0.02387, train loss3: 1.42986\n",
      "train loss1: 0.59945, train loss2: 0.39919, train loss3: 0.65488\n",
      "train loss1: 0.64660, train loss2: 0.12310, train loss3: 25.49984\n",
      "train loss1: 0.60804, train loss2: 0.08100, train loss3: 0.45723\n",
      "train loss1: 0.59133, train loss2: 0.02300, train loss3: 0.16426\n",
      "train loss1: 0.60718, train loss2: 0.05337, train loss3: 1.35995\n",
      "train loss1: 0.61188, train loss2: 0.19996, train loss3: 1.21822\n",
      "train loss1: 0.61512, train loss2: 0.07383, train loss3: 0.62090\n",
      "train loss1: 0.63102, train loss2: 0.02632, train loss3: 0.10299\n",
      "train loss1: 0.65973, train loss2: 0.03785, train loss3: 2.15027\n",
      "train loss1: 0.75453, train loss2: 0.26964, train loss3: 0.93555\n",
      "train loss1: 0.62212, train loss2: 2.06018, train loss3: 1.00226\n",
      "train loss1: 0.63004, train loss2: 0.06588, train loss3: 0.18444\n",
      "train loss1: 0.77966, train loss2: 0.03232, train loss3: 0.33568\n",
      "train loss1: 0.59261, train loss2: 0.05800, train loss3: 0.70569\n",
      "train loss1: 0.68720, train loss2: 2.78081, train loss3: 0.23402\n",
      "train loss1: 0.59599, train loss2: 0.05192, train loss3: 0.17604\n",
      "train loss1: 0.60729, train loss2: 0.02142, train loss3: 0.11555\n",
      "train loss1: 0.60489, train loss2: 0.08164, train loss3: 4.34696\n",
      "train loss1: 0.70479, train loss2: 0.06997, train loss3: 0.21132\n",
      "train loss1: 0.60829, train loss2: 0.06204, train loss3: 1.36513\n",
      "train loss1: 0.62589, train loss2: 0.22606, train loss3: 2.36656\n",
      "train loss1: 0.60478, train loss2: 0.08000, train loss3: 0.57253\n",
      "train loss1: 0.59376, train loss2: 2.68365, train loss3: 0.06225\n",
      "train loss1: 0.58730, train loss2: 0.04284, train loss3: 0.54992\n",
      "train loss1: 0.61003, train loss2: 0.08580, train loss3: 6.33277\n",
      "train loss1: 0.60739, train loss2: 0.13287, train loss3: 1.72430\n",
      "train loss1: 0.64665, train loss2: 0.03354, train loss3: 1.09933\n",
      "train loss1: 0.70340, train loss2: 0.05034, train loss3: 0.95284\n",
      "train loss1: 0.59689, train loss2: 0.77593, train loss3: 3.86040\n",
      "train loss1: 0.60942, train loss2: 0.20101, train loss3: 2.38275\n",
      "train loss1: 0.60491, train loss2: 0.04710, train loss3: 0.79390\n",
      "train loss1: 0.61282, train loss2: 0.76457, train loss3: 2.92788\n",
      "train loss1: 0.60976, train loss2: 0.06133, train loss3: 1.81111\n",
      "train loss1: 0.60051, train loss2: 0.04889, train loss3: 7.88941\n",
      "train loss1: 0.61415, train loss2: 0.43319, train loss3: 0.82441\n",
      "train loss1: 0.60684, train loss2: 0.29000, train loss3: 12.00541\n",
      "train loss1: 0.59461, train loss2: 0.82865, train loss3: 0.38567\n",
      "train loss1: 0.57972, train loss2: 0.10728, train loss3: 0.21976\n",
      "train loss1: 0.61466, train loss2: 0.69974, train loss3: 0.31687\n",
      "train loss1: 0.61214, train loss2: 0.02632, train loss3: 1.88581\n",
      "train loss1: 0.62882, train loss2: 0.42565, train loss3: 0.14037\n",
      "train loss1: 0.59394, train loss2: 0.01249, train loss3: 2.16015\n",
      "train loss1: 0.61122, train loss2: 2.05856, train loss3: 0.63796\n",
      "train loss1: 0.62295, train loss2: 0.73807, train loss3: 4.04393\n",
      "train loss1: 0.61240, train loss2: 0.11664, train loss3: 0.64903\n",
      "train loss1: 0.61128, train loss2: 0.80451, train loss3: 0.42053\n",
      "train loss1: 0.60554, train loss2: 0.67518, train loss3: 1.10288\n",
      "train loss1: 0.62822, train loss2: 0.53122, train loss3: 20.00680\n",
      "train loss1: 0.62925, train loss2: 0.04833, train loss3: 0.59879\n",
      "train loss1: 0.60766, train loss2: 0.07067, train loss3: 2.03645\n",
      "train loss1: 0.59933, train loss2: 0.03160, train loss3: 0.55368\n",
      "train loss1: 0.60111, train loss2: 0.22197, train loss3: 0.51314\n",
      "train loss1: 0.62761, train loss2: 0.04539, train loss3: 1.25334\n",
      "train loss1: 0.62462, train loss2: 0.19450, train loss3: 28.57223\n",
      "train loss1: 0.60296, train loss2: 0.06729, train loss3: 0.19521\n",
      "train loss1: 0.60430, train loss2: 0.75194, train loss3: 0.42438\n",
      "train loss1: 0.60839, train loss2: 0.41958, train loss3: 1.77001\n",
      "train loss1: 0.59427, train loss2: 0.28299, train loss3: 2.04010\n",
      "train loss1: 0.61897, train loss2: 0.07063, train loss3: 1.01009\n",
      "train loss1: 0.59498, train loss2: 0.31954, train loss3: 0.48286\n",
      "train loss1: 0.61074, train loss2: 46.81275, train loss3: 5.16169\n",
      "train loss1: 0.62957, train loss2: 0.23466, train loss3: 0.53221\n",
      "train loss1: 0.62587, train loss2: 0.07579, train loss3: 1.59592\n",
      "train loss1: 0.61484, train loss2: 0.10204, train loss3: 0.73796\n",
      "train loss1: 0.62313, train loss2: 1.78573, train loss3: 0.09450\n",
      "train loss1: 0.60927, train loss2: 0.41747, train loss3: 0.32721\n",
      "train loss1: 0.60332, train loss2: 0.59498, train loss3: 0.32432\n",
      "train loss1: 0.58752, train loss2: 0.22104, train loss3: 0.76898\n",
      "train loss1: 0.60813, train loss2: 0.82641, train loss3: 1.41364\n",
      "train loss1: 0.60448, train loss2: 0.04296, train loss3: 0.71446\n",
      "train loss1: 0.62406, train loss2: 0.06011, train loss3: 0.28969\n",
      "train loss1: 0.60609, train loss2: 0.16111, train loss3: 1.28544\n",
      "train loss1: 0.60321, train loss2: 0.04805, train loss3: 1.11204\n",
      "train loss1: 0.59752, train loss2: 0.08346, train loss3: 4.93773\n",
      "train loss1: 0.59332, train loss2: 0.03086, train loss3: 0.51222\n",
      "train loss1: 0.60368, train loss2: 0.05675, train loss3: 15.25167\n",
      "train loss1: 0.60504, train loss2: 0.11111, train loss3: 16.47982\n",
      "train loss1: 0.68479, train loss2: 0.31030, train loss3: 1.01223\n",
      "train loss1: 0.69493, train loss2: 1.65342, train loss3: 0.39566\n",
      "train loss1: 0.60091, train loss2: 0.65633, train loss3: 0.40017\n",
      "train loss1: 0.60274, train loss2: 0.18906, train loss3: 0.44957\n",
      "train loss1: 0.58914, train loss2: 0.03798, train loss3: 1.23856\n",
      "train loss1: 0.61640, train loss2: 0.04548, train loss3: 0.23950\n",
      "train loss1: 0.58334, train loss2: 3.86707, train loss3: 1.20039\n",
      "train loss1: 0.61938, train loss2: 0.14773, train loss3: 1.00655\n",
      "train loss1: 0.61163, train loss2: 0.04872, train loss3: 0.50969\n",
      "train loss1: 0.60057, train loss2: 0.04588, train loss3: 1.66452\n",
      "train loss1: 0.69778, train loss2: 0.02200, train loss3: 2.10950\n",
      "train loss1: 0.65993, train loss2: 0.06208, train loss3: 0.26070\n",
      "train loss1: 0.60058, train loss2: 8.87870, train loss3: 1.81476\n",
      "train loss1: 0.59889, train loss2: 0.02061, train loss3: 1.14485\n",
      "train loss1: 0.58988, train loss2: 48.39400, train loss3: 0.33909\n",
      "train loss1: 0.60307, train loss2: 0.19221, train loss3: 2.35836\n",
      "train loss1: 0.59369, train loss2: 0.04504, train loss3: 0.63257\n",
      "train loss1: 0.59974, train loss2: 0.09191, train loss3: 3.62181\n",
      "train loss1: 0.62414, train loss2: 0.58170, train loss3: 1.67268\n",
      "train loss1: 0.63092, train loss2: 0.03064, train loss3: 184.49577\n",
      "train loss1: 0.60180, train loss2: 0.11558, train loss3: 1.26728\n",
      "train loss1: 0.61241, train loss2: 0.12541, train loss3: 0.19681\n",
      "train loss1: 0.59811, train loss2: 0.04729, train loss3: 0.54129\n",
      "train loss1: 0.61733, train loss2: 0.03960, train loss3: 2.06005\n",
      "train loss1: 0.59974, train loss2: 0.02756, train loss3: 4.96325\n",
      "train loss1: 0.58808, train loss2: 147.12541, train loss3: 0.70202\n",
      "train loss1: 0.60399, train loss2: 0.09705, train loss3: 0.54846\n",
      "train loss1: 0.61090, train loss2: 0.20229, train loss3: 0.34539\n",
      "train loss1: 0.60371, train loss2: 0.59869, train loss3: 0.20769\n",
      "train loss1: 0.59964, train loss2: 0.21845, train loss3: 0.11754\n",
      "train loss1: 0.60748, train loss2: 0.26963, train loss3: 2.22758\n",
      "train loss1: 0.65663, train loss2: 0.16375, train loss3: 3.45332\n",
      "train loss1: 0.64309, train loss2: 0.99158, train loss3: 2.19203\n",
      "train loss1: 0.62173, train loss2: 0.06994, train loss3: 0.52435\n",
      "train loss1: 0.59439, train loss2: 0.26496, train loss3: 237.79822\n",
      "train loss1: 0.69548, train loss2: 1.40975, train loss3: 1.45031\n",
      "train loss1: 0.70668, train loss2: 0.05453, train loss3: 0.38498\n",
      "train loss1: 0.62780, train loss2: 192.06023, train loss3: 1.47534\n",
      "train loss1: 0.61145, train loss2: 0.03496, train loss3: 1.17965\n",
      "train loss1: 0.59245, train loss2: 0.27179, train loss3: 0.26149\n",
      "train loss1: 0.57573, train loss2: 0.01322, train loss3: 0.47817\n",
      "train loss1: 0.59084, train loss2: 0.23306, train loss3: 1.25394\n",
      "train loss1: 0.63067, train loss2: 1.91607, train loss3: 1.02018\n",
      "train loss1: 0.60853, train loss2: 0.05583, train loss3: 2.09487\n",
      "train loss1: 0.61923, train loss2: 0.08641, train loss3: 0.33348\n",
      "train loss1: 0.61204, train loss2: 2.67317, train loss3: 0.30161\n",
      "train loss1: 0.58938, train loss2: 0.06352, train loss3: 0.78166\n",
      "train loss1: 0.69249, train loss2: 4.72665, train loss3: 12.34165\n",
      "train loss1: 0.58891, train loss2: 0.04848, train loss3: 0.49025\n",
      "train loss1: 0.59579, train loss2: 0.03549, train loss3: 0.23212\n",
      "train loss1: 0.61433, train loss2: 0.34499, train loss3: 27.96394\n",
      "train loss1: 0.59637, train loss2: 0.06142, train loss3: 55.21053\n",
      "train loss1: 0.59548, train loss2: 0.04335, train loss3: 4.25339\n",
      "train loss1: 0.62143, train loss2: 0.11110, train loss3: 0.54125\n",
      "train loss1: 0.60387, train loss2: 0.22651, train loss3: 3.67568\n",
      "train loss1: 0.65563, train loss2: 0.08149, train loss3: 1.86578\n",
      "train loss1: 0.59342, train loss2: 0.02643, train loss3: 0.43716\n",
      "train loss1: 0.61336, train loss2: 0.08713, train loss3: 0.58974\n",
      "train loss1: 0.62475, train loss2: 0.06569, train loss3: 2.25302\n",
      "train loss1: 0.59735, train loss2: 0.08838, train loss3: 0.15480\n",
      "train loss1: 0.58577, train loss2: 0.01740, train loss3: 0.51144\n",
      "train loss1: 0.67966, train loss2: 0.26572, train loss3: 0.64701\n",
      "train loss1: 0.59481, train loss2: 0.09953, train loss3: 15.92215\n",
      "train loss1: 0.68937, train loss2: 8.64992, train loss3: 0.21709\n",
      "train loss1: 0.60739, train loss2: 11.51128, train loss3: 8.83456\n",
      "train loss1: 0.60112, train loss2: 0.54493, train loss3: 4.41236\n",
      "train loss1: 0.61223, train loss2: 0.49668, train loss3: 5.87809\n",
      "train loss1: 0.63276, train loss2: 0.06067, train loss3: 0.51282\n",
      "train loss1: 0.61405, train loss2: 0.33186, train loss3: 0.99900\n",
      "train loss1: 0.60438, train loss2: 0.05169, train loss3: 1.41457\n",
      "train loss1: 0.59668, train loss2: 1.18435, train loss3: 0.55643\n",
      "train loss1: 0.63100, train loss2: 0.04843, train loss3: 0.36647\n",
      "train loss1: 0.59484, train loss2: 0.04784, train loss3: 1.21335\n",
      "train loss1: 0.61697, train loss2: 0.15901, train loss3: 0.41600\n",
      "train loss1: 0.59471, train loss2: 0.64235, train loss3: 0.29466\n",
      "train loss1: 0.66631, train loss2: 0.06836, train loss3: 0.75356\n",
      "train loss1: 0.60520, train loss2: 1.05882, train loss3: 0.77415\n",
      "train loss1: 0.58074, train loss2: 0.20244, train loss3: 19.47095\n",
      "train loss1: 0.70047, train loss2: 0.09941, train loss3: 0.18534\n",
      "train loss1: 0.59218, train loss2: 0.07283, train loss3: 0.58748\n",
      "train loss1: 0.58216, train loss2: 0.08365, train loss3: 0.05624\n",
      "train loss1: 0.58791, train loss2: 0.09281, train loss3: 0.32188\n",
      "train loss1: 0.60921, train loss2: 45.40546, train loss3: 0.29605\n",
      "train loss1: 0.58171, train loss2: 0.07748, train loss3: 8.19098\n",
      "train loss1: 0.59500, train loss2: 0.02307, train loss3: 0.19555\n",
      "train loss1: 0.59974, train loss2: 0.19365, train loss3: 28.39829\n",
      "train loss1: 0.58868, train loss2: 3.37438, train loss3: 1.41081\n",
      "train loss1: 0.60543, train loss2: 0.27863, train loss3: 0.65149\n",
      "train loss1: 0.59062, train loss2: 0.03587, train loss3: 0.56973\n",
      "train loss1: 0.59322, train loss2: 0.18825, train loss3: 1.24840\n",
      "train loss1: 0.59145, train loss2: 0.06087, train loss3: 0.67722\n",
      "train loss1: 0.59308, train loss2: 0.05098, train loss3: 0.36140\n",
      "train loss1: 0.58549, train loss2: 0.10247, train loss3: 1.29103\n",
      "train loss1: 0.60399, train loss2: 0.06968, train loss3: 0.22041\n",
      "train loss1: 0.62024, train loss2: 0.14943, train loss3: 1.98361\n",
      "train loss1: 0.58450, train loss2: 0.31933, train loss3: 0.37231\n",
      "train loss1: 0.58644, train loss2: 0.17234, train loss3: 4.36592\n",
      "train loss1: 0.61084, train loss2: 0.07684, train loss3: 2.52184\n",
      "train loss1: 0.59292, train loss2: 0.04795, train loss3: 0.04388\n",
      "train loss1: 0.60144, train loss2: 0.19844, train loss3: 0.24315\n",
      "train loss1: 0.58975, train loss2: 0.04688, train loss3: 0.35766\n",
      "train loss1: 0.60451, train loss2: 0.02419, train loss3: 0.21773\n",
      "train loss1: 0.61650, train loss2: 0.11425, train loss3: 17.69547\n",
      "train loss1: 0.58454, train loss2: 0.30996, train loss3: 0.95075\n",
      "train loss1: 0.60457, train loss2: 0.19465, train loss3: 1.48611\n",
      "train loss1: 0.60420, train loss2: 0.05541, train loss3: 1.84038\n",
      "train loss1: 0.60262, train loss2: 0.21670, train loss3: 1.77998\n",
      "train loss1: 0.58633, train loss2: 0.15201, train loss3: 3.72597\n",
      "train loss1: 0.69395, train loss2: 0.09928, train loss3: 1.44255\n",
      "train loss1: 0.59823, train loss2: 0.06361, train loss3: 0.99709\n",
      "train loss1: 0.60695, train loss2: 0.92683, train loss3: 6.89679\n",
      "train loss1: 0.60615, train loss2: 1.27285, train loss3: 0.92481\n",
      "train loss1: 0.59755, train loss2: 0.07810, train loss3: 1.03268\n",
      "train loss1: 0.59917, train loss2: 0.06762, train loss3: 0.32419\n",
      "train loss1: 0.59630, train loss2: 0.02698, train loss3: 1.34758\n",
      "train loss1: 0.63544, train loss2: 15.09520, train loss3: 0.22733\n",
      "train loss1: 0.64585, train loss2: 1.33530, train loss3: 0.19263\n",
      "train loss1: 0.58979, train loss2: 0.04897, train loss3: 0.91826\n",
      "train loss1: 0.59397, train loss2: 0.09536, train loss3: 1.45686\n",
      "train loss1: 0.59423, train loss2: 0.15433, train loss3: 0.33796\n",
      "train loss1: 0.61019, train loss2: 0.19973, train loss3: 1.33712\n",
      "train loss1: 0.58948, train loss2: 0.32881, train loss3: 0.67689\n",
      "train loss1: 0.60413, train loss2: 0.05844, train loss3: 0.69354\n",
      "train loss1: 0.60865, train loss2: 0.05517, train loss3: 1.75936\n",
      "train loss1: 0.59611, train loss2: 0.13656, train loss3: 1.57507\n",
      "train loss1: 0.59493, train loss2: 1.94552, train loss3: 0.20972\n",
      "train loss1: 0.56912, train loss2: 0.03891, train loss3: 9.73203\n",
      "train loss1: 0.72969, train loss2: 0.03875, train loss3: 0.66595\n",
      "train loss1: 0.60746, train loss2: 0.04382, train loss3: 0.36986\n",
      "train loss1: 0.60921, train loss2: 0.09353, train loss3: 1.57368\n",
      "train loss1: 0.59621, train loss2: 0.16685, train loss3: 0.29187\n",
      "train loss1: 0.58343, train loss2: 0.24149, train loss3: 0.38523\n",
      "train loss1: 0.62265, train loss2: 0.02469, train loss3: 0.92206\n",
      "train loss1: 0.58926, train loss2: 0.03846, train loss3: 0.24449\n",
      "train loss1: 0.59617, train loss2: 0.04043, train loss3: 0.16936\n",
      "train loss1: 0.59259, train loss2: 0.03848, train loss3: 2.79542\n",
      "train loss1: 0.60606, train loss2: 0.35485, train loss3: 9.56812\n",
      "train loss1: 0.69310, train loss2: 0.03402, train loss3: 1.24250\n",
      "train loss1: 0.54989, train loss2: 0.17772, train loss3: 0.09902\n",
      "Epoch: 6/10\n",
      "train loss1: 0.58433, train loss2: 0.13458, train loss3: 2.14046\n",
      "train loss1: 0.57843, train loss2: 0.08638, train loss3: 0.37010\n",
      "train loss1: 0.59633, train loss2: 0.23872, train loss3: 0.94624\n",
      "train loss1: 0.59250, train loss2: 0.08034, train loss3: 3.63497\n",
      "train loss1: 0.70129, train loss2: 0.13494, train loss3: 0.30980\n",
      "train loss1: 0.59877, train loss2: 0.02236, train loss3: 1.15318\n",
      "train loss1: 0.66843, train loss2: 0.05691, train loss3: 1.36270\n",
      "train loss1: 0.58841, train loss2: 0.08162, train loss3: 0.99198\n",
      "train loss1: 0.58832, train loss2: 0.59446, train loss3: 0.79453\n",
      "train loss1: 0.58850, train loss2: 0.16382, train loss3: 0.79315\n",
      "train loss1: 0.59474, train loss2: 0.06347, train loss3: 0.64502\n",
      "train loss1: 0.61983, train loss2: 0.04932, train loss3: 0.53876\n",
      "train loss1: 0.58642, train loss2: 0.09796, train loss3: 0.84686\n",
      "train loss1: 0.59542, train loss2: 14.82902, train loss3: 0.19289\n",
      "train loss1: 0.62386, train loss2: 0.08423, train loss3: 2.17053\n",
      "train loss1: 0.59310, train loss2: 0.07634, train loss3: 0.87753\n",
      "train loss1: 0.59933, train loss2: 0.03314, train loss3: 0.43258\n",
      "train loss1: 0.58026, train loss2: 0.08461, train loss3: 0.62903\n",
      "train loss1: 0.59481, train loss2: 0.05778, train loss3: 0.49419\n",
      "train loss1: 0.58289, train loss2: 147.02190, train loss3: 0.20084\n",
      "train loss1: 0.59556, train loss2: 0.17791, train loss3: 0.60415\n",
      "train loss1: 0.59846, train loss2: 0.81060, train loss3: 0.37782\n",
      "train loss1: 0.60993, train loss2: 0.09692, train loss3: 0.79184\n",
      "train loss1: 0.60192, train loss2: 0.03146, train loss3: 0.74064\n",
      "train loss1: 0.60112, train loss2: 0.03074, train loss3: 18.78319\n",
      "train loss1: 0.65689, train loss2: 0.20252, train loss3: 1.57571\n",
      "train loss1: 0.59178, train loss2: 0.07138, train loss3: 1.60371\n",
      "train loss1: 0.59857, train loss2: 0.15555, train loss3: 0.14189\n",
      "train loss1: 0.59227, train loss2: 0.30201, train loss3: 0.34259\n",
      "train loss1: 0.59882, train loss2: 0.28309, train loss3: 0.12323\n",
      "train loss1: 0.61260, train loss2: 0.39593, train loss3: 2.11468\n",
      "train loss1: 0.58610, train loss2: 0.03886, train loss3: 0.16404\n",
      "train loss1: 0.62917, train loss2: 0.06427, train loss3: 12.01753\n",
      "train loss1: 0.59894, train loss2: 0.03702, train loss3: 1.11324\n",
      "train loss1: 0.57942, train loss2: 55.53143, train loss3: 0.68252\n",
      "train loss1: 0.69229, train loss2: 0.04811, train loss3: 10.11874\n",
      "train loss1: 0.58791, train loss2: 0.45043, train loss3: 0.20090\n",
      "train loss1: 0.59862, train loss2: 0.05634, train loss3: 1.54297\n",
      "train loss1: 0.59889, train loss2: 0.04309, train loss3: 0.57961\n",
      "train loss1: 0.68599, train loss2: 0.02944, train loss3: 1.42761\n",
      "train loss1: 0.61271, train loss2: 0.03597, train loss3: 1.05946\n",
      "train loss1: 0.57593, train loss2: 0.95996, train loss3: 0.51136\n",
      "train loss1: 0.61584, train loss2: 0.09256, train loss3: 0.36595\n",
      "train loss1: 0.59649, train loss2: 0.14203, train loss3: 0.35403\n",
      "train loss1: 0.60660, train loss2: 0.12040, train loss3: 0.77264\n",
      "train loss1: 0.57800, train loss2: 0.07395, train loss3: 1.06816\n",
      "train loss1: 0.60400, train loss2: 0.02621, train loss3: 0.51722\n",
      "train loss1: 0.60800, train loss2: 0.06248, train loss3: 12.59691\n",
      "train loss1: 0.63043, train loss2: 0.05133, train loss3: 0.13626\n",
      "train loss1: 0.58601, train loss2: 0.07189, train loss3: 1.29312\n",
      "train loss1: 0.58913, train loss2: 0.87457, train loss3: 19.82295\n",
      "train loss1: 0.76709, train loss2: 0.11043, train loss3: 0.47727\n",
      "train loss1: 0.58812, train loss2: 0.28585, train loss3: 1.82152\n",
      "train loss1: 0.60192, train loss2: 0.40711, train loss3: 0.44358\n",
      "train loss1: 0.58164, train loss2: 0.06584, train loss3: 1.72701\n",
      "train loss1: 0.67682, train loss2: 0.21803, train loss3: 3.12580\n",
      "train loss1: 0.59020, train loss2: 0.05245, train loss3: 1.23970\n",
      "train loss1: 0.57972, train loss2: 0.04380, train loss3: 0.32769\n",
      "train loss1: 0.60623, train loss2: 0.91396, train loss3: 0.44236\n",
      "train loss1: 0.60770, train loss2: 0.84851, train loss3: 0.34398\n",
      "train loss1: 0.64456, train loss2: 0.05545, train loss3: 1.04572\n",
      "train loss1: 0.57733, train loss2: 0.43625, train loss3: 0.34221\n",
      "train loss1: 0.59738, train loss2: 2.52565, train loss3: 0.58936\n",
      "train loss1: 0.59464, train loss2: 0.31730, train loss3: 139.50835\n",
      "train loss1: 0.62253, train loss2: 0.06717, train loss3: 0.14803\n",
      "train loss1: 0.59297, train loss2: 0.06775, train loss3: 3.23020\n",
      "train loss1: 0.60869, train loss2: 0.25202, train loss3: 0.18837\n",
      "train loss1: 0.58487, train loss2: 0.08377, train loss3: 0.07324\n",
      "train loss1: 0.68834, train loss2: 0.02735, train loss3: 0.16430\n",
      "train loss1: 0.59504, train loss2: 0.17970, train loss3: 0.80739\n",
      "train loss1: 0.61593, train loss2: 0.05728, train loss3: 0.43955\n",
      "train loss1: 0.59069, train loss2: 0.10274, train loss3: 0.65104\n",
      "train loss1: 0.59329, train loss2: 0.10301, train loss3: 0.38579\n",
      "train loss1: 0.68796, train loss2: 0.03177, train loss3: 0.82450\n",
      "train loss1: 0.58495, train loss2: 0.06268, train loss3: 0.65456\n",
      "train loss1: 0.59991, train loss2: 0.04925, train loss3: 0.75140\n",
      "train loss1: 0.58630, train loss2: 0.06620, train loss3: 0.22536\n",
      "train loss1: 0.58215, train loss2: 0.08234, train loss3: 0.47070\n",
      "train loss1: 0.58796, train loss2: 0.04526, train loss3: 0.06665\n",
      "train loss1: 0.59012, train loss2: 0.30679, train loss3: 0.48718\n",
      "train loss1: 0.60413, train loss2: 0.05086, train loss3: 0.90005\n",
      "train loss1: 0.58281, train loss2: 0.12861, train loss3: 0.21911\n",
      "train loss1: 0.59383, train loss2: 0.10143, train loss3: 0.83132\n",
      "train loss1: 0.59623, train loss2: 0.06958, train loss3: 21.10931\n",
      "train loss1: 0.60012, train loss2: 0.16732, train loss3: 0.20938\n",
      "train loss1: 0.56101, train loss2: 0.03142, train loss3: 9.08575\n",
      "train loss1: 0.56228, train loss2: 0.33788, train loss3: 1.36741\n",
      "train loss1: 0.58417, train loss2: 0.03253, train loss3: 1.07585\n",
      "train loss1: 0.59683, train loss2: 0.04226, train loss3: 0.17844\n",
      "train loss1: 0.57816, train loss2: 0.18220, train loss3: 0.66028\n",
      "train loss1: 0.59268, train loss2: 0.26635, train loss3: 9.01192\n",
      "train loss1: 0.57159, train loss2: 0.92730, train loss3: 0.68066\n",
      "train loss1: 0.58995, train loss2: 1.20335, train loss3: 0.57110\n",
      "train loss1: 0.58376, train loss2: 0.12278, train loss3: 0.82657\n",
      "train loss1: 0.62922, train loss2: 0.02538, train loss3: 0.88554\n",
      "train loss1: 0.58778, train loss2: 0.04632, train loss3: 1.90980\n",
      "train loss1: 0.65255, train loss2: 3.32215, train loss3: 0.44499\n",
      "train loss1: 0.58881, train loss2: 0.29015, train loss3: 0.19519\n",
      "train loss1: 0.58985, train loss2: 4.37131, train loss3: 0.29387\n",
      "train loss1: 0.58866, train loss2: 0.30095, train loss3: 6.66775\n",
      "train loss1: 0.58592, train loss2: 0.62715, train loss3: 0.47829\n",
      "train loss1: 0.59183, train loss2: 3.78493, train loss3: 0.87859\n",
      "train loss1: 0.59716, train loss2: 0.49432, train loss3: 0.35596\n",
      "train loss1: 0.57164, train loss2: 0.06235, train loss3: 0.40250\n",
      "train loss1: 0.56613, train loss2: 0.08633, train loss3: 0.31079\n",
      "train loss1: 0.59483, train loss2: 0.63570, train loss3: 0.53616\n",
      "train loss1: 0.58283, train loss2: 0.39177, train loss3: 0.98393\n",
      "train loss1: 0.69656, train loss2: 0.05745, train loss3: 0.25803\n",
      "train loss1: 0.59661, train loss2: 0.02596, train loss3: 0.57448\n",
      "train loss1: 0.58739, train loss2: 0.82376, train loss3: 0.87071\n",
      "train loss1: 0.58443, train loss2: 0.31843, train loss3: 2.40085\n",
      "train loss1: 0.59108, train loss2: 0.11629, train loss3: 0.65855\n",
      "train loss1: 0.58724, train loss2: 0.31559, train loss3: 4.44412\n",
      "train loss1: 0.61770, train loss2: 0.10258, train loss3: 0.80515\n",
      "train loss1: 0.58837, train loss2: 0.40363, train loss3: 0.37441\n",
      "train loss1: 0.58240, train loss2: 0.04579, train loss3: 0.43176\n",
      "train loss1: 0.56970, train loss2: 0.13180, train loss3: 0.17428\n",
      "train loss1: 0.57147, train loss2: 0.03293, train loss3: 0.89689\n",
      "train loss1: 0.58830, train loss2: 0.03858, train loss3: 1.75705\n",
      "train loss1: 0.57172, train loss2: 4.40443, train loss3: 1.20629\n",
      "train loss1: 0.59514, train loss2: 0.11125, train loss3: 0.98287\n",
      "train loss1: 0.62387, train loss2: 0.96960, train loss3: 0.40958\n",
      "train loss1: 0.59623, train loss2: 0.46308, train loss3: 0.97267\n",
      "train loss1: 0.59577, train loss2: 0.08251, train loss3: 4.67067\n",
      "train loss1: 0.67351, train loss2: 0.03035, train loss3: 1.20241\n",
      "train loss1: 0.58909, train loss2: 0.07326, train loss3: 1.43651\n",
      "train loss1: 0.60129, train loss2: 0.26833, train loss3: 0.66524\n",
      "train loss1: 0.57082, train loss2: 0.82738, train loss3: 29.02964\n",
      "train loss1: 0.58234, train loss2: 0.07307, train loss3: 1.78961\n",
      "train loss1: 0.58522, train loss2: 1.93428, train loss3: 0.11403\n",
      "train loss1: 0.59867, train loss2: 0.04580, train loss3: 0.24133\n",
      "train loss1: 0.66182, train loss2: 0.15347, train loss3: 1.16165\n",
      "train loss1: 0.57973, train loss2: 0.37532, train loss3: 0.25920\n",
      "train loss1: 0.57741, train loss2: 0.02581, train loss3: 2.74436\n",
      "train loss1: 0.58506, train loss2: 0.16110, train loss3: 0.27709\n",
      "train loss1: 0.58901, train loss2: 0.19746, train loss3: 0.36622\n",
      "train loss1: 0.58185, train loss2: 1.33819, train loss3: 6.22042\n",
      "train loss1: 0.57739, train loss2: 11.37535, train loss3: 0.37073\n",
      "train loss1: 0.60736, train loss2: 0.14642, train loss3: 0.21749\n",
      "train loss1: 0.61256, train loss2: 0.42397, train loss3: 240.09250\n",
      "train loss1: 0.57696, train loss2: 1.61617, train loss3: 0.28208\n",
      "train loss1: 0.58686, train loss2: 192.12700, train loss3: 0.60570\n",
      "train loss1: 0.60099, train loss2: 0.06550, train loss3: 0.26272\n",
      "train loss1: 0.56944, train loss2: 0.56034, train loss3: 0.22006\n",
      "train loss1: 0.61693, train loss2: 0.08351, train loss3: 0.13836\n",
      "train loss1: 0.56878, train loss2: 0.16623, train loss3: 2.84067\n",
      "train loss1: 0.57646, train loss2: 0.09226, train loss3: 1.93616\n",
      "train loss1: 0.58859, train loss2: 0.13523, train loss3: 0.27736\n",
      "train loss1: 0.59706, train loss2: 0.22223, train loss3: 0.25201\n",
      "train loss1: 0.58341, train loss2: 0.13212, train loss3: 0.35418\n",
      "train loss1: 0.63268, train loss2: 0.11425, train loss3: 0.76525\n",
      "train loss1: 0.58175, train loss2: 0.08863, train loss3: 0.48821\n",
      "train loss1: 0.63757, train loss2: 45.39279, train loss3: 7.24136\n",
      "train loss1: 0.58478, train loss2: 0.60633, train loss3: 1.25050\n",
      "train loss1: 0.59378, train loss2: 0.17806, train loss3: 3.80687\n",
      "train loss1: 0.57869, train loss2: 0.04486, train loss3: 0.30412\n",
      "train loss1: 0.59406, train loss2: 0.13999, train loss3: 2.30939\n",
      "train loss1: 0.58229, train loss2: 0.40354, train loss3: 0.55000\n",
      "train loss1: 0.57107, train loss2: 0.69568, train loss3: 2.62700\n",
      "train loss1: 0.60474, train loss2: 0.11876, train loss3: 4.23812\n",
      "train loss1: 0.58747, train loss2: 0.17157, train loss3: 0.17847\n",
      "train loss1: 0.57744, train loss2: 0.07103, train loss3: 0.48315\n",
      "train loss1: 0.60485, train loss2: 0.03103, train loss3: 1.23103\n",
      "train loss1: 0.58378, train loss2: 0.14016, train loss3: 1.19312\n",
      "train loss1: 0.56935, train loss2: 0.07764, train loss3: 10.46834\n",
      "train loss1: 0.57276, train loss2: 2.72084, train loss3: 0.65031\n",
      "train loss1: 0.78387, train loss2: 0.02960, train loss3: 1.97014\n",
      "train loss1: 0.56096, train loss2: 0.07403, train loss3: 0.97980\n",
      "train loss1: 0.58063, train loss2: 0.35495, train loss3: 1.11578\n",
      "train loss1: 0.59658, train loss2: 0.06015, train loss3: 2.38119\n",
      "train loss1: 0.59728, train loss2: 0.13651, train loss3: 2.62895\n",
      "train loss1: 0.58142, train loss2: 0.08371, train loss3: 0.43469\n",
      "train loss1: 0.58152, train loss2: 0.09815, train loss3: 5.15797\n",
      "train loss1: 0.57115, train loss2: 2.01831, train loss3: 1.21875\n",
      "train loss1: 0.61435, train loss2: 112.25720, train loss3: 1.66533\n",
      "train loss1: 0.57544, train loss2: 0.12574, train loss3: 0.92582\n",
      "train loss1: 0.58000, train loss2: 0.02703, train loss3: 0.28295\n",
      "train loss1: 0.59301, train loss2: 0.12749, train loss3: 55.61190\n",
      "train loss1: 0.59487, train loss2: 8.64601, train loss3: 0.09623\n",
      "train loss1: 0.69037, train loss2: 0.09619, train loss3: 0.35386\n",
      "train loss1: 0.57749, train loss2: 0.02819, train loss3: 16.51197\n",
      "train loss1: 0.57980, train loss2: 0.07480, train loss3: 0.19603\n",
      "train loss1: 0.60389, train loss2: 2.68888, train loss3: 6.45836\n",
      "train loss1: 0.59280, train loss2: 0.08028, train loss3: 19.60556\n",
      "train loss1: 0.57578, train loss2: 0.07280, train loss3: 1.62243\n",
      "train loss1: 0.68176, train loss2: 0.13289, train loss3: 3.02348\n",
      "train loss1: 0.57618, train loss2: 0.09869, train loss3: 0.84527\n",
      "train loss1: 0.58224, train loss2: 0.08275, train loss3: 2.64002\n",
      "train loss1: 0.58960, train loss2: 0.04814, train loss3: 3.31679\n",
      "train loss1: 0.58562, train loss2: 0.40791, train loss3: 0.55293\n",
      "train loss1: 0.59650, train loss2: 0.03880, train loss3: 1.42692\n",
      "train loss1: 0.58762, train loss2: 0.38495, train loss3: 0.11826\n",
      "train loss1: 0.59786, train loss2: 0.04904, train loss3: 184.77429\n",
      "train loss1: 0.57389, train loss2: 1.94491, train loss3: 0.31698\n",
      "train loss1: 0.63347, train loss2: 0.03771, train loss3: 0.15493\n",
      "train loss1: 0.60319, train loss2: 0.04214, train loss3: 28.28057\n",
      "train loss1: 0.58045, train loss2: 2.23915, train loss3: 0.05939\n",
      "train loss1: 0.59261, train loss2: 0.22555, train loss3: 11.84042\n",
      "train loss1: 0.59375, train loss2: 0.04446, train loss3: 6.35429\n",
      "train loss1: 0.57388, train loss2: 0.39557, train loss3: 1.23834\n",
      "train loss1: 0.56915, train loss2: 0.67544, train loss3: 0.24309\n",
      "train loss1: 0.58265, train loss2: 0.08491, train loss3: 0.14946\n",
      "train loss1: 0.58375, train loss2: 1.94577, train loss3: 0.38773\n",
      "train loss1: 0.58281, train loss2: 0.75036, train loss3: 0.41985\n",
      "train loss1: 0.57276, train loss2: 0.02260, train loss3: 0.40284\n",
      "train loss1: 0.56922, train loss2: 0.25111, train loss3: 0.53391\n",
      "train loss1: 0.57267, train loss2: 0.88921, train loss3: 0.13414\n",
      "train loss1: 0.58612, train loss2: 0.65938, train loss3: 0.19855\n",
      "train loss1: 0.61640, train loss2: 0.06833, train loss3: 0.23007\n",
      "train loss1: 0.59460, train loss2: 0.02638, train loss3: 0.39163\n",
      "train loss1: 0.57283, train loss2: 0.08513, train loss3: 15.79218\n",
      "train loss1: 0.57837, train loss2: 0.03723, train loss3: 0.44503\n",
      "train loss1: 0.58267, train loss2: 0.13299, train loss3: 2.98953\n",
      "train loss1: 0.61054, train loss2: 0.06828, train loss3: 0.61137\n",
      "train loss1: 0.56181, train loss2: 0.06424, train loss3: 1.09383\n",
      "train loss1: 0.57863, train loss2: 0.33193, train loss3: 0.24533\n",
      "train loss1: 0.59090, train loss2: 0.37350, train loss3: 2.08149\n",
      "train loss1: 0.63819, train loss2: 0.03860, train loss3: 0.18248\n",
      "train loss1: 0.55770, train loss2: 0.05468, train loss3: 2.10465\n",
      "train loss1: 0.59949, train loss2: 0.05992, train loss3: 2.03215\n",
      "train loss1: 0.57841, train loss2: 0.65174, train loss3: 1.02700\n",
      "train loss1: 0.58383, train loss2: 0.13195, train loss3: 0.40591\n",
      "train loss1: 0.60122, train loss2: 0.10984, train loss3: 0.11227\n",
      "train loss1: 0.57126, train loss2: 0.01938, train loss3: 0.43523\n",
      "train loss1: 0.57629, train loss2: 0.02549, train loss3: 7.80803\n",
      "train loss1: 0.58020, train loss2: 0.08249, train loss3: 0.50736\n",
      "train loss1: 0.58522, train loss2: 1.89979, train loss3: 6.93906\n",
      "train loss1: 0.55531, train loss2: 0.05368, train loss3: 1.13332\n",
      "train loss1: 0.58542, train loss2: 0.08270, train loss3: 6.26992\n",
      "train loss1: 0.58214, train loss2: 0.10504, train loss3: 0.81538\n",
      "train loss1: 0.57043, train loss2: 0.96773, train loss3: 0.16150\n",
      "train loss1: 0.58110, train loss2: 0.08652, train loss3: 2.12024\n",
      "train loss1: 0.60077, train loss2: 0.09162, train loss3: 1.88273\n",
      "train loss1: 0.58702, train loss2: 0.22699, train loss3: 0.21380\n",
      "train loss1: 0.67781, train loss2: 0.32205, train loss3: 0.25313\n",
      "train loss1: 0.58874, train loss2: 0.04873, train loss3: 0.39316\n",
      "train loss1: 0.58034, train loss2: 0.05903, train loss3: 12.42496\n",
      "train loss1: 0.59584, train loss2: 0.12853, train loss3: 1.42714\n",
      "train loss1: 0.59916, train loss2: 0.23406, train loss3: 0.81806\n",
      "train loss1: 0.56993, train loss2: 2.24691, train loss3: 0.55278\n",
      "train loss1: 0.55702, train loss2: 1.03808, train loss3: 1.49730\n",
      "train loss1: 0.56848, train loss2: 0.02198, train loss3: 0.34034\n",
      "train loss1: 0.58674, train loss2: 0.03234, train loss3: 0.44525\n",
      "train loss1: 0.76195, train loss2: 0.04969, train loss3: 1.07156\n",
      "train loss1: 0.58061, train loss2: 0.05779, train loss3: 35.61820\n",
      "train loss1: 0.68767, train loss2: 0.05722, train loss3: 0.74640\n",
      "train loss1: 0.57876, train loss2: 0.05046, train loss3: 2.04256\n",
      "train loss1: 0.59849, train loss2: 3.49133, train loss3: 0.58550\n",
      "train loss1: 0.59596, train loss2: 0.05866, train loss3: 2.48964\n",
      "train loss1: 0.60828, train loss2: 0.19958, train loss3: 2.27244\n",
      "train loss1: 0.58404, train loss2: 0.17905, train loss3: 1.35930\n",
      "train loss1: 0.61036, train loss2: 4.03363, train loss3: 1.12574\n",
      "train loss1: 0.57686, train loss2: 0.34937, train loss3: 0.62237\n",
      "train loss1: 0.59194, train loss2: 48.34028, train loss3: 0.29735\n",
      "train loss1: 0.57795, train loss2: 0.06587, train loss3: 3.56800\n",
      "train loss1: 0.60099, train loss2: 0.04274, train loss3: 4.77658\n",
      "train loss1: 0.58333, train loss2: 0.01698, train loss3: 0.67892\n",
      "train loss1: 0.56581, train loss2: 0.02382, train loss3: 0.44081\n",
      "train loss1: 0.59039, train loss2: 0.21511, train loss3: 0.15799\n",
      "train loss1: 0.58025, train loss2: 0.08193, train loss3: 0.19941\n",
      "train loss1: 0.61869, train loss2: 0.03295, train loss3: 0.33009\n",
      "train loss1: 0.57131, train loss2: 0.04243, train loss3: 1.97530\n",
      "train loss1: 0.71550, train loss2: 0.03519, train loss3: 1.18562\n",
      "train loss1: 0.57555, train loss2: 0.43032, train loss3: 0.11558\n",
      "train loss1: 0.56361, train loss2: 0.03998, train loss3: 1.50672\n",
      "train loss1: 0.56400, train loss2: 0.04894, train loss3: 0.40987\n",
      "train loss1: 0.57521, train loss2: 0.04097, train loss3: 0.23137\n",
      "train loss1: 0.56858, train loss2: 1.39643, train loss3: 1.50375\n",
      "train loss1: 0.69333, train loss2: 2.23264, train loss3: 0.61024\n",
      "train loss1: 0.57618, train loss2: 0.04818, train loss3: 1.86165\n",
      "train loss1: 0.57549, train loss2: 1.64902, train loss3: 1.43013\n",
      "train loss1: 0.58865, train loss2: 0.23581, train loss3: 0.58554\n",
      "train loss1: 0.56884, train loss2: 0.05055, train loss3: 2.04085\n",
      "train loss1: 0.59170, train loss2: 0.24172, train loss3: 0.24295\n",
      "train loss1: 0.60187, train loss2: 0.07794, train loss3: 1.70938\n",
      "train loss1: 0.58761, train loss2: 0.05490, train loss3: 0.65896\n",
      "train loss1: 0.58929, train loss2: 0.10695, train loss3: 2.36360\n",
      "train loss1: 0.58743, train loss2: 0.11481, train loss3: 0.71700\n",
      "train loss1: 0.57875, train loss2: 0.26486, train loss3: 4.50931\n",
      "train loss1: 0.57868, train loss2: 0.02900, train loss3: 0.67434\n",
      "train loss1: 0.58223, train loss2: 0.09195, train loss3: 0.13519\n",
      "train loss1: 0.55790, train loss2: 0.57190, train loss3: 1.04995\n",
      "train loss1: 0.57166, train loss2: 4.56116, train loss3: 0.46461\n",
      "train loss1: 0.68548, train loss2: 0.27447, train loss3: 0.52348\n",
      "train loss1: 0.56972, train loss2: 0.78427, train loss3: 0.49814\n",
      "train loss1: 0.57256, train loss2: 0.07922, train loss3: 3.29866\n",
      "train loss1: 0.59021, train loss2: 0.04775, train loss3: 1.60728\n",
      "train loss1: 0.60007, train loss2: 0.13004, train loss3: 1.78557\n",
      "train loss1: 0.64361, train loss2: 0.08184, train loss3: 0.20450\n",
      "train loss1: 0.56578, train loss2: 0.05838, train loss3: 15.55635\n",
      "train loss1: 0.57270, train loss2: 0.03645, train loss3: 0.83590\n",
      "train loss1: 0.58322, train loss2: 0.03811, train loss3: 0.62119\n",
      "train loss1: 0.55959, train loss2: 0.02862, train loss3: 20.59559\n",
      "train loss1: 0.59368, train loss2: 0.04783, train loss3: 27.09974\n",
      "train loss1: 0.56705, train loss2: 0.27598, train loss3: 0.85971\n",
      "train loss1: 0.56132, train loss2: 0.27987, train loss3: 5.10106\n",
      "train loss1: 0.54741, train loss2: 0.00093, train loss3: 1.62045\n",
      "Epoch: 7/10\n",
      "train loss1: 0.55964, train loss2: 0.03752, train loss3: 1.27123\n",
      "train loss1: 0.58757, train loss2: 0.90679, train loss3: 0.35015\n",
      "train loss1: 0.59140, train loss2: 0.05695, train loss3: 0.68655\n",
      "train loss1: 0.59465, train loss2: 0.04424, train loss3: 2.75436\n",
      "train loss1: 0.57569, train loss2: 0.07484, train loss3: 0.81110\n",
      "train loss1: 0.55799, train loss2: 0.21379, train loss3: 2.72206\n",
      "train loss1: 0.58079, train loss2: 0.64730, train loss3: 0.75821\n",
      "train loss1: 0.57032, train loss2: 2.80924, train loss3: 1.59442\n",
      "train loss1: 0.57693, train loss2: 0.05584, train loss3: 0.11516\n",
      "train loss1: 0.56087, train loss2: 0.62719, train loss3: 1.62317\n",
      "train loss1: 0.56505, train loss2: 45.46185, train loss3: 0.16010\n",
      "train loss1: 0.57078, train loss2: 0.92684, train loss3: 1.29835\n",
      "train loss1: 0.56017, train loss2: 0.94123, train loss3: 0.13794\n",
      "train loss1: 0.58512, train loss2: 0.12042, train loss3: 9.46426\n",
      "train loss1: 0.61647, train loss2: 0.34478, train loss3: 0.36805\n",
      "train loss1: 0.57562, train loss2: 0.07256, train loss3: 1.01737\n",
      "train loss1: 0.58532, train loss2: 0.36066, train loss3: 2.63664\n",
      "train loss1: 0.56022, train loss2: 0.02116, train loss3: 0.76352\n",
      "train loss1: 0.56473, train loss2: 0.20281, train loss3: 0.51322\n",
      "train loss1: 0.59910, train loss2: 0.38644, train loss3: 1.27847\n",
      "train loss1: 0.56293, train loss2: 0.71588, train loss3: 0.43567\n",
      "train loss1: 0.59302, train loss2: 0.06011, train loss3: 25.49343\n",
      "train loss1: 0.58721, train loss2: 0.08826, train loss3: 15.69610\n",
      "train loss1: 0.58534, train loss2: 2.63535, train loss3: 0.38796\n",
      "train loss1: 0.56950, train loss2: 0.06824, train loss3: 4.47103\n",
      "train loss1: 0.59537, train loss2: 0.03319, train loss3: 0.64537\n",
      "train loss1: 0.57408, train loss2: 0.87382, train loss3: 0.88094\n",
      "train loss1: 0.58035, train loss2: 0.02967, train loss3: 0.71012\n",
      "train loss1: 0.57767, train loss2: 0.10732, train loss3: 1.12852\n",
      "train loss1: 0.58318, train loss2: 0.04102, train loss3: 0.33350\n",
      "train loss1: 0.57010, train loss2: 0.06144, train loss3: 0.25173\n",
      "train loss1: 0.57317, train loss2: 0.19175, train loss3: 0.25931\n",
      "train loss1: 0.57367, train loss2: 0.96698, train loss3: 0.70162\n",
      "train loss1: 0.58942, train loss2: 0.05948, train loss3: 0.27205\n",
      "train loss1: 0.58671, train loss2: 0.01040, train loss3: 1.98705\n",
      "train loss1: 0.57038, train loss2: 0.25860, train loss3: 0.29483\n",
      "train loss1: 0.59190, train loss2: 0.05274, train loss3: 0.53466\n",
      "train loss1: 0.64809, train loss2: 0.72379, train loss3: 1.23474\n",
      "train loss1: 0.54544, train loss2: 0.03304, train loss3: 0.49598\n",
      "train loss1: 0.58234, train loss2: 4.14606, train loss3: 1.41847\n",
      "train loss1: 0.59304, train loss2: 8.62469, train loss3: 0.20575\n",
      "train loss1: 0.58657, train loss2: 0.04819, train loss3: 1.11558\n",
      "train loss1: 0.58587, train loss2: 1.13373, train loss3: 0.20038\n",
      "train loss1: 0.55991, train loss2: 0.06248, train loss3: 0.25932\n",
      "train loss1: 0.57731, train loss2: 0.02331, train loss3: 2.13442\n",
      "train loss1: 0.57837, train loss2: 0.19202, train loss3: 0.18789\n",
      "train loss1: 0.57043, train loss2: 0.06875, train loss3: 0.29685\n",
      "train loss1: 0.55566, train loss2: 0.05188, train loss3: 5.04898\n",
      "train loss1: 0.56383, train loss2: 0.06976, train loss3: 0.58386\n",
      "train loss1: 0.58078, train loss2: 3.54119, train loss3: 4.56769\n",
      "train loss1: 0.58746, train loss2: 0.07517, train loss3: 0.22703\n",
      "train loss1: 0.60922, train loss2: 0.16213, train loss3: 0.20424\n",
      "train loss1: 0.56552, train loss2: 0.36230, train loss3: 0.10797\n",
      "train loss1: 0.56735, train loss2: 0.45092, train loss3: 1.22067\n",
      "train loss1: 0.56922, train loss2: 0.03908, train loss3: 0.11150\n",
      "train loss1: 0.56338, train loss2: 0.61178, train loss3: 0.37289\n",
      "train loss1: 0.58861, train loss2: 0.09558, train loss3: 0.45918\n",
      "train loss1: 0.57900, train loss2: 0.04374, train loss3: 0.65302\n",
      "train loss1: 0.57159, train loss2: 0.04136, train loss3: 0.79724\n",
      "train loss1: 0.59659, train loss2: 0.37731, train loss3: 1.15675\n",
      "train loss1: 0.56539, train loss2: 0.02896, train loss3: 1.46635\n",
      "train loss1: 0.57184, train loss2: 0.19058, train loss3: 0.45173\n",
      "train loss1: 0.56897, train loss2: 0.11672, train loss3: 1.57401\n",
      "train loss1: 0.57695, train loss2: 0.06056, train loss3: 1.52175\n",
      "train loss1: 0.55028, train loss2: 0.49183, train loss3: 0.39220\n",
      "train loss1: 0.56784, train loss2: 0.09383, train loss3: 16.28297\n",
      "train loss1: 0.56113, train loss2: 0.05459, train loss3: 1.32337\n",
      "train loss1: 0.59618, train loss2: 0.01810, train loss3: 0.26978\n",
      "train loss1: 0.56893, train loss2: 0.29363, train loss3: 0.85628\n",
      "train loss1: 0.56637, train loss2: 0.07651, train loss3: 0.25651\n",
      "train loss1: 0.55843, train loss2: 0.09300, train loss3: 0.56923\n",
      "train loss1: 0.59720, train loss2: 0.21302, train loss3: 0.83918\n",
      "train loss1: 0.56873, train loss2: 0.46188, train loss3: 4.88257\n",
      "train loss1: 0.58525, train loss2: 0.06000, train loss3: 0.16620\n",
      "train loss1: 0.62136, train loss2: 0.04781, train loss3: 0.20953\n",
      "train loss1: 0.57484, train loss2: 0.02139, train loss3: 9.69088\n",
      "train loss1: 0.59762, train loss2: 0.17511, train loss3: 20.59666\n",
      "train loss1: 0.59131, train loss2: 0.72084, train loss3: 2.36605\n",
      "train loss1: 0.65118, train loss2: 0.27460, train loss3: 0.30704\n",
      "train loss1: 0.57504, train loss2: 0.41415, train loss3: 0.26348\n",
      "train loss1: 0.55603, train loss2: 0.07067, train loss3: 4.90151\n",
      "train loss1: 0.57242, train loss2: 0.62551, train loss3: 1.03217\n",
      "train loss1: 0.59557, train loss2: 2.60427, train loss3: 0.37592\n",
      "train loss1: 0.57726, train loss2: 3.80674, train loss3: 0.50161\n",
      "train loss1: 0.56022, train loss2: 0.13262, train loss3: 0.23421\n",
      "train loss1: 0.57553, train loss2: 0.28945, train loss3: 139.26056\n",
      "train loss1: 0.56141, train loss2: 0.04911, train loss3: 0.63767\n",
      "train loss1: 0.57655, train loss2: 0.38017, train loss3: 0.74796\n",
      "train loss1: 0.59602, train loss2: 4.32398, train loss3: 0.27085\n",
      "train loss1: 0.57180, train loss2: 0.06612, train loss3: 0.14157\n",
      "train loss1: 0.56843, train loss2: 0.06827, train loss3: 2.38057\n",
      "train loss1: 0.57262, train loss2: 0.06478, train loss3: 1.73499\n",
      "train loss1: 0.56056, train loss2: 1.69658, train loss3: 0.59228\n",
      "train loss1: 0.58440, train loss2: 147.20139, train loss3: 1.98381\n",
      "train loss1: 0.57454, train loss2: 0.06213, train loss3: 0.67959\n",
      "train loss1: 0.56773, train loss2: 0.19116, train loss3: 27.77907\n",
      "train loss1: 0.56015, train loss2: 0.05127, train loss3: 0.69515\n",
      "train loss1: 0.58668, train loss2: 0.05177, train loss3: 0.88994\n",
      "train loss1: 0.57386, train loss2: 1.00960, train loss3: 0.28434\n",
      "train loss1: 0.58648, train loss2: 0.01800, train loss3: 0.65877\n",
      "train loss1: 0.58624, train loss2: 0.03559, train loss3: 1.28861\n",
      "train loss1: 0.57255, train loss2: 0.05029, train loss3: 0.61601\n",
      "train loss1: 0.57891, train loss2: 2.24464, train loss3: 0.73675\n",
      "train loss1: 0.58584, train loss2: 2.59840, train loss3: 0.72558\n",
      "train loss1: 0.57583, train loss2: 0.14273, train loss3: 1.32969\n",
      "train loss1: 0.58269, train loss2: 0.05209, train loss3: 5.46652\n",
      "train loss1: 0.54611, train loss2: 0.24068, train loss3: 0.34372\n",
      "train loss1: 0.56703, train loss2: 0.03151, train loss3: 0.83537\n",
      "train loss1: 0.57625, train loss2: 0.07184, train loss3: 0.57318\n",
      "train loss1: 0.56134, train loss2: 0.23734, train loss3: 3.20302\n",
      "train loss1: 0.59102, train loss2: 0.10714, train loss3: 0.27390\n",
      "train loss1: 0.57797, train loss2: 111.85073, train loss3: 3.38483\n",
      "train loss1: 0.56999, train loss2: 0.02383, train loss3: 1.92427\n",
      "train loss1: 0.55732, train loss2: 0.34373, train loss3: 0.40108\n",
      "train loss1: 0.57588, train loss2: 0.28058, train loss3: 2.21552\n",
      "train loss1: 0.55816, train loss2: 0.22611, train loss3: 0.98854\n",
      "train loss1: 0.59604, train loss2: 1.37396, train loss3: 2.27762\n",
      "train loss1: 0.58569, train loss2: 0.03646, train loss3: 8.96745\n",
      "train loss1: 0.65438, train loss2: 0.58890, train loss3: 0.40214\n",
      "train loss1: 0.58872, train loss2: 0.35783, train loss3: 2.52889\n",
      "train loss1: 0.58571, train loss2: 0.29799, train loss3: 2.96462\n",
      "train loss1: 0.55063, train loss2: 0.79351, train loss3: 0.39885\n",
      "train loss1: 0.58678, train loss2: 4.28794, train loss3: 0.28219\n",
      "train loss1: 0.59997, train loss2: 0.37778, train loss3: 4.68532\n",
      "train loss1: 0.57620, train loss2: 0.04586, train loss3: 0.34035\n",
      "train loss1: 0.58207, train loss2: 0.14547, train loss3: 1.11346\n",
      "train loss1: 0.56562, train loss2: 2.21296, train loss3: 10.42925\n",
      "train loss1: 0.57406, train loss2: 0.03690, train loss3: 0.35886\n",
      "train loss1: 0.58160, train loss2: 1.04741, train loss3: 0.64665\n",
      "train loss1: 0.64594, train loss2: 0.17481, train loss3: 0.94771\n",
      "train loss1: 0.58516, train loss2: 0.06119, train loss3: 19.42357\n",
      "train loss1: 0.56328, train loss2: 0.08689, train loss3: 0.12766\n",
      "train loss1: 0.56737, train loss2: 0.10529, train loss3: 1.04281\n",
      "train loss1: 0.56076, train loss2: 0.04223, train loss3: 0.94039\n",
      "train loss1: 0.58794, train loss2: 0.08669, train loss3: 2.27865\n",
      "train loss1: 0.57900, train loss2: 0.18352, train loss3: 15.60127\n",
      "train loss1: 0.56387, train loss2: 0.10133, train loss3: 1.94845\n",
      "train loss1: 0.58928, train loss2: 48.51752, train loss3: 0.69829\n",
      "train loss1: 0.63417, train loss2: 0.03017, train loss3: 2.21134\n",
      "train loss1: 0.56425, train loss2: 0.05755, train loss3: 2.62555\n",
      "train loss1: 0.57700, train loss2: 0.09357, train loss3: 0.44271\n",
      "train loss1: 0.57328, train loss2: 0.07365, train loss3: 19.93875\n",
      "train loss1: 0.57461, train loss2: 0.18247, train loss3: 12.70397\n",
      "train loss1: 0.61498, train loss2: 0.04805, train loss3: 0.08797\n",
      "train loss1: 0.58036, train loss2: 0.06923, train loss3: 0.50841\n",
      "train loss1: 0.59284, train loss2: 0.11943, train loss3: 0.80900\n",
      "train loss1: 0.58047, train loss2: 0.13521, train loss3: 0.53845\n",
      "train loss1: 0.57263, train loss2: 0.32029, train loss3: 1.10855\n",
      "train loss1: 0.57355, train loss2: 0.14847, train loss3: 19.00122\n",
      "train loss1: 0.57135, train loss2: 0.04800, train loss3: 9.72944\n",
      "train loss1: 0.60781, train loss2: 0.04339, train loss3: 1.92347\n",
      "train loss1: 0.60341, train loss2: 0.05348, train loss3: 2.54577\n",
      "train loss1: 0.57803, train loss2: 0.12404, train loss3: 0.97685\n",
      "train loss1: 0.57505, train loss2: 0.04530, train loss3: 0.28416\n",
      "train loss1: 0.58972, train loss2: 0.04764, train loss3: 29.12428\n",
      "train loss1: 0.62884, train loss2: 0.50783, train loss3: 0.15823\n",
      "train loss1: 0.58371, train loss2: 0.04337, train loss3: 0.62565\n",
      "train loss1: 0.56986, train loss2: 0.07856, train loss3: 0.25650\n",
      "train loss1: 0.56847, train loss2: 0.12254, train loss3: 3.02907\n",
      "train loss1: 0.57296, train loss2: 0.02502, train loss3: 0.27127\n",
      "train loss1: 0.60285, train loss2: 0.03126, train loss3: 2.63457\n",
      "train loss1: 0.55560, train loss2: 0.05765, train loss3: 0.79219\n",
      "train loss1: 0.54720, train loss2: 0.13218, train loss3: 0.61877\n",
      "train loss1: 0.56009, train loss2: 1.62754, train loss3: 1.27065\n",
      "train loss1: 0.57743, train loss2: 0.13585, train loss3: 0.40661\n",
      "train loss1: 0.57272, train loss2: 0.02668, train loss3: 0.62531\n",
      "train loss1: 0.56891, train loss2: 0.05218, train loss3: 10.70627\n",
      "train loss1: 0.57701, train loss2: 46.94130, train loss3: 0.32319\n",
      "train loss1: 0.60464, train loss2: 0.20753, train loss3: 0.22527\n",
      "train loss1: 0.59806, train loss2: 0.02806, train loss3: 0.40358\n",
      "train loss1: 0.56903, train loss2: 0.11468, train loss3: 0.21107\n",
      "train loss1: 0.60103, train loss2: 0.78365, train loss3: 0.39017\n",
      "train loss1: 0.55181, train loss2: 0.91176, train loss3: 0.85614\n",
      "train loss1: 0.56771, train loss2: 0.22008, train loss3: 0.35857\n",
      "train loss1: 0.57181, train loss2: 0.02606, train loss3: 1.28668\n",
      "train loss1: 0.54065, train loss2: 0.03025, train loss3: 0.88970\n",
      "train loss1: 0.56135, train loss2: 0.04869, train loss3: 0.56196\n",
      "train loss1: 0.54759, train loss2: 0.06757, train loss3: 0.42294\n",
      "train loss1: 0.56803, train loss2: 0.39317, train loss3: 1.24318\n",
      "train loss1: 0.73483, train loss2: 0.07241, train loss3: 238.43289\n",
      "train loss1: 0.57331, train loss2: 0.10470, train loss3: 0.27398\n",
      "train loss1: 0.59106, train loss2: 0.03392, train loss3: 0.35062\n",
      "train loss1: 0.57103, train loss2: 0.31171, train loss3: 0.81415\n",
      "train loss1: 0.58004, train loss2: 0.07001, train loss3: 2.02742\n",
      "train loss1: 0.57197, train loss2: 0.04070, train loss3: 2.69446\n",
      "train loss1: 0.57170, train loss2: 0.09417, train loss3: 0.14703\n",
      "train loss1: 0.56463, train loss2: 0.02201, train loss3: 0.77422\n",
      "train loss1: 0.55118, train loss2: 0.03507, train loss3: 0.61233\n",
      "train loss1: 0.55806, train loss2: 0.05431, train loss3: 0.15005\n",
      "train loss1: 0.57785, train loss2: 0.03293, train loss3: 7.68854\n",
      "train loss1: 0.54820, train loss2: 0.04655, train loss3: 0.67792\n",
      "train loss1: 0.56549, train loss2: 0.10269, train loss3: 1.00016\n",
      "train loss1: 0.56741, train loss2: 0.03069, train loss3: 0.36162\n",
      "train loss1: 0.66956, train loss2: 0.04452, train loss3: 8.12202\n",
      "train loss1: 0.56740, train loss2: 0.11399, train loss3: 31.37027\n",
      "train loss1: 0.58857, train loss2: 0.08147, train loss3: 0.80363\n",
      "train loss1: 0.57682, train loss2: 0.04928, train loss3: 0.33457\n",
      "train loss1: 0.55485, train loss2: 0.66581, train loss3: 0.50068\n",
      "train loss1: 0.66403, train loss2: 0.11026, train loss3: 1.49067\n",
      "train loss1: 0.58204, train loss2: 0.03801, train loss3: 0.69072\n",
      "train loss1: 0.57717, train loss2: 0.09538, train loss3: 0.66100\n",
      "train loss1: 0.57827, train loss2: 0.15415, train loss3: 5.49596\n",
      "train loss1: 0.58376, train loss2: 0.09425, train loss3: 0.35914\n",
      "train loss1: 0.56734, train loss2: 0.08675, train loss3: 0.29460\n",
      "train loss1: 0.57968, train loss2: 0.30301, train loss3: 184.34985\n",
      "train loss1: 0.59279, train loss2: 0.14235, train loss3: 0.24020\n",
      "train loss1: 0.58169, train loss2: 0.08473, train loss3: 0.44083\n",
      "train loss1: 0.56072, train loss2: 0.04988, train loss3: 1.12015\n",
      "train loss1: 0.57698, train loss2: 0.09543, train loss3: 0.06731\n",
      "train loss1: 0.55735, train loss2: 3.53725, train loss3: 0.66338\n",
      "train loss1: 0.55956, train loss2: 0.08567, train loss3: 0.75619\n",
      "train loss1: 0.57365, train loss2: 0.16533, train loss3: 0.39197\n",
      "train loss1: 0.55517, train loss2: 0.23254, train loss3: 2.20968\n",
      "train loss1: 0.61105, train loss2: 0.04657, train loss3: 0.49975\n",
      "train loss1: 0.56630, train loss2: 2.06349, train loss3: 0.75218\n",
      "train loss1: 0.58038, train loss2: 1.09083, train loss3: 0.66712\n",
      "train loss1: 0.64812, train loss2: 14.83749, train loss3: 0.40402\n",
      "train loss1: 0.55893, train loss2: 0.14764, train loss3: 5.05790\n",
      "train loss1: 0.57185, train loss2: 0.18068, train loss3: 0.55972\n",
      "train loss1: 0.57211, train loss2: 0.08212, train loss3: 0.36077\n",
      "train loss1: 0.67263, train loss2: 0.03694, train loss3: 0.25461\n",
      "train loss1: 0.57286, train loss2: 0.90763, train loss3: 0.33551\n",
      "train loss1: 0.55451, train loss2: 0.04467, train loss3: 0.36417\n",
      "train loss1: 0.57555, train loss2: 0.10806, train loss3: 2.26084\n",
      "train loss1: 0.56013, train loss2: 0.11515, train loss3: 0.77948\n",
      "train loss1: 0.58397, train loss2: 0.06164, train loss3: 55.30572\n",
      "train loss1: 0.58105, train loss2: 0.37807, train loss3: 0.15595\n",
      "train loss1: 0.57729, train loss2: 0.11997, train loss3: 12.67797\n",
      "train loss1: 0.55556, train loss2: 0.31249, train loss3: 3.42319\n",
      "train loss1: 0.56325, train loss2: 0.06620, train loss3: 5.96350\n",
      "train loss1: 0.57562, train loss2: 0.07970, train loss3: 0.10114\n",
      "train loss1: 0.55975, train loss2: 1.88933, train loss3: 1.93537\n",
      "train loss1: 0.56081, train loss2: 0.04275, train loss3: 0.84004\n",
      "train loss1: 0.58486, train loss2: 192.06206, train loss3: 0.28519\n",
      "train loss1: 0.56814, train loss2: 0.03977, train loss3: 1.00036\n",
      "train loss1: 0.55742, train loss2: 0.06818, train loss3: 0.68400\n",
      "train loss1: 0.56152, train loss2: 0.08941, train loss3: 0.64765\n",
      "train loss1: 0.59988, train loss2: 0.05551, train loss3: 5.63001\n",
      "train loss1: 0.57243, train loss2: 0.34366, train loss3: 0.74188\n",
      "train loss1: 0.55229, train loss2: 0.11290, train loss3: 0.59241\n",
      "train loss1: 0.57650, train loss2: 0.04020, train loss3: 7.09803\n",
      "train loss1: 0.56360, train loss2: 0.08077, train loss3: 0.25538\n",
      "train loss1: 0.57090, train loss2: 0.35763, train loss3: 0.12074\n",
      "train loss1: 0.55123, train loss2: 0.16869, train loss3: 0.82560\n",
      "train loss1: 0.56915, train loss2: 0.23975, train loss3: 0.21544\n",
      "train loss1: 0.56788, train loss2: 0.05158, train loss3: 1.79442\n",
      "train loss1: 0.57969, train loss2: 0.04240, train loss3: 0.26204\n",
      "train loss1: 0.56141, train loss2: 0.12272, train loss3: 2.31094\n",
      "train loss1: 0.59699, train loss2: 0.33364, train loss3: 0.52815\n",
      "train loss1: 0.56270, train loss2: 0.08598, train loss3: 1.18528\n",
      "train loss1: 0.55713, train loss2: 0.09283, train loss3: 1.93937\n",
      "train loss1: 0.58234, train loss2: 0.05007, train loss3: 0.11842\n",
      "train loss1: 0.55906, train loss2: 0.08388, train loss3: 0.19034\n",
      "train loss1: 0.57238, train loss2: 0.15298, train loss3: 0.23148\n",
      "train loss1: 0.56365, train loss2: 0.74852, train loss3: 0.34664\n",
      "train loss1: 0.54270, train loss2: 0.12487, train loss3: 0.50965\n",
      "train loss1: 0.55905, train loss2: 0.17294, train loss3: 0.08267\n",
      "train loss1: 0.66080, train loss2: 0.14853, train loss3: 0.21956\n",
      "train loss1: 0.56662, train loss2: 0.04363, train loss3: 0.85838\n",
      "train loss1: 0.59059, train loss2: 0.15825, train loss3: 0.68227\n",
      "train loss1: 0.60452, train loss2: 0.15646, train loss3: 12.42709\n",
      "train loss1: 0.58196, train loss2: 0.04936, train loss3: 0.53524\n",
      "train loss1: 0.57544, train loss2: 0.05766, train loss3: 1.24418\n",
      "train loss1: 0.55420, train loss2: 0.11755, train loss3: 0.52613\n",
      "train loss1: 0.58051, train loss2: 0.75663, train loss3: 2.56787\n",
      "train loss1: 0.56522, train loss2: 0.05805, train loss3: 2.39784\n",
      "train loss1: 0.58464, train loss2: 0.14251, train loss3: 11.93071\n",
      "train loss1: 0.55322, train loss2: 0.02384, train loss3: 0.77240\n",
      "train loss1: 0.56902, train loss2: 1.94119, train loss3: 0.65870\n",
      "train loss1: 0.57379, train loss2: 0.12202, train loss3: 0.90095\n",
      "train loss1: 0.56994, train loss2: 0.05028, train loss3: 0.11894\n",
      "train loss1: 0.56305, train loss2: 0.03531, train loss3: 1.08237\n",
      "train loss1: 0.74546, train loss2: 0.07183, train loss3: 0.98942\n",
      "train loss1: 0.58765, train loss2: 0.76335, train loss3: 2.57200\n",
      "train loss1: 0.56147, train loss2: 0.06428, train loss3: 0.21712\n",
      "train loss1: 0.57186, train loss2: 1.22994, train loss3: 0.74431\n",
      "train loss1: 0.57033, train loss2: 0.07362, train loss3: 5.15251\n",
      "train loss1: 0.60524, train loss2: 1.98750, train loss3: 1.18859\n",
      "train loss1: 0.60002, train loss2: 1.18615, train loss3: 8.80322\n",
      "train loss1: 0.55763, train loss2: 0.19644, train loss3: 0.10324\n",
      "train loss1: 0.57044, train loss2: 0.53639, train loss3: 0.66124\n",
      "train loss1: 0.57259, train loss2: 0.33687, train loss3: 0.35236\n",
      "train loss1: 0.55501, train loss2: 11.27140, train loss3: 3.35517\n",
      "train loss1: 0.57354, train loss2: 2.12655, train loss3: 3.13466\n",
      "train loss1: 0.57465, train loss2: 0.07000, train loss3: 7.70439\n",
      "train loss1: 0.57215, train loss2: 8.84494, train loss3: 0.51366\n",
      "train loss1: 0.54322, train loss2: 0.13598, train loss3: 0.59270\n",
      "train loss1: 0.55592, train loss2: 0.30155, train loss3: 1.18662\n",
      "train loss1: 0.56557, train loss2: 0.03973, train loss3: 0.17250\n",
      "train loss1: 0.55347, train loss2: 0.04687, train loss3: 0.35935\n",
      "train loss1: 0.55637, train loss2: 0.22468, train loss3: 5.75257\n",
      "train loss1: 0.60108, train loss2: 0.10276, train loss3: 1.49599\n",
      "train loss1: 0.56197, train loss2: 0.39183, train loss3: 0.21669\n",
      "train loss1: 0.56621, train loss2: 0.03750, train loss3: 0.30379\n",
      "train loss1: 0.57497, train loss2: 0.06066, train loss3: 0.40304\n",
      "train loss1: 0.56862, train loss2: 2.28559, train loss3: 0.67085\n",
      "train loss1: 0.53021, train loss2: 0.01489, train loss3: 0.55555\n",
      "Epoch: 8/10\n",
      "train loss1: 0.57209, train loss2: 0.17985, train loss3: 0.51052\n",
      "train loss1: 0.56863, train loss2: 0.60346, train loss3: 0.32021\n",
      "train loss1: 0.56453, train loss2: 0.06904, train loss3: 0.51735\n",
      "train loss1: 0.66418, train loss2: 0.20690, train loss3: 1.14496\n",
      "train loss1: 0.58651, train loss2: 0.22826, train loss3: 1.18468\n",
      "train loss1: 0.58140, train loss2: 3.47118, train loss3: 2.55815\n",
      "train loss1: 0.64916, train loss2: 0.15700, train loss3: 0.88648\n",
      "train loss1: 0.56395, train loss2: 0.66516, train loss3: 1.28345\n",
      "train loss1: 0.55782, train loss2: 0.33250, train loss3: 12.36477\n",
      "train loss1: 0.58988, train loss2: 0.02323, train loss3: 0.33970\n",
      "train loss1: 0.56793, train loss2: 2.85425, train loss3: 0.32525\n",
      "train loss1: 0.57245, train loss2: 149.68541, train loss3: 0.21415\n",
      "train loss1: 0.56276, train loss2: 0.08871, train loss3: 0.07150\n",
      "train loss1: 0.55709, train loss2: 0.12246, train loss3: 0.28763\n",
      "train loss1: 0.55697, train loss2: 0.30968, train loss3: 2.30039\n",
      "train loss1: 0.55941, train loss2: 0.20302, train loss3: 1.92497\n",
      "train loss1: 0.55164, train loss2: 0.03339, train loss3: 0.53661\n",
      "train loss1: 0.56692, train loss2: 1.93773, train loss3: 5.25784\n",
      "train loss1: 0.57965, train loss2: 0.17773, train loss3: 0.30399\n",
      "train loss1: 0.56510, train loss2: 0.11623, train loss3: 0.95306\n",
      "train loss1: 0.58399, train loss2: 0.03615, train loss3: 1.07839\n",
      "train loss1: 0.59762, train loss2: 0.05968, train loss3: 3.33126\n",
      "train loss1: 0.58476, train loss2: 0.76700, train loss3: 0.68959\n",
      "train loss1: 0.54540, train loss2: 0.07443, train loss3: 0.22238\n",
      "train loss1: 0.58026, train loss2: 0.13964, train loss3: 0.14684\n",
      "train loss1: 0.56888, train loss2: 0.06292, train loss3: 0.10301\n",
      "train loss1: 0.55739, train loss2: 0.03093, train loss3: 1.37915\n",
      "train loss1: 0.57201, train loss2: 0.93935, train loss3: 23.68277\n",
      "train loss1: 0.56653, train loss2: 0.09685, train loss3: 0.09789\n",
      "train loss1: 0.56257, train loss2: 0.10728, train loss3: 0.30231\n",
      "train loss1: 0.53027, train loss2: 0.06229, train loss3: 0.81322\n",
      "train loss1: 0.58971, train loss2: 0.15222, train loss3: 0.32984\n",
      "train loss1: 0.66423, train loss2: 0.08076, train loss3: 0.39217\n",
      "train loss1: 0.55805, train loss2: 1.46135, train loss3: 0.54612\n",
      "train loss1: 0.57139, train loss2: 0.13835, train loss3: 0.73496\n",
      "train loss1: 0.56269, train loss2: 0.10063, train loss3: 0.82824\n",
      "train loss1: 0.61673, train loss2: 0.03263, train loss3: 0.50853\n",
      "train loss1: 0.56731, train loss2: 0.02870, train loss3: 1.47999\n",
      "train loss1: 0.55634, train loss2: 0.03650, train loss3: 1.08088\n",
      "train loss1: 0.55043, train loss2: 0.05733, train loss3: 0.23702\n",
      "train loss1: 0.55038, train loss2: 0.03470, train loss3: 3.41522\n",
      "train loss1: 0.54057, train loss2: 0.04831, train loss3: 0.20221\n",
      "train loss1: 0.53760, train loss2: 0.10543, train loss3: 1.08872\n",
      "train loss1: 0.56181, train loss2: 0.05044, train loss3: 3.41100\n",
      "train loss1: 0.57315, train loss2: 0.04984, train loss3: 3.16875\n",
      "train loss1: 0.55812, train loss2: 0.13651, train loss3: 0.25087\n",
      "train loss1: 0.55746, train loss2: 0.08106, train loss3: 1.87530\n",
      "train loss1: 0.55641, train loss2: 0.01613, train loss3: 0.98176\n",
      "train loss1: 0.57794, train loss2: 0.10348, train loss3: 0.92778\n",
      "train loss1: 0.56857, train loss2: 0.04892, train loss3: 1.40621\n",
      "train loss1: 0.55917, train loss2: 0.34751, train loss3: 1.19573\n",
      "train loss1: 0.56332, train loss2: 0.13264, train loss3: 0.54544\n",
      "train loss1: 0.55594, train loss2: 0.03533, train loss3: 6.86062\n",
      "train loss1: 0.55173, train loss2: 0.06413, train loss3: 0.36083\n",
      "train loss1: 0.57966, train loss2: 1.90690, train loss3: 2.22137\n",
      "train loss1: 0.58087, train loss2: 0.19440, train loss3: 0.63229\n",
      "train loss1: 0.60634, train loss2: 0.04606, train loss3: 0.46750\n",
      "train loss1: 0.56034, train loss2: 0.13001, train loss3: 0.34540\n",
      "train loss1: 0.56771, train loss2: 0.13369, train loss3: 1.68103\n",
      "train loss1: 0.58073, train loss2: 0.04940, train loss3: 1.51586\n",
      "train loss1: 0.56317, train loss2: 2.60683, train loss3: 0.31587\n",
      "train loss1: 0.55088, train loss2: 0.04378, train loss3: 20.08307\n",
      "train loss1: 0.67015, train loss2: 112.31903, train loss3: 0.84700\n",
      "train loss1: 0.55710, train loss2: 0.10536, train loss3: 1.86221\n",
      "train loss1: 0.55310, train loss2: 0.13634, train loss3: 0.34575\n",
      "train loss1: 0.56817, train loss2: 48.58329, train loss3: 0.97554\n",
      "train loss1: 0.58833, train loss2: 0.05225, train loss3: 0.29341\n",
      "train loss1: 0.58704, train loss2: 0.26185, train loss3: 0.47886\n",
      "train loss1: 0.57311, train loss2: 0.04318, train loss3: 0.15293\n",
      "train loss1: 0.54452, train loss2: 4.33877, train loss3: 0.93860\n",
      "train loss1: 0.54495, train loss2: 0.01608, train loss3: 0.83561\n",
      "train loss1: 0.55797, train loss2: 0.03109, train loss3: 1.21697\n",
      "train loss1: 0.55955, train loss2: 0.63794, train loss3: 2.12108\n",
      "train loss1: 0.57390, train loss2: 1.03002, train loss3: 4.50092\n",
      "train loss1: 0.57515, train loss2: 4.66260, train loss3: 0.24505\n",
      "train loss1: 0.55527, train loss2: 0.03497, train loss3: 4.25939\n",
      "train loss1: 0.55314, train loss2: 0.27783, train loss3: 0.35599\n",
      "train loss1: 0.58229, train loss2: 0.12981, train loss3: 0.60230\n",
      "train loss1: 0.57565, train loss2: 0.87462, train loss3: 0.40748\n",
      "train loss1: 0.55989, train loss2: 0.34283, train loss3: 0.18093\n",
      "train loss1: 0.56321, train loss2: 0.10787, train loss3: 1.60933\n",
      "train loss1: 0.54169, train loss2: 0.62821, train loss3: 0.42258\n",
      "train loss1: 0.56749, train loss2: 0.05329, train loss3: 56.68127\n",
      "train loss1: 0.55595, train loss2: 0.05895, train loss3: 0.83941\n",
      "train loss1: 0.55227, train loss2: 3.04623, train loss3: 0.56496\n",
      "train loss1: 0.57362, train loss2: 0.08037, train loss3: 4.54898\n",
      "train loss1: 0.58881, train loss2: 1.10066, train loss3: 2.00651\n",
      "train loss1: 0.55977, train loss2: 0.03095, train loss3: 1.13707\n",
      "train loss1: 0.54533, train loss2: 0.07964, train loss3: 0.22533\n",
      "train loss1: 0.56620, train loss2: 0.29377, train loss3: 1.20064\n",
      "train loss1: 0.55084, train loss2: 0.02599, train loss3: 1.08149\n",
      "train loss1: 0.56422, train loss2: 0.03541, train loss3: 12.68209\n",
      "train loss1: 0.55741, train loss2: 0.04719, train loss3: 1.06099\n",
      "train loss1: 0.55177, train loss2: 0.12722, train loss3: 0.79449\n",
      "train loss1: 0.54216, train loss2: 0.16499, train loss3: 0.62429\n",
      "train loss1: 0.65787, train loss2: 0.04323, train loss3: 0.12466\n",
      "train loss1: 0.57163, train loss2: 0.06288, train loss3: 0.47422\n",
      "train loss1: 0.57243, train loss2: 0.04466, train loss3: 1.23683\n",
      "train loss1: 0.55229, train loss2: 0.08319, train loss3: 1.85606\n",
      "train loss1: 0.57097, train loss2: 0.03554, train loss3: 0.43635\n",
      "train loss1: 0.55087, train loss2: 0.02439, train loss3: 0.22656\n",
      "train loss1: 0.56008, train loss2: 0.02029, train loss3: 0.43396\n",
      "train loss1: 0.56730, train loss2: 0.38485, train loss3: 0.57057\n",
      "train loss1: 0.57105, train loss2: 0.20446, train loss3: 0.33552\n",
      "train loss1: 0.55163, train loss2: 0.87583, train loss3: 0.71476\n",
      "train loss1: 0.56442, train loss2: 0.03973, train loss3: 0.74382\n",
      "train loss1: 0.56111, train loss2: 0.22934, train loss3: 9.02208\n",
      "train loss1: 0.58361, train loss2: 0.03334, train loss3: 3.50287\n",
      "train loss1: 0.58553, train loss2: 0.91564, train loss3: 1.59638\n",
      "train loss1: 0.59734, train loss2: 0.26129, train loss3: 1.43859\n",
      "train loss1: 0.56969, train loss2: 0.27677, train loss3: 21.59188\n",
      "train loss1: 0.55039, train loss2: 0.06295, train loss3: 0.77058\n",
      "train loss1: 0.57008, train loss2: 0.10422, train loss3: 4.98172\n",
      "train loss1: 0.55570, train loss2: 0.06394, train loss3: 0.09355\n",
      "train loss1: 0.54525, train loss2: 1.92732, train loss3: 6.40465\n",
      "train loss1: 0.57671, train loss2: 0.46590, train loss3: 0.20571\n",
      "train loss1: 0.54071, train loss2: 0.08925, train loss3: 139.84387\n",
      "train loss1: 0.57431, train loss2: 0.12981, train loss3: 0.47993\n",
      "train loss1: 0.66768, train loss2: 0.03690, train loss3: 0.24153\n",
      "train loss1: 0.56397, train loss2: 0.02100, train loss3: 3.04793\n",
      "train loss1: 0.55280, train loss2: 0.65837, train loss3: 2.26135\n",
      "train loss1: 0.55052, train loss2: 0.07216, train loss3: 0.77096\n",
      "train loss1: 0.55003, train loss2: 1.64822, train loss3: 5.81862\n",
      "train loss1: 0.57536, train loss2: 0.08900, train loss3: 0.45307\n",
      "train loss1: 0.54926, train loss2: 0.23475, train loss3: 0.65315\n",
      "train loss1: 0.56707, train loss2: 0.03388, train loss3: 3.13683\n",
      "train loss1: 0.59205, train loss2: 0.02790, train loss3: 8.00785\n",
      "train loss1: 0.55171, train loss2: 0.49990, train loss3: 1.34869\n",
      "train loss1: 0.56395, train loss2: 0.08393, train loss3: 9.74698\n",
      "train loss1: 0.56813, train loss2: 0.05032, train loss3: 0.33677\n",
      "train loss1: 0.57140, train loss2: 0.07692, train loss3: 0.20369\n",
      "train loss1: 0.65785, train loss2: 0.19736, train loss3: 19.27688\n",
      "train loss1: 0.56568, train loss2: 0.11511, train loss3: 1.42303\n",
      "train loss1: 0.55599, train loss2: 0.11526, train loss3: 0.47064\n",
      "train loss1: 0.55208, train loss2: 0.02974, train loss3: 0.26047\n",
      "train loss1: 0.55765, train loss2: 0.05996, train loss3: 2.45326\n",
      "train loss1: 0.55374, train loss2: 0.05925, train loss3: 5.77182\n",
      "train loss1: 0.55426, train loss2: 0.04685, train loss3: 2.26444\n",
      "train loss1: 0.55730, train loss2: 8.80674, train loss3: 1.39454\n",
      "train loss1: 0.56454, train loss2: 0.02594, train loss3: 4.26696\n",
      "train loss1: 0.55263, train loss2: 0.46847, train loss3: 0.50819\n",
      "train loss1: 0.55578, train loss2: 0.05414, train loss3: 0.40809\n",
      "train loss1: 0.53973, train loss2: 0.27878, train loss3: 0.39587\n",
      "train loss1: 0.56336, train loss2: 0.03221, train loss3: 1.13100\n",
      "train loss1: 0.55274, train loss2: 0.11132, train loss3: 0.82333\n",
      "train loss1: 0.57836, train loss2: 0.01841, train loss3: 0.30758\n",
      "train loss1: 0.56780, train loss2: 0.04192, train loss3: 2.39109\n",
      "train loss1: 0.54976, train loss2: 0.07218, train loss3: 1.63291\n",
      "train loss1: 0.56992, train loss2: 0.46517, train loss3: 28.77835\n",
      "train loss1: 0.58001, train loss2: 0.25118, train loss3: 0.60053\n",
      "train loss1: 0.58987, train loss2: 0.03760, train loss3: 0.25464\n",
      "train loss1: 0.57413, train loss2: 0.07902, train loss3: 2.56890\n",
      "train loss1: 0.55199, train loss2: 0.17806, train loss3: 0.65781\n",
      "train loss1: 0.64545, train loss2: 0.24211, train loss3: 9.52771\n",
      "train loss1: 0.55102, train loss2: 0.74599, train loss3: 0.14145\n",
      "train loss1: 0.55059, train loss2: 1.68464, train loss3: 1.22288\n",
      "train loss1: 0.54868, train loss2: 0.06532, train loss3: 1.10233\n",
      "train loss1: 0.55181, train loss2: 0.04345, train loss3: 0.87502\n",
      "train loss1: 0.57111, train loss2: 0.24556, train loss3: 0.76555\n",
      "train loss1: 0.56918, train loss2: 0.09554, train loss3: 1.43712\n",
      "train loss1: 0.56962, train loss2: 0.09913, train loss3: 0.18773\n",
      "train loss1: 0.58650, train loss2: 3.39139, train loss3: 0.33970\n",
      "train loss1: 0.56841, train loss2: 0.25798, train loss3: 0.82395\n",
      "train loss1: 0.56065, train loss2: 0.18967, train loss3: 0.39031\n",
      "train loss1: 0.54626, train loss2: 1.67043, train loss3: 12.72841\n",
      "train loss1: 0.56466, train loss2: 0.32937, train loss3: 0.51573\n",
      "train loss1: 0.55510, train loss2: 0.52327, train loss3: 0.49271\n",
      "train loss1: 0.57206, train loss2: 0.16394, train loss3: 0.19899\n",
      "train loss1: 0.57787, train loss2: 0.21619, train loss3: 1.32796\n",
      "train loss1: 0.54572, train loss2: 0.11119, train loss3: 0.81329\n",
      "train loss1: 0.55392, train loss2: 0.04378, train loss3: 3.42800\n",
      "train loss1: 0.55678, train loss2: 0.11018, train loss3: 0.22509\n",
      "train loss1: 0.56134, train loss2: 0.21852, train loss3: 0.17522\n",
      "train loss1: 0.57257, train loss2: 1.61269, train loss3: 1.19825\n",
      "train loss1: 0.55266, train loss2: 0.16143, train loss3: 1.10415\n",
      "train loss1: 0.56319, train loss2: 1.59356, train loss3: 0.40667\n",
      "train loss1: 0.55040, train loss2: 0.67596, train loss3: 0.29205\n",
      "train loss1: 0.58422, train loss2: 0.02043, train loss3: 1.63379\n",
      "train loss1: 0.56526, train loss2: 0.06605, train loss3: 0.38846\n",
      "train loss1: 0.57557, train loss2: 0.19897, train loss3: 1.66156\n",
      "train loss1: 0.57216, train loss2: 0.10118, train loss3: 1.43673\n",
      "train loss1: 0.55879, train loss2: 0.15778, train loss3: 0.20120\n",
      "train loss1: 0.57919, train loss2: 0.05697, train loss3: 0.34077\n",
      "train loss1: 0.57464, train loss2: 0.18741, train loss3: 0.09670\n",
      "train loss1: 0.55583, train loss2: 0.24684, train loss3: 1.30773\n",
      "train loss1: 0.56631, train loss2: 0.02983, train loss3: 0.82949\n",
      "train loss1: 0.55377, train loss2: 0.26978, train loss3: 2.37856\n",
      "train loss1: 0.55683, train loss2: 0.08935, train loss3: 1.03497\n",
      "train loss1: 0.58046, train loss2: 2.63064, train loss3: 8.94173\n",
      "train loss1: 0.56886, train loss2: 46.76702, train loss3: 0.89514\n",
      "train loss1: 0.55961, train loss2: 0.29798, train loss3: 0.87468\n",
      "train loss1: 0.57706, train loss2: 17.44711, train loss3: 1.66888\n",
      "train loss1: 0.56567, train loss2: 0.04614, train loss3: 0.28113\n",
      "train loss1: 0.55525, train loss2: 0.04913, train loss3: 0.35411\n",
      "train loss1: 0.53304, train loss2: 0.04841, train loss3: 0.16608\n",
      "train loss1: 0.59281, train loss2: 0.52231, train loss3: 0.16291\n",
      "train loss1: 0.56569, train loss2: 2.06953, train loss3: 0.09990\n",
      "train loss1: 0.55465, train loss2: 0.25434, train loss3: 184.69240\n",
      "train loss1: 0.56490, train loss2: 0.03129, train loss3: 6.30156\n",
      "train loss1: 0.55492, train loss2: 0.22059, train loss3: 0.21036\n",
      "train loss1: 0.56143, train loss2: 0.06399, train loss3: 1.44446\n",
      "train loss1: 0.57256, train loss2: 0.04607, train loss3: 0.57986\n",
      "train loss1: 0.55210, train loss2: 0.03162, train loss3: 0.37308\n",
      "train loss1: 0.55673, train loss2: 0.62271, train loss3: 18.74821\n",
      "train loss1: 0.58549, train loss2: 192.25592, train loss3: 1.56211\n",
      "train loss1: 0.54585, train loss2: 0.23489, train loss3: 264.48288\n",
      "train loss1: 0.59260, train loss2: 0.08986, train loss3: 6.93485\n",
      "train loss1: 0.53969, train loss2: 0.05897, train loss3: 0.49881\n",
      "train loss1: 0.60691, train loss2: 0.50839, train loss3: 0.57029\n",
      "train loss1: 0.56934, train loss2: 0.33898, train loss3: 0.28318\n",
      "train loss1: 0.54988, train loss2: 0.05159, train loss3: 1.71373\n",
      "train loss1: 0.54820, train loss2: 0.17164, train loss3: 29.71998\n",
      "train loss1: 0.56793, train loss2: 0.12705, train loss3: 5.18083\n",
      "train loss1: 0.53796, train loss2: 0.05340, train loss3: 3.72514\n",
      "train loss1: 0.55822, train loss2: 0.35642, train loss3: 0.43159\n",
      "train loss1: 0.54909, train loss2: 0.03014, train loss3: 0.13142\n",
      "train loss1: 0.59952, train loss2: 0.06210, train loss3: 9.28725\n",
      "train loss1: 0.59929, train loss2: 0.93615, train loss3: 0.55862\n",
      "train loss1: 0.53709, train loss2: 0.05422, train loss3: 0.98231\n",
      "train loss1: 0.55955, train loss2: 0.04113, train loss3: 0.60006\n",
      "train loss1: 0.54695, train loss2: 0.04318, train loss3: 1.87685\n",
      "train loss1: 0.54994, train loss2: 0.32528, train loss3: 5.26024\n",
      "train loss1: 0.57329, train loss2: 0.01698, train loss3: 0.56809\n",
      "train loss1: 0.55726, train loss2: 0.11496, train loss3: 0.43093\n",
      "train loss1: 0.57496, train loss2: 0.06242, train loss3: 0.55031\n",
      "train loss1: 0.57213, train loss2: 0.07936, train loss3: 0.42447\n",
      "train loss1: 0.54364, train loss2: 4.05531, train loss3: 0.44739\n",
      "train loss1: 0.54934, train loss2: 0.09952, train loss3: 0.24053\n",
      "train loss1: 0.55899, train loss2: 0.75682, train loss3: 0.10359\n",
      "train loss1: 0.57516, train loss2: 0.08559, train loss3: 0.94561\n",
      "train loss1: 0.55430, train loss2: 0.21052, train loss3: 1.03746\n",
      "train loss1: 0.56594, train loss2: 0.09916, train loss3: 1.63848\n",
      "train loss1: 0.56404, train loss2: 0.02924, train loss3: 0.30532\n",
      "train loss1: 0.58564, train loss2: 0.16992, train loss3: 5.19488\n",
      "train loss1: 0.55461, train loss2: 0.87697, train loss3: 1.13674\n",
      "train loss1: 0.64322, train loss2: 0.03917, train loss3: 0.31505\n",
      "train loss1: 0.54334, train loss2: 0.05036, train loss3: 0.28496\n",
      "train loss1: 0.54197, train loss2: 0.04991, train loss3: 0.22304\n",
      "train loss1: 0.53814, train loss2: 0.29279, train loss3: 1.12692\n",
      "train loss1: 0.56554, train loss2: 11.27931, train loss3: 2.45881\n",
      "train loss1: 0.55941, train loss2: 0.04923, train loss3: 0.38845\n",
      "train loss1: 0.53727, train loss2: 0.04270, train loss3: 0.06608\n",
      "train loss1: 0.55530, train loss2: 0.10202, train loss3: 0.75166\n",
      "train loss1: 0.57192, train loss2: 0.08548, train loss3: 0.30038\n",
      "train loss1: 0.57045, train loss2: 0.29777, train loss3: 0.17794\n",
      "train loss1: 0.54331, train loss2: 9.12774, train loss3: 1.83725\n",
      "train loss1: 0.56840, train loss2: 0.08626, train loss3: 1.04608\n",
      "train loss1: 0.55815, train loss2: 0.04520, train loss3: 0.74847\n",
      "train loss1: 0.55246, train loss2: 0.03598, train loss3: 0.68175\n",
      "train loss1: 0.55436, train loss2: 0.24598, train loss3: 0.29858\n",
      "train loss1: 0.56050, train loss2: 0.02700, train loss3: 0.28634\n",
      "train loss1: 0.57032, train loss2: 0.05526, train loss3: 0.58778\n",
      "train loss1: 0.55393, train loss2: 0.03035, train loss3: 0.10434\n",
      "train loss1: 0.55100, train loss2: 3.84632, train loss3: 1.22675\n",
      "train loss1: 0.56061, train loss2: 0.06820, train loss3: 0.60859\n",
      "train loss1: 0.55991, train loss2: 0.06959, train loss3: 0.12414\n",
      "train loss1: 0.54716, train loss2: 0.63890, train loss3: 15.30040\n",
      "train loss1: 0.55503, train loss2: 0.06936, train loss3: 1.46385\n",
      "train loss1: 0.57142, train loss2: 0.09174, train loss3: 16.39148\n",
      "train loss1: 0.56613, train loss2: 0.06506, train loss3: 0.18475\n",
      "train loss1: 0.56942, train loss2: 0.11812, train loss3: 0.38521\n",
      "train loss1: 0.55926, train loss2: 0.29305, train loss3: 3.17411\n",
      "train loss1: 0.56233, train loss2: 0.13459, train loss3: 32.63582\n",
      "train loss1: 0.55631, train loss2: 0.02206, train loss3: 0.10071\n",
      "train loss1: 0.54786, train loss2: 0.06795, train loss3: 7.03973\n",
      "train loss1: 0.54585, train loss2: 0.51585, train loss3: 0.19199\n",
      "train loss1: 0.54445, train loss2: 0.05805, train loss3: 1.06532\n",
      "train loss1: 0.61802, train loss2: 0.13787, train loss3: 0.63892\n",
      "train loss1: 0.59646, train loss2: 2.21152, train loss3: 0.98511\n",
      "train loss1: 0.56306, train loss2: 0.04819, train loss3: 0.59199\n",
      "train loss1: 0.54757, train loss2: 0.03840, train loss3: 0.65827\n",
      "train loss1: 0.55063, train loss2: 0.09829, train loss3: 0.27232\n",
      "train loss1: 0.55381, train loss2: 0.05344, train loss3: 0.81967\n",
      "train loss1: 0.53650, train loss2: 0.06720, train loss3: 1.33146\n",
      "train loss1: 0.53267, train loss2: 0.06059, train loss3: 0.77652\n",
      "train loss1: 0.56872, train loss2: 0.06555, train loss3: 0.45898\n",
      "train loss1: 0.55042, train loss2: 0.10036, train loss3: 1.87976\n",
      "train loss1: 0.56112, train loss2: 0.04763, train loss3: 2.37319\n",
      "train loss1: 0.64588, train loss2: 0.04940, train loss3: 1.45945\n",
      "train loss1: 0.58527, train loss2: 0.03861, train loss3: 0.43607\n",
      "train loss1: 0.54953, train loss2: 0.04577, train loss3: 0.63912\n",
      "train loss1: 0.54685, train loss2: 46.18332, train loss3: 2.12343\n",
      "train loss1: 0.55008, train loss2: 0.04212, train loss3: 0.24269\n",
      "train loss1: 0.56397, train loss2: 0.19913, train loss3: 4.66530\n",
      "train loss1: 0.55288, train loss2: 0.35183, train loss3: 0.63470\n",
      "train loss1: 0.54954, train loss2: 0.03644, train loss3: 0.25381\n",
      "train loss1: 0.55347, train loss2: 0.29082, train loss3: 1.09394\n",
      "train loss1: 0.54054, train loss2: 0.60510, train loss3: 3.59761\n",
      "train loss1: 0.56666, train loss2: 0.41776, train loss3: 1.17554\n",
      "train loss1: 0.56243, train loss2: 0.25036, train loss3: 1.39477\n",
      "train loss1: 0.56226, train loss2: 0.12503, train loss3: 0.16177\n",
      "train loss1: 0.56361, train loss2: 0.02665, train loss3: 2.68918\n",
      "train loss1: 0.58535, train loss2: 0.75958, train loss3: 0.34651\n",
      "train loss1: 0.56252, train loss2: 0.24069, train loss3: 0.24071\n",
      "train loss1: 0.55122, train loss2: 0.20246, train loss3: 1.91073\n",
      "train loss1: 0.56020, train loss2: 0.06288, train loss3: 3.46944\n",
      "train loss1: 0.61261, train loss2: 0.02652, train loss3: 0.31172\n",
      "Epoch: 9/10\n",
      "train loss1: 0.56756, train loss2: 0.04321, train loss3: 0.15832\n",
      "train loss1: 0.55598, train loss2: 0.02851, train loss3: 7.72487\n",
      "train loss1: 0.54871, train loss2: 0.03570, train loss3: 3.54044\n",
      "train loss1: 0.56602, train loss2: 0.04922, train loss3: 0.99190\n",
      "train loss1: 0.55574, train loss2: 0.08082, train loss3: 0.51732\n",
      "train loss1: 0.56191, train loss2: 0.02704, train loss3: 0.41899\n",
      "train loss1: 0.54159, train loss2: 0.28840, train loss3: 5.34111\n",
      "train loss1: 0.56380, train loss2: 0.54336, train loss3: 0.24495\n",
      "train loss1: 0.56093, train loss2: 0.16712, train loss3: 2.71973\n",
      "train loss1: 0.55713, train loss2: 0.23031, train loss3: 0.78706\n",
      "train loss1: 0.64190, train loss2: 1.02567, train loss3: 19.93716\n",
      "train loss1: 0.56311, train loss2: 0.05523, train loss3: 0.63137\n",
      "train loss1: 0.57293, train loss2: 0.96455, train loss3: 0.67336\n",
      "train loss1: 0.55114, train loss2: 0.02823, train loss3: 0.12920\n",
      "train loss1: 0.55941, train loss2: 1.85397, train loss3: 0.54873\n",
      "train loss1: 0.54929, train loss2: 0.13423, train loss3: 0.41071\n",
      "train loss1: 0.63499, train loss2: 0.03935, train loss3: 0.37391\n",
      "train loss1: 0.54514, train loss2: 0.06648, train loss3: 4.64727\n",
      "train loss1: 0.55307, train loss2: 0.15425, train loss3: 0.47183\n",
      "train loss1: 0.56220, train loss2: 0.95027, train loss3: 0.51438\n",
      "train loss1: 0.54637, train loss2: 2.56640, train loss3: 8.86373\n",
      "train loss1: 0.53939, train loss2: 0.06616, train loss3: 5.77004\n",
      "train loss1: 0.54844, train loss2: 1.74386, train loss3: 0.14301\n",
      "train loss1: 0.58614, train loss2: 0.10554, train loss3: 0.13146\n",
      "train loss1: 0.55769, train loss2: 0.05655, train loss3: 1.57725\n",
      "train loss1: 0.54634, train loss2: 0.23103, train loss3: 0.23379\n",
      "train loss1: 0.55502, train loss2: 0.10024, train loss3: 4.96119\n",
      "train loss1: 0.56275, train loss2: 0.27292, train loss3: 0.33576\n",
      "train loss1: 0.55891, train loss2: 0.27402, train loss3: 3.21078\n",
      "train loss1: 0.55760, train loss2: 0.02989, train loss3: 0.64026\n",
      "train loss1: 0.53266, train loss2: 2.85706, train loss3: 2.29638\n",
      "train loss1: 0.54778, train loss2: 0.12866, train loss3: 1.04335\n",
      "train loss1: 0.57865, train loss2: 0.33313, train loss3: 0.45521\n",
      "train loss1: 0.56936, train loss2: 0.05986, train loss3: 1.40172\n",
      "train loss1: 0.55444, train loss2: 0.05355, train loss3: 0.27829\n",
      "train loss1: 0.54673, train loss2: 0.04820, train loss3: 11.99363\n",
      "train loss1: 0.55435, train loss2: 0.10570, train loss3: 16.19214\n",
      "train loss1: 0.53935, train loss2: 0.10611, train loss3: 0.52978\n",
      "train loss1: 0.63676, train loss2: 0.07901, train loss3: 0.44570\n",
      "train loss1: 0.56572, train loss2: 0.22847, train loss3: 3.15819\n",
      "train loss1: 0.53588, train loss2: 0.06368, train loss3: 1.70918\n",
      "train loss1: 0.54267, train loss2: 0.10561, train loss3: 12.29284\n",
      "train loss1: 0.54910, train loss2: 0.13698, train loss3: 0.25821\n",
      "train loss1: 0.58903, train loss2: 0.04965, train loss3: 0.12660\n",
      "train loss1: 0.54452, train loss2: 0.91397, train loss3: 0.24410\n",
      "train loss1: 0.55215, train loss2: 0.05000, train loss3: 0.61799\n",
      "train loss1: 0.54732, train loss2: 0.62588, train loss3: 0.56508\n",
      "train loss1: 0.57447, train loss2: 0.45308, train loss3: 1.00352\n",
      "train loss1: 0.53927, train loss2: 0.05341, train loss3: 1.02868\n",
      "train loss1: 0.58059, train loss2: 0.03963, train loss3: 15.96959\n",
      "train loss1: 0.55790, train loss2: 0.23858, train loss3: 0.28968\n",
      "train loss1: 0.58423, train loss2: 0.07727, train loss3: 0.31873\n",
      "train loss1: 0.58044, train loss2: 0.05020, train loss3: 1.44617\n",
      "train loss1: 0.55474, train loss2: 0.02694, train loss3: 0.11994\n",
      "train loss1: 0.56397, train loss2: 0.07027, train loss3: 0.22280\n",
      "train loss1: 0.54895, train loss2: 0.02884, train loss3: 6.30194\n",
      "train loss1: 0.57519, train loss2: 0.04479, train loss3: 0.28420\n",
      "train loss1: 0.57102, train loss2: 0.02886, train loss3: 0.16353\n",
      "train loss1: 0.55566, train loss2: 0.23457, train loss3: 0.54462\n",
      "train loss1: 0.55135, train loss2: 0.09200, train loss3: 0.39513\n",
      "train loss1: 0.55234, train loss2: 0.05540, train loss3: 0.87570\n",
      "train loss1: 0.65496, train loss2: 2.70885, train loss3: 8.23220\n",
      "train loss1: 0.57219, train loss2: 0.28511, train loss3: 1.92033\n",
      "train loss1: 0.57610, train loss2: 0.26349, train loss3: 1.33862\n",
      "train loss1: 0.56457, train loss2: 0.41545, train loss3: 0.49882\n",
      "train loss1: 0.55332, train loss2: 0.09605, train loss3: 0.41107\n",
      "train loss1: 0.55738, train loss2: 0.13094, train loss3: 0.14844\n",
      "train loss1: 0.56995, train loss2: 0.85298, train loss3: 17.03261\n",
      "train loss1: 0.55937, train loss2: 0.90957, train loss3: 0.28097\n",
      "train loss1: 0.57231, train loss2: 0.12556, train loss3: 1.56241\n",
      "train loss1: 0.56119, train loss2: 0.02645, train loss3: 0.14188\n",
      "train loss1: 0.55623, train loss2: 0.07809, train loss3: 1.63706\n",
      "train loss1: 0.56679, train loss2: 0.05989, train loss3: 1.60362\n",
      "train loss1: 0.53578, train loss2: 0.70393, train loss3: 0.42200\n",
      "train loss1: 0.55348, train loss2: 0.03805, train loss3: 0.88250\n",
      "train loss1: 0.56159, train loss2: 0.10444, train loss3: 1.79688\n",
      "train loss1: 0.54952, train loss2: 0.08959, train loss3: 1.28464\n",
      "train loss1: 0.54437, train loss2: 0.19541, train loss3: 3.93114\n",
      "train loss1: 0.65811, train loss2: 0.05603, train loss3: 0.89415\n",
      "train loss1: 0.57186, train loss2: 0.09078, train loss3: 23.95230\n",
      "train loss1: 0.54488, train loss2: 0.04937, train loss3: 0.90975\n",
      "train loss1: 0.55573, train loss2: 0.03446, train loss3: 0.88983\n",
      "train loss1: 0.55614, train loss2: 1.71407, train loss3: 0.34473\n",
      "train loss1: 0.66228, train loss2: 0.74994, train loss3: 0.21176\n",
      "train loss1: 0.55422, train loss2: 0.49436, train loss3: 0.50031\n",
      "train loss1: 0.54375, train loss2: 0.07314, train loss3: 0.43802\n",
      "train loss1: 0.55053, train loss2: 0.39171, train loss3: 1.21180\n",
      "train loss1: 0.55633, train loss2: 14.86024, train loss3: 5.12198\n",
      "train loss1: 0.55202, train loss2: 0.04943, train loss3: 0.73211\n",
      "train loss1: 0.56784, train loss2: 0.40598, train loss3: 1.07185\n",
      "train loss1: 0.67006, train loss2: 0.06874, train loss3: 3.67228\n",
      "train loss1: 0.54644, train loss2: 3.42643, train loss3: 0.13116\n",
      "train loss1: 0.55647, train loss2: 0.39576, train loss3: 1.16610\n",
      "train loss1: 0.53902, train loss2: 0.09675, train loss3: 0.37619\n",
      "train loss1: 0.56754, train loss2: 0.36610, train loss3: 0.23779\n",
      "train loss1: 0.57008, train loss2: 0.29152, train loss3: 2.06669\n",
      "train loss1: 0.54712, train loss2: 0.07784, train loss3: 4.99243\n",
      "train loss1: 0.55286, train loss2: 0.03391, train loss3: 0.19112\n",
      "train loss1: 0.56603, train loss2: 0.03215, train loss3: 7.32162\n",
      "train loss1: 0.55202, train loss2: 0.04177, train loss3: 0.36858\n",
      "train loss1: 0.60817, train loss2: 0.11389, train loss3: 0.25990\n",
      "train loss1: 0.53743, train loss2: 3.57112, train loss3: 0.20227\n",
      "train loss1: 0.55533, train loss2: 2.36982, train loss3: 1.19463\n",
      "train loss1: 0.53782, train loss2: 0.39174, train loss3: 3.49168\n",
      "train loss1: 0.53871, train loss2: 0.25624, train loss3: 0.59939\n",
      "train loss1: 0.59611, train loss2: 1.61425, train loss3: 3.43461\n",
      "train loss1: 0.53686, train loss2: 0.07636, train loss3: 1.55078\n",
      "train loss1: 0.52291, train loss2: 0.03805, train loss3: 0.16421\n",
      "train loss1: 0.54574, train loss2: 0.07792, train loss3: 0.19601\n",
      "train loss1: 0.54541, train loss2: 0.14265, train loss3: 6.96697\n",
      "train loss1: 0.56373, train loss2: 0.01546, train loss3: 0.57342\n",
      "train loss1: 0.58451, train loss2: 0.39182, train loss3: 0.27287\n",
      "train loss1: 0.54561, train loss2: 0.07502, train loss3: 1.92793\n",
      "train loss1: 0.54741, train loss2: 0.05343, train loss3: 0.37267\n",
      "train loss1: 0.54072, train loss2: 0.23847, train loss3: 3.18307\n",
      "train loss1: 0.55702, train loss2: 0.12311, train loss3: 19.42355\n",
      "train loss1: 0.54789, train loss2: 0.71106, train loss3: 0.11273\n",
      "train loss1: 0.56558, train loss2: 0.11480, train loss3: 0.25129\n",
      "train loss1: 0.54868, train loss2: 0.12540, train loss3: 0.83152\n",
      "train loss1: 0.53857, train loss2: 1.25299, train loss3: 0.94363\n",
      "train loss1: 0.56052, train loss2: 0.11626, train loss3: 0.40389\n",
      "train loss1: 0.56219, train loss2: 2.27172, train loss3: 21.15762\n",
      "train loss1: 0.53473, train loss2: 0.04089, train loss3: 1.71659\n",
      "train loss1: 0.55495, train loss2: 0.24095, train loss3: 3.26051\n",
      "train loss1: 0.53307, train loss2: 0.34118, train loss3: 2.32057\n",
      "train loss1: 0.55383, train loss2: 46.78756, train loss3: 0.33283\n",
      "train loss1: 0.54531, train loss2: 0.05982, train loss3: 0.45755\n",
      "train loss1: 0.53852, train loss2: 0.05723, train loss3: 3.39374\n",
      "train loss1: 0.55137, train loss2: 0.15960, train loss3: 2.50131\n",
      "train loss1: 0.54390, train loss2: 4.09421, train loss3: 6.32220\n",
      "train loss1: 0.53066, train loss2: 0.04276, train loss3: 18.90821\n",
      "train loss1: 0.56615, train loss2: 0.24330, train loss3: 0.31025\n",
      "train loss1: 0.55827, train loss2: 0.02919, train loss3: 1.55700\n",
      "train loss1: 0.55150, train loss2: 11.26542, train loss3: 0.79897\n",
      "train loss1: 0.54840, train loss2: 113.00592, train loss3: 0.43315\n",
      "train loss1: 0.53982, train loss2: 0.06248, train loss3: 0.87440\n",
      "train loss1: 0.55521, train loss2: 0.08831, train loss3: 1.06288\n",
      "train loss1: 0.56349, train loss2: 0.02339, train loss3: 8.26790\n",
      "train loss1: 0.54414, train loss2: 0.04530, train loss3: 0.25757\n",
      "train loss1: 0.56143, train loss2: 0.03255, train loss3: 2.72924\n",
      "train loss1: 0.55950, train loss2: 4.29424, train loss3: 0.69173\n",
      "train loss1: 0.56514, train loss2: 0.04814, train loss3: 0.82468\n",
      "train loss1: 0.53652, train loss2: 0.04712, train loss3: 1.23887\n",
      "train loss1: 0.57095, train loss2: 0.36508, train loss3: 16.19447\n",
      "train loss1: 0.55875, train loss2: 0.13603, train loss3: 0.39362\n",
      "train loss1: 0.54144, train loss2: 0.05182, train loss3: 2.06892\n",
      "train loss1: 0.57259, train loss2: 0.03293, train loss3: 0.54499\n",
      "train loss1: 0.63620, train loss2: 0.09314, train loss3: 0.27020\n",
      "train loss1: 0.54974, train loss2: 147.07213, train loss3: 6.08982\n",
      "train loss1: 0.53575, train loss2: 0.09892, train loss3: 1.30170\n",
      "train loss1: 0.55976, train loss2: 0.04745, train loss3: 0.28864\n",
      "train loss1: 0.58209, train loss2: 0.78156, train loss3: 0.66312\n",
      "train loss1: 0.55359, train loss2: 0.07453, train loss3: 1.98799\n",
      "train loss1: 0.54090, train loss2: 0.04044, train loss3: 0.45467\n",
      "train loss1: 0.54061, train loss2: 0.08806, train loss3: 0.61653\n",
      "train loss1: 0.55836, train loss2: 0.07840, train loss3: 0.73739\n",
      "train loss1: 0.54572, train loss2: 0.44203, train loss3: 0.87265\n",
      "train loss1: 0.55347, train loss2: 0.05517, train loss3: 0.15685\n",
      "train loss1: 0.54614, train loss2: 0.04882, train loss3: 0.16825\n",
      "train loss1: 0.53845, train loss2: 0.04192, train loss3: 0.34556\n",
      "train loss1: 0.53638, train loss2: 1.95902, train loss3: 0.18893\n",
      "train loss1: 0.54258, train loss2: 0.40838, train loss3: 1.32029\n",
      "train loss1: 0.56386, train loss2: 8.81083, train loss3: 0.12723\n",
      "train loss1: 0.54625, train loss2: 0.88085, train loss3: 0.18533\n",
      "train loss1: 0.54429, train loss2: 0.87428, train loss3: 0.10456\n",
      "train loss1: 0.55492, train loss2: 0.07836, train loss3: 0.30093\n",
      "train loss1: 0.54804, train loss2: 0.06993, train loss3: 2.66832\n",
      "train loss1: 0.53076, train loss2: 0.11990, train loss3: 1.14067\n",
      "train loss1: 0.57134, train loss2: 0.98172, train loss3: 0.26380\n",
      "train loss1: 0.56364, train loss2: 0.25442, train loss3: 0.41131\n",
      "train loss1: 0.54975, train loss2: 0.13003, train loss3: 1.29219\n",
      "train loss1: 0.55624, train loss2: 0.06946, train loss3: 2.64833\n",
      "train loss1: 0.55564, train loss2: 0.10847, train loss3: 205.28453\n",
      "train loss1: 0.55375, train loss2: 1.68509, train loss3: 1.98084\n",
      "train loss1: 0.54329, train loss2: 0.07672, train loss3: 0.63959\n",
      "train loss1: 0.54799, train loss2: 2.63295, train loss3: 1.36013\n",
      "train loss1: 0.55168, train loss2: 0.04324, train loss3: 0.63710\n",
      "train loss1: 0.55994, train loss2: 0.02844, train loss3: 1.06864\n",
      "train loss1: 0.53800, train loss2: 0.02546, train loss3: 3.70970\n",
      "train loss1: 0.55526, train loss2: 0.06885, train loss3: 145.85707\n",
      "train loss1: 0.54835, train loss2: 0.04566, train loss3: 0.22486\n",
      "train loss1: 0.65657, train loss2: 0.06457, train loss3: 0.29759\n",
      "train loss1: 0.56424, train loss2: 48.32852, train loss3: 0.79680\n",
      "train loss1: 0.56243, train loss2: 1.09782, train loss3: 0.17897\n",
      "train loss1: 0.55078, train loss2: 0.03950, train loss3: 1.39746\n",
      "train loss1: 0.55850, train loss2: 0.07426, train loss3: 1.79507\n",
      "train loss1: 0.55576, train loss2: 0.12103, train loss3: 0.52751\n",
      "train loss1: 0.54684, train loss2: 0.30900, train loss3: 0.96096\n",
      "train loss1: 0.55700, train loss2: 0.16825, train loss3: 1.44226\n",
      "train loss1: 0.55451, train loss2: 0.02864, train loss3: 28.22642\n",
      "train loss1: 0.53545, train loss2: 0.20799, train loss3: 0.66749\n",
      "train loss1: 0.54668, train loss2: 0.07962, train loss3: 0.43796\n",
      "train loss1: 0.55594, train loss2: 0.29272, train loss3: 2.19074\n",
      "train loss1: 0.55048, train loss2: 0.03090, train loss3: 0.54242\n",
      "train loss1: 0.55178, train loss2: 2.50010, train loss3: 0.20278\n",
      "train loss1: 0.53536, train loss2: 0.03168, train loss3: 0.23403\n",
      "train loss1: 0.53720, train loss2: 0.05428, train loss3: 0.59303\n",
      "train loss1: 0.55039, train loss2: 0.02438, train loss3: 0.58058\n",
      "train loss1: 0.52799, train loss2: 0.01894, train loss3: 1.84257\n",
      "train loss1: 0.54876, train loss2: 0.09605, train loss3: 0.50826\n",
      "train loss1: 0.55832, train loss2: 0.18235, train loss3: 2.23652\n",
      "train loss1: 0.54198, train loss2: 0.66323, train loss3: 0.28021\n",
      "train loss1: 0.53409, train loss2: 0.05461, train loss3: 1.09204\n",
      "train loss1: 0.54419, train loss2: 0.22362, train loss3: 0.39530\n",
      "train loss1: 0.52897, train loss2: 0.04020, train loss3: 0.28306\n",
      "train loss1: 0.53176, train loss2: 0.17993, train loss3: 0.84215\n",
      "train loss1: 0.53477, train loss2: 0.08309, train loss3: 0.24138\n",
      "train loss1: 0.54219, train loss2: 0.08270, train loss3: 0.21262\n",
      "train loss1: 0.52499, train loss2: 0.05153, train loss3: 2.29366\n",
      "train loss1: 0.53339, train loss2: 0.19313, train loss3: 28.06768\n",
      "train loss1: 0.56094, train loss2: 3.82887, train loss3: 238.16048\n",
      "train loss1: 0.56270, train loss2: 0.19365, train loss3: 5.11486\n",
      "train loss1: 0.55005, train loss2: 0.01968, train loss3: 0.60579\n",
      "train loss1: 0.56304, train loss2: 8.94950, train loss3: 1.30635\n",
      "train loss1: 0.54834, train loss2: 0.05851, train loss3: 0.37026\n",
      "train loss1: 0.55093, train loss2: 0.16313, train loss3: 0.07600\n",
      "train loss1: 0.55377, train loss2: 0.06365, train loss3: 0.28705\n",
      "train loss1: 0.56432, train loss2: 0.07901, train loss3: 0.20618\n",
      "train loss1: 0.56024, train loss2: 0.05176, train loss3: 0.37047\n",
      "train loss1: 0.55736, train loss2: 0.02990, train loss3: 0.15136\n",
      "train loss1: 0.55124, train loss2: 0.09367, train loss3: 0.22993\n",
      "train loss1: 0.56460, train loss2: 0.06739, train loss3: 0.16858\n",
      "train loss1: 0.54577, train loss2: 0.19651, train loss3: 0.30190\n",
      "train loss1: 0.55871, train loss2: 0.07718, train loss3: 1.31211\n",
      "train loss1: 0.54865, train loss2: 0.03244, train loss3: 0.27512\n",
      "train loss1: 0.55298, train loss2: 0.09148, train loss3: 0.92962\n",
      "train loss1: 0.55036, train loss2: 0.02811, train loss3: 0.15014\n",
      "train loss1: 0.54299, train loss2: 0.23434, train loss3: 0.26502\n",
      "train loss1: 0.54264, train loss2: 0.02701, train loss3: 0.76257\n",
      "train loss1: 0.53421, train loss2: 0.13494, train loss3: 30.26730\n",
      "train loss1: 0.55680, train loss2: 0.10292, train loss3: 0.36018\n",
      "train loss1: 0.56500, train loss2: 0.35594, train loss3: 0.28227\n",
      "train loss1: 0.55130, train loss2: 0.05232, train loss3: 0.09901\n",
      "train loss1: 0.54620, train loss2: 45.39559, train loss3: 2.43078\n",
      "train loss1: 0.53249, train loss2: 0.10509, train loss3: 0.41819\n",
      "train loss1: 0.54067, train loss2: 1.11840, train loss3: 1.00559\n",
      "train loss1: 0.57119, train loss2: 0.91370, train loss3: 1.34763\n",
      "train loss1: 0.55844, train loss2: 0.03870, train loss3: 2.29510\n",
      "train loss1: 0.53447, train loss2: 0.04655, train loss3: 0.74643\n",
      "train loss1: 0.54748, train loss2: 0.03984, train loss3: 0.18995\n",
      "train loss1: 0.53983, train loss2: 0.16149, train loss3: 1.10120\n",
      "train loss1: 0.52490, train loss2: 0.12082, train loss3: 0.82515\n",
      "train loss1: 0.65188, train loss2: 0.79904, train loss3: 4.43646\n",
      "train loss1: 0.52404, train loss2: 0.07664, train loss3: 0.89282\n",
      "train loss1: 0.55194, train loss2: 0.18705, train loss3: 0.08610\n",
      "train loss1: 0.58229, train loss2: 0.27958, train loss3: 1.11731\n",
      "train loss1: 0.55546, train loss2: 0.21760, train loss3: 0.68809\n",
      "train loss1: 0.53842, train loss2: 0.19873, train loss3: 0.45998\n",
      "train loss1: 0.54283, train loss2: 0.08921, train loss3: 0.52350\n",
      "train loss1: 0.55287, train loss2: 192.13345, train loss3: 0.53390\n",
      "train loss1: 0.53251, train loss2: 0.04722, train loss3: 0.28442\n",
      "train loss1: 0.54344, train loss2: 0.40588, train loss3: 1.35709\n",
      "train loss1: 0.53490, train loss2: 0.07333, train loss3: 3.15519\n",
      "train loss1: 0.53089, train loss2: 0.09555, train loss3: 0.10879\n",
      "train loss1: 0.55201, train loss2: 0.10299, train loss3: 0.27044\n",
      "train loss1: 0.55190, train loss2: 0.04427, train loss3: 0.23586\n",
      "train loss1: 0.55022, train loss2: 0.07194, train loss3: 0.38233\n",
      "train loss1: 0.56977, train loss2: 0.16037, train loss3: 1.78366\n",
      "train loss1: 0.53218, train loss2: 0.70797, train loss3: 0.21374\n",
      "train loss1: 0.58490, train loss2: 0.02126, train loss3: 0.27193\n",
      "train loss1: 0.54802, train loss2: 2.07081, train loss3: 0.64428\n",
      "train loss1: 0.54169, train loss2: 0.24182, train loss3: 0.17282\n",
      "train loss1: 0.53955, train loss2: 0.16910, train loss3: 16.47376\n",
      "train loss1: 0.55086, train loss2: 0.43244, train loss3: 0.80330\n",
      "train loss1: 0.54952, train loss2: 0.02853, train loss3: 1.02137\n",
      "train loss1: 0.54334, train loss2: 0.05208, train loss3: 0.47120\n",
      "train loss1: 0.52432, train loss2: 0.22843, train loss3: 0.16067\n",
      "train loss1: 0.54886, train loss2: 0.27324, train loss3: 9.90676\n",
      "train loss1: 0.54647, train loss2: 0.15987, train loss3: 0.43258\n",
      "train loss1: 0.53844, train loss2: 0.04849, train loss3: 3.01744\n",
      "train loss1: 0.55930, train loss2: 0.27159, train loss3: 0.20240\n",
      "train loss1: 0.55401, train loss2: 0.86692, train loss3: 2.59905\n",
      "train loss1: 0.54807, train loss2: 0.39072, train loss3: 0.51881\n",
      "train loss1: 0.52904, train loss2: 0.58611, train loss3: 0.95683\n",
      "train loss1: 0.53630, train loss2: 0.03078, train loss3: 0.63268\n",
      "train loss1: 0.56093, train loss2: 0.02186, train loss3: 2.46028\n",
      "train loss1: 0.54770, train loss2: 0.03150, train loss3: 0.43539\n",
      "train loss1: 0.53223, train loss2: 0.02795, train loss3: 0.16086\n",
      "train loss1: 0.52507, train loss2: 0.18367, train loss3: 0.16912\n",
      "train loss1: 0.57093, train loss2: 0.10745, train loss3: 2.98813\n",
      "train loss1: 0.53631, train loss2: 0.03287, train loss3: 1.34115\n",
      "train loss1: 0.53492, train loss2: 0.10252, train loss3: 54.95736\n",
      "train loss1: 0.53438, train loss2: 0.02297, train loss3: 0.40676\n",
      "train loss1: 0.55177, train loss2: 0.21471, train loss3: 0.43177\n",
      "train loss1: 0.54477, train loss2: 0.73049, train loss3: 0.45884\n",
      "train loss1: 0.53076, train loss2: 4.48779, train loss3: 1.58869\n",
      "train loss1: 0.53921, train loss2: 0.04638, train loss3: 0.35660\n",
      "train loss1: 0.55998, train loss2: 0.01586, train loss3: 1.11398\n",
      "train loss1: 0.53583, train loss2: 0.05933, train loss3: 0.15993\n",
      "train loss1: 0.55715, train loss2: 0.01817, train loss3: 0.48511\n",
      "train loss1: 0.53640, train loss2: 0.09525, train loss3: 0.39378\n",
      "train loss1: 0.55080, train loss2: 2.68390, train loss3: 1.20011\n",
      "train loss1: 0.54894, train loss2: 0.25879, train loss3: 2.83967\n",
      "train loss1: 0.55820, train loss2: 0.06035, train loss3: 0.28344\n",
      "train loss1: 0.53680, train loss2: 0.18508, train loss3: 0.27952\n",
      "train loss1: 0.55028, train loss2: 0.09726, train loss3: 5.43021\n",
      "train loss1: 0.53881, train loss2: 0.01624, train loss3: 0.17216\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    c = 0\n",
    "    print(\"Epoch: {}/{}\".format(epoch, n_epochs))\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "\n",
    "        y1, y2, y3 = y[:, 0], y[:, 1], y[:, 2]\n",
    "        y_1, y_2, y_3 = y_hat[0], y_hat[1], y_hat[2]\n",
    "\n",
    "        loss1 = bce_loss_fn(y_1, y1.view(-1, 1))\n",
    "        loss2 = 0.000001 * mse_loss_fn(y_2, y2.view(-1, 1))\n",
    "        loss3 = 0.00001 * mse_loss_fn(y_3, y3.view(-1, 1))\n",
    "        \n",
    "        loss = loss1 + loss2 + loss3\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        print('train loss1: {:.5f}, train loss2: {:.5f}, train loss3: {:.5f}'.format(loss1.item(), loss2.item(), loss3.item()))\n",
    "\n",
    "    losses.append(np.mean(epoch_loss))\n",
    "\n",
    "    # auc1 = test(train_loader)\n",
    "    # print('train loss: {:.5f}, val task1 auc: {:.5f}, val task2 auc: {:.3f}, val task3 auc: {:.3f}'.format(np.mean(epoch_loss), auc1, auc2, auc3))\n",
    "\n",
    "    # print('train loss: {:.5f}, val task1 auc: {:.5f}'.format(np.mean(epoch_loss), auc1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f056458-424e-4590-8a9d-a5a3a937e0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_188/4025087072.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test task1 auc: {:.3f}, test task2 auc: {:.3f}, test task3 auc: {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_188/277935921.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mt1_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt3_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt3_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "auc1, auc2, auc3 = test(test_loader)\n",
    "print('test task1 auc: {:.3f}, test task2 auc: {:.3f}, test task3 auc: {:.3f}'.format(auc1, auc2, auc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a955fa-f01d-47ee-88aa-84cfd34d6abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
